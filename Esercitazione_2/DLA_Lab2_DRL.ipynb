{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f6ae2782-57ea-48f0-b948-b412d0076ffc",
      "metadata": {
        "id": "f6ae2782-57ea-48f0-b948-b412d0076ffc"
      },
      "source": [
        "# Deep Reinforcement Learning Laboratory\n",
        "\n",
        "In this laboratory session we will work on getting more advanced versions of Deep Reinforcement Learning algorithms up and running. Deep Reinforcement Learning is **hard**, and getting agents to stably train can be frustrating and requires quite a bit of subtlety in analysis of intermediate results. We will start by refactoring (a bit) my implementation of `REINFORCE` on the [Cartpole environment](https://gymnasium.farama.org/environments/classic_control/cart_pole/)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3Cvjww4Spf4j",
      "metadata": {
        "id": "3Cvjww4Spf4j"
      },
      "source": [
        "### Import the packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ZJfqVOJpyiW",
      "metadata": {
        "id": "3ZJfqVOJpyiW"
      },
      "outputs": [],
      "source": [
        "# %%capture\n",
        "# !apt install python-opengl\n",
        "# !apt install ffmpeg\n",
        "# !apt install xvfb\n",
        "# !pip install pyvirtualdisplay\n",
        "# !pip install pyglet==1.5.1\n",
        "# !pip install gym_pygame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7AYhIWIGprT5",
      "metadata": {
        "id": "7AYhIWIGprT5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "from collections import deque\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import imageio\n",
        "from PIL import Image\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "# Gym\n",
        "import gymnasium as gym\n",
        "\n",
        "import wandb\n",
        "from wandb.integration.sb3 import WandbCallback\n",
        "import copy\n",
        "import random\n",
        "import cv2\n",
        "\n",
        "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
        "from stable_baselines3.common.vec_env import VecTransposeImage, DummyVecEnv, VecFrameStack, VecNormalize, SubprocVecEnv\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.vec_env import VecFrameStack"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fadaf0c-f9b2-4680-8456-0eadb5eb8c2f",
      "metadata": {
        "id": "3fadaf0c-f9b2-4680-8456-0eadb5eb8c2f"
      },
      "source": [
        "## Exercise 1: Improving my `REINFORCE` Implementation (warm up)\n",
        "\n",
        "In this exercise we will refactor a bit and improve some aspects of my `REINFORCE` implementation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "riPqQhsEqhV7",
      "metadata": {
        "id": "riPqQhsEqhV7"
      },
      "source": [
        "### The CartPole environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "M58K8c372-kF",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M58K8c372-kF",
        "outputId": "2d045f4c-9add-47f8-c65f-21828572d32d"
      },
      "outputs": [],
      "source": [
        "# Virtual display\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython.display import Video, display\n",
        "\n",
        "virtual_display = Display(visible=0, size=(1400, 900))\n",
        "virtual_display.start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yPyG-LNTqZeq",
      "metadata": {
        "id": "yPyG-LNTqZeq"
      },
      "outputs": [],
      "source": [
        "env_id = \"CartPole-v1\"\n",
        "\n",
        "env = gym.make(env_id)\n",
        "\n",
        "eval_env = gym.make(env_id)\n",
        "\n",
        "s_size = env.observation_space.shape[0]\n",
        "a_size = env.action_space.n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5oZubromqv0e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5oZubromqv0e",
        "outputId": "0b55cd73-da04-48ca-f6dc-045b9b631f4a"
      },
      "outputs": [],
      "source": [
        "print(\"_____OBSERVATION SPACE_____ \\n\")\n",
        "print(\"The State Space is: \", s_size)\n",
        "print(\"Sample observation\", env.observation_space.sample()) # Get a random observation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RD7TPeGkqxva",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RD7TPeGkqxva",
        "outputId": "bb21487f-0333-4b09-d0e0-220d9bfcee45"
      },
      "outputs": [],
      "source": [
        "print(\"\\n _____ACTION SPACE_____ \\n\")\n",
        "print(\"The Action Space is: \", a_size)\n",
        "print(\"Action Space Sample\", env.action_space.sample()) # Take a random action"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "v1NcD7CPq7tn",
      "metadata": {
        "id": "v1NcD7CPq7tn"
      },
      "source": [
        "### Reinforce Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BzIc4ySKoHw0",
      "metadata": {
        "id": "BzIc4ySKoHw0"
      },
      "source": [
        "> ⚠️ **Disclaimer**  \n",
        "> L'implementazione riportata di seguito si basa su una combinazione di risorse pubblicamente disponibili e suggerite dal corso **Deep Reinforcement Learning with Hugging Face**. In particolare, il codice e i concetti sono stati ispirati e adattati da:\n",
        ">\n",
        "> - [Esempio ufficiale PyTorch REINFORCE](https://github.com/pytorch/examples/blob/main/reinforcement_learning/reinforce.py)  \n",
        "> - [Implementazione REINFORCE di Udacity](https://github.com/udacity/deep-reinforcement-learning/blob/master/reinforce/REINFORCE.ipynb)  \n",
        "> - [Pull Request di ottimizzazione di Chris1nexus](https://github.com/huggingface/deep-rl-class/pull/95)\n",
        ">\n",
        "> In particolare, l'efficiente calcolo del **reward-to-go** è stato adattato dal lavoro di [Chris1nexus](https://github.com/Chris1nexus), il cui codice e spiegazioni sono ben documentati nella [pull request](https://github.com/huggingface/deep-rl-class/pull/95) al corso Hugging Face.\n",
        ">\n",
        "> Queste fonti sono state selezionate tra quelle raccomandate nel modulo dedicato alla policy gradient del corso, e hanno fornito una base solida sia dal punto di vista teorico che implementativo.\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/reinforce.png\" alt=\"Reinforce\"/>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RlCw4IjCoBrz",
      "metadata": {
        "id": "RlCw4IjCoBrz"
      },
      "outputs": [],
      "source": [
        "class Policy(nn.Module):\n",
        "    def __init__(self, s_size, a_size, h_size):\n",
        "        super(Policy, self).__init__()\n",
        "        self.fc1 = nn.Linear(s_size, h_size)\n",
        "        self.fc2 = nn.Linear(h_size, a_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return F.softmax(x, dim=1)\n",
        "\n",
        "    def act(self, state):\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "        probs = self.forward(state).cpu()\n",
        "        m = Categorical(probs)\n",
        "        action = m.sample()\n",
        "        return action.item(), m.log_prob(action)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6T0UPJS_rWF9",
      "metadata": {
        "id": "6T0UPJS_rWF9"
      },
      "source": [
        "### Reinforce Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FleRr65QrZj3",
      "metadata": {
        "id": "FleRr65QrZj3"
      },
      "outputs": [],
      "source": [
        "class ReinforceTrainer:\n",
        "    def __init__(self, policy, optimizer, env, gamma=0.99, max_t=1000, project_name=None):\n",
        "        self.policy = policy\n",
        "        self.optimizer = optimizer\n",
        "        self.env = env\n",
        "        self.gamma = gamma\n",
        "        self.max_t = max_t\n",
        "        self.project_name = project_name\n",
        "\n",
        "        self.scores = []\n",
        "        self.scores_deque = deque(maxlen=100)\n",
        "        self.best_score = -float('inf')\n",
        "\n",
        "        if self.project_name:\n",
        "            wandb.init(project=self.project_name)\n",
        "\n",
        "    def compute_returns(self, rewards):\n",
        "        returns = deque(maxlen=self.max_t)\n",
        "        for t in reversed(range(len(rewards))):\n",
        "            R = rewards[t] + self.gamma * (returns[0] if returns else 0.0)\n",
        "            returns.appendleft(R)\n",
        "        returns = torch.tensor(returns)\n",
        "        eps = np.finfo(np.float32).eps.item()\n",
        "        return (returns - returns.mean()) / (returns.std() + eps)\n",
        "\n",
        "    def reinforce(self, num_episodes=500, print_every=50, save_path=\"best_reinforce.pt\"):\n",
        "        for i_episode in range(1, num_episodes + 1):\n",
        "            saved_log_probs = []\n",
        "            rewards = []\n",
        "            # Gymnasium reset returns a tuple (observation, info), we only need the observation\n",
        "            state, _ = self.env.reset()\n",
        "\n",
        "            for t in range(self.max_t):\n",
        "                action, log_prob = self.policy.act(state)\n",
        "                saved_log_probs.append(log_prob)\n",
        "                # Gymnasium step returns (observation, reward, terminated, truncated, info)\n",
        "                next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
        "                rewards.append(reward)\n",
        "                done = terminated or truncated # In Gymnasium, done is terminated or truncated\n",
        "                state = next_state # Update state for the next iteration\n",
        "                if done:\n",
        "                    break\n",
        "\n",
        "            total_reward = sum(rewards)\n",
        "            self.scores.append(total_reward)\n",
        "            self.scores_deque.append(total_reward)\n",
        "\n",
        "            returns = self.compute_returns(rewards)\n",
        "\n",
        "            policy_loss = [-log_prob * R for log_prob, R in zip(saved_log_probs, returns)]\n",
        "            policy_loss = torch.cat(policy_loss).sum()\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            policy_loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            avg_score = np.mean(self.scores_deque)\n",
        "\n",
        "            if self.project_name:\n",
        "                wandb.log({\n",
        "                    'episode': i_episode,\n",
        "                    'reward': total_reward,\n",
        "                    'avg_reward': avg_score\n",
        "                })\n",
        "\n",
        "            if avg_score > self.best_score:\n",
        "                self.best_score = avg_score\n",
        "                torch.save(self.policy.state_dict(), save_path)\n",
        "                if self.project_name:\n",
        "                    wandb.run.summary[\"best_avg_reward\"] = avg_score\n",
        "\n",
        "            if i_episode % print_every == 0:\n",
        "                print(f\"Episode {i_episode}\\tAverage Score: {avg_score:.2f}\")\n",
        "\n",
        "        self.plot_rewards(self.scores) # Use self.scores which is accumulated over all episodes\n",
        "\n",
        "        return self.scores\n",
        "\n",
        "    def plot_rewards(self, reward_list, window=100):\n",
        "          plt.figure(figsize=(12, 6))\n",
        "          plt.plot(reward_list, label='Reward per Episode')\n",
        "\n",
        "          if len(reward_list) >= window:\n",
        "              moving_avg = np.convolve(reward_list, np.ones(window)/window, mode='valid')\n",
        "              plt.plot(range(window - 1, len(reward_list)), moving_avg, label=f'{window}-Episode Moving Avg', linewidth=2)\n",
        "\n",
        "          plt.xlabel('Episode')\n",
        "          plt.ylabel('Reward')\n",
        "          plt.title('Training Progress')\n",
        "          plt.legend()\n",
        "          plt.grid(True)\n",
        "          plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aEpVADzW2s-T",
      "metadata": {
        "id": "aEpVADzW2s-T"
      },
      "outputs": [],
      "source": [
        "def evaluate_agent(env, max_steps, n_eval_episodes, policy):\n",
        "    \"\"\"\n",
        "    Evaluate the agent for `n_eval_episodes` episodes and return the average and standard deviation of rewards.\n",
        "\n",
        "    :param env: The evaluation environment (gym.Env)\n",
        "    :param max_steps: Maximum number of steps per episode\n",
        "    :param n_eval_episodes: Number of episodes to evaluate the agent\n",
        "    :param policy: The Reinforce policy with an `act(state)` method\n",
        "    :return: Tuple of (mean_reward, std_reward)\n",
        "    \"\"\"\n",
        "    episode_rewards = []\n",
        "\n",
        "    for episode in range(n_eval_episodes):\n",
        "        state, _ = env.reset()  # Gymnasium reset returns (obs, info)\n",
        "        total_rewards_ep = 0\n",
        "\n",
        "        for step in range(max_steps):\n",
        "            action, _ = policy.act(state)\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            total_rewards_ep += reward\n",
        "            done = terminated or truncated\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "            state = next_state\n",
        "\n",
        "        episode_rewards.append(total_rewards_ep)\n",
        "\n",
        "    mean_reward = np.mean(episode_rewards)\n",
        "    std_reward = np.std(episode_rewards)\n",
        "\n",
        "    return mean_reward, std_reward\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ql0TeZuSuDeQ",
      "metadata": {
        "id": "ql0TeZuSuDeQ"
      },
      "outputs": [],
      "source": [
        "def record_video(env, policy, out_path, fps=30):\n",
        "    \"\"\"\n",
        "    Record a video of an agent acting in an environment.\n",
        "    :param env: the environment with render_mode='rgb_array'\n",
        "    :param policy: the agent, must implement policy.act(state)\n",
        "    :param out_path: full path to output .mp4 or .gif\n",
        "    :param fps: frames per second\n",
        "    \"\"\"\n",
        "    images = []\n",
        "    state, _ = env.reset()\n",
        "    done = False\n",
        "\n",
        "    img = env.render()\n",
        "    images.append(img)\n",
        "\n",
        "    while not done:\n",
        "        action, _ = policy.act(state)\n",
        "        obs, reward, done, truncated, info = env.step(action)\n",
        "        done = terminated or truncated\n",
        "        img = env.render()\n",
        "        images.append(img)\n",
        "\n",
        "    imageio.mimsave(out_path, [np.array(img) for img in images], fps=fps)\n",
        "    print(f\"Video saved to {out_path}\")\n",
        "    display(Video(out_path, embed=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nRL2ik9lw_Tx",
      "metadata": {
        "id": "nRL2ik9lw_Tx"
      },
      "outputs": [],
      "source": [
        "cartpole_hyperparameters = {\n",
        "    \"h_size\": 16,\n",
        "    \"n_training_episodes\": 600,\n",
        "    \"n_evaluation_episodes\": 10,\n",
        "    \"max_t\": 1000,\n",
        "    \"gamma\": 1.0,\n",
        "    \"lr\": 1e-2,\n",
        "    \"env_id\": env_id,\n",
        "    \"state_space\": s_size,\n",
        "    \"action_space\": a_size,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kppKkmSsrMnY",
      "metadata": {
        "id": "kppKkmSsrMnY"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88smzENVuF4i",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "88smzENVuF4i",
        "outputId": "f650f342-ed62-416d-e5e2-f1554247bad9"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.n\n",
        "policy = Policy(cartpole_hyperparameters[\"state_space\"], cartpole_hyperparameters[\"action_space\"], cartpole_hyperparameters[\"h_size\"]).to(device)\n",
        "optimizer = optim.Adam(policy.parameters(), lr=1e-2)\n",
        "\n",
        "trainer = ReinforceTrainer(policy, optimizer, env, gamma=0.99, max_t=cartpole_hyperparameters['max_t'])\n",
        "\n",
        "scores = trainer.reinforce(cartpole_hyperparameters['n_training_episodes'], print_every=10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BvakuGwIVnES",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "id": "BvakuGwIVnES",
        "outputId": "3e03b787-9a48-4c28-8f29-4abe8e83b930"
      },
      "outputs": [],
      "source": [
        "record_video(env, policy, \"Cart_Pole.mp4\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a5adad7-759b-4000-925b-701f41fe6e97",
      "metadata": {
        "id": "2a5adad7-759b-4000-925b-701f41fe6e97"
      },
      "source": [
        "-----\n",
        "## Exercise 2: `REINFORCE` with a Value Baseline (warm up)\n",
        "\n",
        "\n",
        "In questa variante dell’algoritmo REINFORCE, invece di aggiornare la policy solo in base al ritorno totale \\( G_t \\), si introduce una **baseline** per ridurre la varianza della stima del gradiente. Una scelta comune è usare una **value function** \\( V(s_t) \\), appresa tramite regressione sui ritorni osservati.\n",
        "\n",
        "L’aggiornamento della policy diventa:\n",
        "\n",
        "\\[\n",
        "\\theta \\leftarrow \\theta + \\alpha \\, (G_t - V(s_t)) \\, \\nabla_\\theta \\log \\pi(a_t | s_t)\n",
        "\\]\n",
        "\n",
        "Dove:\n",
        "- \\( G_t \\) è il ritorno cumulato dall’istante \\( t \\)\n",
        "- \\( V(s_t) \\) è il valore stimato dello stato corrente\n",
        "- \\( A_t = G_t - V(s_t) \\) è il **vantaggio**: quanto il risultato effettivo è migliore del previsto\n",
        "\n",
        "In pratica:\n",
        "- Addestriamo una seconda rete (`value_net`) per approssimare \\( V(s) \\)\n",
        "- Usiamo \\( A_t \\) come pesatura per il gradiente della policy (invece di usare direttamente \\( G_t \\))\n",
        "- Questo riduce la varianza dell’aggiornamento, stabilizzando il training\n",
        "\n",
        "**Nota:** La rete del valore viene ottimizzata separatamente minimizzando l’errore quadratico tra \\( V(s_t) \\) e \\( G_t \\).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Vzk_vGwhYatv",
      "metadata": {
        "id": "Vzk_vGwhYatv"
      },
      "outputs": [],
      "source": [
        "class ValueNetwork(nn.Module):\n",
        "    def __init__(self, state_size, hidden_size=128):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(state_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, state):\n",
        "      return self.net(state)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8b8cf76-5d39-45e9-8d7d-324125c04b4a",
      "metadata": {
        "id": "d8b8cf76-5d39-45e9-8d7d-324125c04b4a"
      },
      "outputs": [],
      "source": [
        "class ReinforceWithBaselineTrainer(ReinforceTrainer):\n",
        "    def __init__(self, policy, value_network, policy_optimizer, value_optimizer,\n",
        "                 env, gamma=0.99, max_t=1000, project_name=None):\n",
        "        super().__init__(policy, policy_optimizer, env, gamma, max_t, project_name)\n",
        "        self.value_network = value_network\n",
        "        self.value_optimizer = value_optimizer\n",
        "\n",
        "    def reinforce(self, num_episodes=500, print_every=50, save_path=\"best_reinforce_baseline.pt\"):\n",
        "        for i_episode in range(1, num_episodes + 1):\n",
        "            saved_log_probs = []\n",
        "            rewards = []\n",
        "            states = []\n",
        "\n",
        "            state, _ = self.env.reset()\n",
        "\n",
        "            for t in range(self.max_t):\n",
        "                action, log_prob = self.policy.act(state)\n",
        "                saved_log_probs.append(log_prob)\n",
        "                states.append(state)\n",
        "                next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
        "                rewards.append(reward)\n",
        "                done = terminated or truncated\n",
        "                state = next_state\n",
        "                if done:\n",
        "                    break\n",
        "\n",
        "            returns = self.compute_returns(rewards)\n",
        "            total_reward = sum(rewards)\n",
        "            self.scores.append(total_reward)\n",
        "            self.scores_deque.append(total_reward)\n",
        "\n",
        "            policy_losses = []\n",
        "            value_losses = []\n",
        "            for log_prob, R, state in zip(saved_log_probs, returns, states):\n",
        "                state_tensor = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "                baseline = self.value_network(state_tensor)\n",
        "                advantage = R - baseline.item()\n",
        "                policy_losses.append(-log_prob * advantage)\n",
        "                value_losses.append(nn.functional.mse_loss(baseline.squeeze(), torch.tensor(R).to(device)))\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            policy_loss = torch.stack(policy_losses).sum()\n",
        "            policy_loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            self.value_optimizer.zero_grad()\n",
        "            value_loss = torch.stack(value_losses).sum()\n",
        "            value_loss.backward()\n",
        "            self.value_optimizer.step()\n",
        "\n",
        "            avg_score = np.mean(self.scores_deque)\n",
        "\n",
        "            if self.project_name:\n",
        "                wandb.log({\n",
        "                    'episode': i_episode,\n",
        "                    'reward': total_reward,\n",
        "                    'avg_reward': avg_score,\n",
        "                    'policy_loss': policy_loss.item(),\n",
        "                    'value_loss': value_loss.item()\n",
        "                })\n",
        "\n",
        "            if avg_score > self.best_score:\n",
        "                self.best_score = avg_score\n",
        "                torch.save(self.policy.state_dict(), save_path)\n",
        "                if self.project_name:\n",
        "                    wandb.run.summary[\"best_avg_reward\"] = avg_score\n",
        "\n",
        "            if i_episode % print_every == 0:\n",
        "                print(f\"Episode {i_episode}\\tAverage Score: {avg_score:.2f}\")\n",
        "\n",
        "        self.plot_rewards(self.scores)\n",
        "        return self.scores\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8cdd5bc3-a439-4d84-891b-f4a840331e07",
      "metadata": {
        "id": "8cdd5bc3-a439-4d84-891b-f4a840331e07"
      },
      "source": [
        "**First Things First**: Recall from the slides on Deep Reinforcement Learning that we can **subtract** any function that doesn't depend on the current action from the q-value without changing the (maximum of our) objecttive function $J$:  \n",
        "\n",
        "$$ \\nabla J(\\boldsymbol{\\theta}) \\propto \\sum_{s} \\mu(s) \\sum_a \\left( q_{\\pi}(s, a) - b(s) \\right) \\nabla \\pi(a \\mid s, \\boldsymbol{\\theta}) $$\n",
        "\n",
        "In `REINFORCE` this means we can subtract from our target $G_t$:\n",
        "\n",
        "$$ \\boldsymbol{\\theta}_{t+1} \\triangleq \\boldsymbol{\\theta}_t + \\alpha (G_t - b(S_t)) \\frac{\\nabla \\pi(A_t \\mid s, \\boldsymbol{\\theta})}{\\pi(A_t \\mid s, \\boldsymbol{\\theta})} $$\n",
        "\n",
        "Since we are only interested in the **maximum** of our objective, we can also **rescale** our target by any function that also doesn't depend on the action. A **simple baseline** which is even independent of the state -- that is, it is **constant** for each episode -- is to just **standardize rewards within the episode**. So, we **subtract** the average return and **divide** by the variance of returns:\n",
        "\n",
        "$$ \\boldsymbol{\\theta}_{t+1} \\triangleq \\boldsymbol{\\theta}_t + \\alpha \\left(\\frac{G_t - \\bar{G}}{\\sigma_G}\\right) \\nabla  \\pi(A_t \\mid s, \\boldsymbol{\\theta}) $$\n",
        "\n",
        "This baseline is **already** implemented in my implementation of `REINFORCE`. Experiment with and without this standardization baseline and compare the performance. We are going to do something more interesting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "faf5b245-9369-4ed9-b5f4-3a52bb975d3f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "faf5b245-9369-4ed9-b5f4-3a52bb975d3f",
        "outputId": "dc0f3e5f-4673-4892-b7a6-942a520cd571"
      },
      "outputs": [],
      "source": [
        "seed = 123\n",
        "state, _ = env.reset(seed=seed)\n",
        "env.action_space.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "\n",
        "state_size = env.observation_space.shape[0]\n",
        "action_size = env.action_space.n\n",
        "hidden_size = 128\n",
        "\n",
        "policy1 = Policy(cartpole_hyperparameters[\"state_space\"], cartpole_hyperparameters[\"action_space\"], cartpole_hyperparameters[\"h_size\"]).to(device)\n",
        "policy2 = copy.deepcopy(policy1)\n",
        "\n",
        "policy_optimizer1 = torch.optim.Adam(policy1.parameters(), lr=1e-2)\n",
        "policy_optimizer2 = torch.optim.Adam(policy2.parameters(), lr=1e-2)\n",
        "\n",
        "# Istanzia rete valore + ottimizzatore per baseline trainer\n",
        "value_net = ValueNetwork(state_size, hidden_size).to(device)\n",
        "value_optimizer = torch.optim.Adam(value_net.parameters(), lr=1e-2)\n",
        "\n",
        "\n",
        "trainer_no_baseline = ReinforceTrainer(policy1, policy_optimizer1, env, gamma=0.99, max_t=1000)\n",
        "trainer_with_baseline = ReinforceWithBaselineTrainer(policy2, value_net, policy_optimizer2, value_optimizer,\n",
        "                                                    env, gamma=0.99, max_t=1000)\n",
        "\n",
        "\n",
        "scores_no_baseline = trainer_no_baseline.reinforce(cartpole_hyperparameters['n_training_episodes'], print_every=10),\n",
        "scores_with_baseline = trainer_with_baseline.reinforce(cartpole_hyperparameters['n_training_episodes'], print_every=10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46cfde8e-f1c7-4d96-85e0-268f687e09df",
      "metadata": {
        "id": "46cfde8e-f1c7-4d96-85e0-268f687e09df"
      },
      "source": [
        "**The Real Exercise**: Standard practice is to use the state-value function $v(s)$ as a baseline. This is intuitively appealing -- we are more interested in updating out policy for returns that estimate the current **value** worse. Our new update becomes:\n",
        "\n",
        "$$ \\boldsymbol{\\theta}_{t+1} \\triangleq \\boldsymbol{\\theta}_t + \\alpha (G_t - \\tilde{v}(S_t \\mid \\mathbf{w})) \\frac{\\nabla \\pi(A_t \\mid s, \\boldsymbol{\\theta})}{\\pi(A_t \\mid s, \\boldsymbol{\\theta})} $$\n",
        "\n",
        "where $\\tilde{v}(s \\mid \\mathbf{w})$ is a **deep neural network** with parameters $w$ that estimates $v_\\pi(s)$. What neural network? Typically, we use the **same** network architecture as that of the Policy.\n",
        "\n",
        "**Your Task**: Modify your implementation to fit a second, baseline network to estimate the value function and use it as **baseline**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4de3a2a8-05c3-44e2-9858-8cc8f431325f",
      "metadata": {
        "id": "4de3a2a8-05c3-44e2-9858-8cc8f431325f"
      },
      "outputs": [],
      "source": [
        "# Your code here."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64bf1447-d222-4b24-a357-5b7f9824390c",
      "metadata": {
        "id": "64bf1447-d222-4b24-a357-5b7f9824390c"
      },
      "source": [
        "-----\n",
        "## Exercise 3: Going Deeper\n",
        "\n",
        "As usual, pick **AT LEAST ONE** of the following exercises to complete.\n",
        "\n",
        "### Exercise 3.1: Solving Lunar Lander with `REINFORCE` (easy)\n",
        "\n",
        "Use my (or even better, improve on my) implementation of `REINFORCE` to solve the [Lunar Lander Environment](https://gymnasium.farama.org/environments/box2d/lunar_lander/). This environment is a little bit harder than Cartpole, but not much. Make sure you perform the same types of analyses we did during the lab session to quantify and qualify the performance of your agents.\n",
        "\n",
        "### Exercise 3.2: Solving Cartpole and Lunar Lander with `Deep Q-Learning` (harder)\n",
        "\n",
        "On policy Deep Reinforcement Learning tends to be **very unstable**. Write an implementation (or adapt an existing one) of `Deep Q-Learning` to solve our two environments (Cartpole and Lunar Lander). To do this you will need to implement a **Replay Buffer** and use a second, slow-moving **target Q-Network** to stabilize learning.\n",
        "\n",
        "### Exercise 3.3: Solving the OpenAI CarRacing environment (hardest)\n",
        "\n",
        "Use `Deep Q-Learning` -- or even better, an off-the-shelf implementation of **Proximal Policy Optimization (PPO)** -- to train an agent to solve the [OpenAI CarRacing](https://github.com/andywu0913/OpenAI-GYM-CarRacing-DQN) environment. This will be the most *fun*, but also the most *difficult*. Some tips:\n",
        "\n",
        "1. Make sure you use the `continuous=False` argument to the environment constructor. This ensures that the action space is **discrete** (we haven't seen how to work with continuous action spaces).\n",
        "2. Your Q-Network will need to be a CNN. A simple one should do, with two convolutional + maxpool layers, folowed by a two dense layers. You will **definitely** want to use a GPU to train your agents.\n",
        "3. The observation space of the environment is a single **color image** (a single frame of the game). Most implementations stack multiple frames (e.g. 3) after converting them to grayscale images as an observation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a8b3886",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e76ae1f7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "HYPERPARAMETERS =  {'checkpoint': 310, 'test': False, 'gamma': 0.99, 'action_repeat': 4, 'valueStackSize': 4, 'actionStack': 4, 'seed': 0, 'numberOfLasers': 5, 'deathThreshold': 2000, 'clip_param': 0.4, 'ppo_epoch': 10, 'buffer_capacity': 500, 'batch_size': 128, 'deathByGreeneryThreshold': 35, 'maxDistance': 100, 'total_episodes': 0, 'actionMultiplier': array([2., 1., 1.]), 'actionBias': array([-1.,  0.,  0.]), 'saveLocation': 'model/distances/train_5/'}\n",
            "LOADING FROM EPISODE 310\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvincenzo-civale\u001b[0m (\u001b[33mvincenzo-civale-universi-degli-studi-di-firenze\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/vcivale/prova/Esercitazione_2/wandb/run-20250613_111719-2i0e10r2</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/vincenzo-civale-universi-degli-studi-di-firenze/CarRacing/runs/2i0e10r2' target=\"_blank\">run_310</a></strong> to <a href='https://wandb.ai/vincenzo-civale-universi-degli-studi-di-firenze/CarRacing' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/vincenzo-civale-universi-degli-studi-di-firenze/CarRacing' target=\"_blank\">https://wandb.ai/vincenzo-civale-universi-degli-studi-di-firenze/CarRacing</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/vincenzo-civale-universi-degli-studi-di-firenze/CarRacing/runs/2i0e10r2' target=\"_blank\">https://wandb.ai/vincenzo-civale-universi-degli-studi-di-firenze/CarRacing/runs/2i0e10r2</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 0: action=0, reward=1.649, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=1, reward=0.890, dead=False\n",
            "  Score cumulativo: 30.79\n",
            "Step 100: action=1, reward=-0.007, dead=False\n",
            "  Score cumulativo: 69.55\n",
            "Step 150: action=1, reward=0.000, dead=False\n",
            "  Score cumulativo: 69.39\n",
            "Step 200: action=1, reward=0.000, dead=False\n",
            "  Score cumulativo: 69.23\n",
            "Step 0: action=3, reward=1.538, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=3, reward=0.794, dead=False\n",
            "  Score cumulativo: 22.86\n",
            "Step 100: action=0, reward=1.448, dead=False\n",
            "  Score cumulativo: 73.35\n",
            "Step 150: action=1, reward=-0.015, dead=False\n",
            "  Score cumulativo: 90.04\n",
            "Step 200: action=1, reward=-0.000, dead=False\n",
            "  Score cumulativo: 89.97\n",
            "UPDATING WEIGHTS AT EPISODE =  311\n",
            "Step 0: action=3, reward=1.657, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 26.86\n",
            "Step 100: action=1, reward=0.000, dead=False\n",
            "  Score cumulativo: 68.23\n",
            "Step 150: action=1, reward=0.000, dead=False\n",
            "  Score cumulativo: 72.14\n",
            "Step 200: action=1, reward=0.000, dead=False\n",
            "  Score cumulativo: 72.07\n",
            "Step 0: action=3, reward=1.619, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 22.59\n",
            "Step 100: action=1, reward=0.000, dead=False\n",
            "  Score cumulativo: 36.24\n",
            "Step 150: action=1, reward=0.000, dead=False\n",
            "  Score cumulativo: 36.19\n",
            "Step 200: action=1, reward=-0.007, dead=False\n",
            "  Score cumulativo: 36.14\n",
            "UPDATING WEIGHTS AT EPISODE =  313\n",
            "Step 0: action=3, reward=1.891, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=1, reward=0.000, dead=False\n",
            "  Score cumulativo: 21.50\n",
            "Step 100: action=1, reward=-0.008, dead=False\n",
            "  Score cumulativo: 21.42\n",
            "Step 150: action=1, reward=-0.000, dead=False\n",
            "  Score cumulativo: 21.30\n",
            "Step 200: action=1, reward=0.000, dead=False\n",
            "  Score cumulativo: 21.21\n",
            "Step 0: action=3, reward=1.744, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=0, reward=0.777, dead=False\n",
            "  Score cumulativo: 27.83\n",
            "Step 100: action=1, reward=0.000, dead=False\n",
            "  Score cumulativo: 33.50\n",
            "Step 150: action=1, reward=0.000, dead=False\n",
            "  Score cumulativo: 33.44\n",
            "Step 200: action=1, reward=-0.015, dead=False\n",
            "  Score cumulativo: 33.34\n",
            "UPDATING WEIGHTS AT EPISODE =  315\n",
            "Step 0: action=3, reward=1.587, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=3, reward=0.819, dead=False\n",
            "  Score cumulativo: 27.90\n",
            "Step 100: action=1, reward=0.000, dead=False\n",
            "  Score cumulativo: 53.48\n",
            "Step 150: action=1, reward=-0.007, dead=False\n",
            "  Score cumulativo: 53.38\n",
            "Step 200: action=1, reward=0.000, dead=False\n",
            "  Score cumulativo: 53.31\n",
            "Step 0: action=3, reward=1.863, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=0, reward=0.836, dead=False\n",
            "  Score cumulativo: 32.58\n",
            "Step 100: action=1, reward=0.936, dead=False\n",
            "  Score cumulativo: 72.23\n",
            "Step 150: action=3, reward=0.956, dead=False\n",
            "  Score cumulativo: 110.28\n",
            "Step 200: action=1, reward=-0.000, dead=False\n",
            "  Score cumulativo: 133.48\n",
            "UPDATING WEIGHTS AT EPISODE =  317\n",
            "Step 0: action=0, reward=1.662, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=0, reward=0.796, dead=False\n",
            "  Score cumulativo: 30.09\n",
            "Step 100: action=2, reward=0.000, dead=False\n",
            "  Score cumulativo: 65.26\n",
            "Step 150: action=1, reward=0.000, dead=False\n",
            "  Score cumulativo: 65.06\n",
            "Step 200: action=1, reward=0.000, dead=False\n",
            "  Score cumulativo: 64.97\n",
            "Step 0: action=0, reward=1.537, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=0, reward=0.733, dead=False\n",
            "  Score cumulativo: 21.65\n",
            "Step 100: action=1, reward=0.000, dead=False\n",
            "  Score cumulativo: 30.98\n",
            "Step 150: action=1, reward=0.000, dead=False\n",
            "  Score cumulativo: 30.81\n",
            "Step 200: action=1, reward=-0.000, dead=False\n",
            "  Score cumulativo: 30.64\n",
            "UPDATING WEIGHTS AT EPISODE =  319\n",
            "Step 0: action=3, reward=1.552, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=0, reward=0.681, dead=False\n",
            "  Score cumulativo: 26.76\n",
            "Step 100: action=0, reward=0.681, dead=False\n",
            "  Score cumulativo: 57.01\n",
            "Step 150: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 70.76\n",
            "Step 200: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 70.16\n",
            "Step 0: action=3, reward=1.714, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 21.48\n",
            "Step 100: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 43.28\n",
            "Step 150: action=2, reward=0.862, dead=False\n",
            "  Score cumulativo: 60.71\n",
            "Step 200: action=1, reward=0.862, dead=False\n",
            "  Score cumulativo: 77.58\n",
            "UPDATING WEIGHTS AT EPISODE =  321\n",
            "-----------------------------------------\n",
            "SAVING AT EPISODE 321\n",
            "-----------------------------------------\n",
            "Step 0: action=0, reward=1.559, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=1, reward=0.000, dead=False\n",
            "  Score cumulativo: 26.42\n",
            "Step 100: action=3, reward=0.020, dead=False\n",
            "  Score cumulativo: 55.58\n",
            "Step 150: action=1, reward=0.000, dead=False\n",
            "  Score cumulativo: 63.62\n",
            "Step 200: action=1, reward=-0.007, dead=False\n",
            "  Score cumulativo: 63.44\n",
            "Step 0: action=0, reward=1.509, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=2, reward=0.820, dead=False\n",
            "  Score cumulativo: 26.24\n",
            "Step 100: action=1, reward=-0.007, dead=False\n",
            "  Score cumulativo: 26.82\n",
            "Step 150: action=1, reward=-0.000, dead=False\n",
            "  Score cumulativo: 26.66\n",
            "Step 200: action=1, reward=-0.000, dead=False\n",
            "  Score cumulativo: 26.45\n",
            "UPDATING WEIGHTS AT EPISODE =  323\n",
            "Step 0: action=0, reward=1.757, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=1, reward=0.000, dead=False\n",
            "  Score cumulativo: 31.37\n",
            "Step 100: action=1, reward=0.000, dead=False\n",
            "  Score cumulativo: 35.07\n",
            "Step 150: action=2, reward=0.000, dead=False\n",
            "  Score cumulativo: 34.47\n",
            "Step 200: action=3, reward=0.020, dead=False\n",
            "  Score cumulativo: 33.55\n",
            "Step 0: action=3, reward=1.562, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 25.07\n",
            "Step 100: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 35.53\n",
            "Step 150: action=0, reward=0.686, dead=False\n",
            "  Score cumulativo: 45.80\n",
            "Step 200: action=2, reward=0.000, dead=False\n",
            "  Score cumulativo: 70.73\n",
            "UPDATING WEIGHTS AT EPISODE =  325\n",
            "Step 0: action=0, reward=1.708, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=1, reward=0.000, dead=False\n",
            "  Score cumulativo: 24.51\n",
            "Step 100: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 52.94\n",
            "Step 150: action=1, reward=-0.000, dead=False\n",
            "  Score cumulativo: 83.28\n",
            "Step 200: action=1, reward=0.919, dead=False\n",
            "  Score cumulativo: 113.23\n",
            "Step 0: action=2, reward=1.700, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=1, reward=-0.000, dead=False\n",
            "  Score cumulativo: 14.99\n",
            "Step 100: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 36.56\n",
            "Step 150: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 65.75\n",
            "Step 200: action=1, reward=0.000, dead=False\n",
            "  Score cumulativo: 65.27\n",
            "UPDATING WEIGHTS AT EPISODE =  327\n",
            "Step 0: action=0, reward=1.588, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=1, reward=0.859, dead=False\n",
            "  Score cumulativo: 25.00\n",
            "Step 100: action=1, reward=0.000, dead=False\n",
            "  Score cumulativo: 32.67\n",
            "Step 150: action=1, reward=0.000, dead=False\n",
            "  Score cumulativo: 28.77\n",
            "Step 200: action=0, reward=0.759, dead=False\n",
            "  Score cumulativo: 27.08\n",
            "Step 0: action=0, reward=1.488, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=0, reward=0.709, dead=False\n",
            "  Score cumulativo: 27.79\n",
            "Step 100: action=3, reward=1.638, dead=False\n",
            "  Score cumulativo: 65.59\n",
            "Step 150: action=3, reward=0.020, dead=False\n",
            "  Score cumulativo: 87.94\n",
            "Step 200: action=1, reward=-0.008, dead=False\n",
            "  Score cumulativo: 87.31\n",
            "UPDATING WEIGHTS AT EPISODE =  329\n",
            "Step 0: action=0, reward=1.675, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=2, reward=-0.000, dead=False\n",
            "  Score cumulativo: 18.08\n",
            "Step 100: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 46.67\n",
            "Step 150: action=1, reward=0.000, dead=False\n",
            "  Score cumulativo: 64.69\n",
            "Step 200: action=1, reward=0.000, dead=False\n",
            "  Score cumulativo: 64.45\n",
            "Step 0: action=1, reward=1.864, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 9.89\n",
            "Step 100: action=2, reward=-0.000, dead=False\n",
            "  Score cumulativo: 12.12\n",
            "Step 150: action=1, reward=-0.000, dead=False\n",
            "  Score cumulativo: 18.65\n",
            "Step 200: action=1, reward=0.947, dead=False\n",
            "  Score cumulativo: 33.58\n",
            "UPDATING WEIGHTS AT EPISODE =  331\n",
            "Step 0: action=0, reward=1.816, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=1, reward=-0.000, dead=False\n",
            "  Score cumulativo: 19.13\n",
            "Step 100: action=1, reward=-0.000, dead=False\n",
            "  Score cumulativo: 18.93\n",
            "Step 150: action=1, reward=-0.000, dead=False\n",
            "  Score cumulativo: 18.67\n",
            "Step 200: action=1, reward=-0.015, dead=False\n",
            "  Score cumulativo: 18.45\n",
            "Step 0: action=0, reward=1.736, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=2, reward=0.000, dead=False\n",
            "  Score cumulativo: 28.98\n",
            "Step 100: action=2, reward=0.000, dead=False\n",
            "  Score cumulativo: 35.31\n",
            "Step 150: action=0, reward=0.833, dead=False\n",
            "  Score cumulativo: 43.97\n",
            "Step 200: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 50.17\n",
            "UPDATING WEIGHTS AT EPISODE =  333\n",
            "-----------------------------------------\n",
            "SAVING AT EPISODE 333\n",
            "-----------------------------------------\n",
            "Step 0: action=1, reward=1.724, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=2, reward=0.877, dead=False\n",
            "  Score cumulativo: 17.03\n",
            "Step 100: action=1, reward=-0.000, dead=False\n",
            "  Score cumulativo: 22.80\n",
            "Step 150: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 21.70\n",
            "Step 200: action=0, reward=0.777, dead=False\n",
            "  Score cumulativo: 20.44\n",
            "Step 0: action=0, reward=1.565, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=2, reward=-0.000, dead=False\n",
            "  Score cumulativo: 16.34\n",
            "Step 100: action=1, reward=0.000, dead=False\n",
            "  Score cumulativo: 39.27\n",
            "Step 150: action=2, reward=0.000, dead=False\n",
            "  Score cumulativo: 48.36\n",
            "Step 200: action=0, reward=0.747, dead=False\n",
            "  Score cumulativo: 49.25\n",
            "UPDATING WEIGHTS AT EPISODE =  335\n",
            "Step 0: action=3, reward=1.842, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=1, reward=0.000, dead=False\n",
            "  Score cumulativo: 27.78\n",
            "Step 100: action=1, reward=0.000, dead=False\n",
            "  Score cumulativo: 27.58\n",
            "Step 150: action=1, reward=-0.000, dead=False\n",
            "  Score cumulativo: 27.34\n",
            "Step 200: action=1, reward=-0.008, dead=False\n",
            "  Score cumulativo: 27.07\n",
            "Step 0: action=0, reward=1.473, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=0, reward=-0.107, dead=False\n",
            "  Score cumulativo: 24.77\n",
            "Step 100: action=1, reward=0.000, dead=False\n",
            "  Score cumulativo: 26.69\n",
            "Step 150: action=1, reward=-0.000, dead=False\n",
            "  Score cumulativo: 27.50\n",
            "Step 200: action=1, reward=-0.000, dead=False\n",
            "  Score cumulativo: 26.30\n",
            "UPDATING WEIGHTS AT EPISODE =  337\n",
            "Step 0: action=0, reward=1.911, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=2, reward=-0.022, dead=False\n",
            "  Score cumulativo: 34.12\n",
            "Step 100: action=1, reward=0.000, dead=False\n",
            "  Score cumulativo: 33.97\n",
            "Step 150: action=1, reward=0.000, dead=False\n",
            "  Score cumulativo: 33.81\n",
            "Step 200: action=1, reward=-0.000, dead=False\n",
            "  Score cumulativo: 33.76\n",
            "Step 0: action=3, reward=1.763, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=1, reward=0.000, dead=False\n",
            "  Score cumulativo: 22.64\n",
            "Step 100: action=1, reward=-0.000, dead=False\n",
            "  Score cumulativo: 22.52\n",
            "Step 150: action=1, reward=-0.000, dead=False\n",
            "  Score cumulativo: 22.35\n",
            "Step 200: action=1, reward=0.000, dead=False\n",
            "  Score cumulativo: 22.04\n",
            "UPDATING WEIGHTS AT EPISODE =  339\n",
            "Step 0: action=0, reward=1.408, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=1, reward=0.000, dead=False\n",
            "  Score cumulativo: 23.48\n",
            "Step 100: action=0, reward=-0.108, dead=False\n",
            "  Score cumulativo: 21.78\n",
            "Step 150: action=1, reward=-0.000, dead=False\n",
            "  Score cumulativo: 20.57\n",
            "Step 200: action=1, reward=-0.007, dead=False\n",
            "  Score cumulativo: 19.05\n",
            "Step 0: action=0, reward=1.708, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=2, reward=0.000, dead=False\n",
            "  Score cumulativo: 29.04\n",
            "Step 100: action=1, reward=-0.007, dead=False\n",
            "  Score cumulativo: 26.79\n",
            "Step 150: action=1, reward=-0.000, dead=False\n",
            "  Score cumulativo: 25.28\n",
            "Step 200: action=2, reward=-0.000, dead=False\n",
            "  Score cumulativo: 26.07\n",
            "UPDATING WEIGHTS AT EPISODE =  341\n",
            "Step 0: action=3, reward=1.543, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=0, reward=-0.108, dead=False\n",
            "  Score cumulativo: 18.97\n",
            "Step 100: action=0, reward=-0.130, dead=False\n",
            "  Score cumulativo: 13.65\n",
            "Step 150: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 7.89\n",
            "Step 200: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 3.08\n",
            "Step 0: action=3, reward=1.562, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 23.42\n",
            "Step 100: action=2, reward=0.786, dead=False\n",
            "  Score cumulativo: 46.62\n",
            "Step 150: action=0, reward=-0.115, dead=False\n",
            "  Score cumulativo: 51.83\n",
            "Step 200: action=4, reward=0.000, dead=False\n",
            "  Score cumulativo: 51.69\n",
            "UPDATING WEIGHTS AT EPISODE =  343\n",
            "Step 0: action=0, reward=1.576, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 24.27\n",
            "Step 100: action=1, reward=0.000, dead=False\n",
            "  Score cumulativo: 28.27\n",
            "Step 150: action=2, reward=0.000, dead=False\n",
            "  Score cumulativo: 29.12\n",
            "Step 200: action=1, reward=-0.000, dead=False\n",
            "  Score cumulativo: 29.97\n",
            "Step 0: action=2, reward=1.788, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=0, reward=0.809, dead=False\n",
            "  Score cumulativo: 24.00\n",
            "Step 100: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 37.66\n",
            "Step 150: action=0, reward=-0.130, dead=False\n",
            "  Score cumulativo: 31.86\n",
            "Step 200: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 26.06\n",
            "Step 0: action=3, reward=1.492, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "UPDATING WEIGHTS AT EPISODE =  346\n",
            "-----------------------------------------\n",
            "SAVING AT EPISODE 346\n",
            "-----------------------------------------\n",
            "Step 50: action=4, reward=-0.000, dead=False\n",
            "  Score cumulativo: 14.73\n",
            "Step 100: action=4, reward=-0.000, dead=False\n",
            "  Score cumulativo: 14.73\n",
            "Step 150: action=4, reward=0.000, dead=False\n",
            "  Score cumulativo: 14.73\n",
            "Step 200: action=4, reward=-0.000, dead=False\n",
            "  Score cumulativo: 14.73\n",
            "Step 0: action=3, reward=1.505, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=2, reward=-0.000, dead=False\n",
            "  Score cumulativo: 11.83\n",
            "Step 100: action=1, reward=0.000, dead=False\n",
            "  Score cumulativo: 10.43\n",
            "Step 150: action=1, reward=0.000, dead=False\n",
            "  Score cumulativo: 9.53\n",
            "Step 200: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 8.83\n",
            "Step 0: action=3, reward=1.624, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "UPDATING WEIGHTS AT EPISODE =  348\n",
            "Step 50: action=3, reward=0.020, dead=False\n",
            "  Score cumulativo: 21.03\n",
            "Step 100: action=0, reward=0.717, dead=False\n",
            "  Score cumulativo: 63.50\n",
            "Step 150: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 88.21\n",
            "Step 200: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 82.75\n",
            "Step 0: action=3, reward=1.921, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=2, reward=0.965, dead=False\n",
            "  Score cumulativo: 25.89\n",
            "Step 100: action=0, reward=0.865, dead=False\n",
            "  Score cumulativo: 52.03\n",
            "Step 150: action=0, reward=-0.122, dead=False\n",
            "  Score cumulativo: 54.30\n",
            "Step 200: action=3, reward=0.985, dead=False\n",
            "  Score cumulativo: 70.40\n",
            "Step 0: action=4, reward=1.775, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=0, reward=0.803, dead=False\n",
            "  Score cumulativo: 20.73\n",
            "UPDATING WEIGHTS AT EPISODE =  350\n",
            "Step 100: action=4, reward=0.000, dead=False\n",
            "  Score cumulativo: 28.36\n",
            "Step 150: action=4, reward=0.000, dead=False\n",
            "  Score cumulativo: 28.26\n",
            "Step 200: action=4, reward=-0.000, dead=False\n",
            "  Score cumulativo: 27.96\n",
            "Step 0: action=3, reward=1.562, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=2, reward=-0.000, dead=False\n",
            "  Score cumulativo: 15.87\n",
            "Step 100: action=2, reward=-0.000, dead=False\n",
            "  Score cumulativo: 17.45\n",
            "Step 150: action=2, reward=0.000, dead=False\n",
            "  Score cumulativo: 17.45\n",
            "Step 200: action=1, reward=0.000, dead=False\n",
            "  Score cumulativo: 17.45\n",
            "Step 0: action=3, reward=1.751, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=3, reward=0.020, dead=False\n",
            "  Score cumulativo: 25.52\n",
            "UPDATING WEIGHTS AT EPISODE =  352\n",
            "Step 100: action=0, reward=-0.122, dead=False\n",
            "  Score cumulativo: 64.87\n",
            "Step 150: action=0, reward=-0.115, dead=False\n",
            "  Score cumulativo: 59.08\n",
            "Step 200: action=0, reward=0.780, dead=False\n",
            "  Score cumulativo: 87.63\n",
            "Step 0: action=3, reward=1.646, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 30.40\n",
            "Step 100: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 27.60\n",
            "Step 150: action=0, reward=-0.107, dead=False\n",
            "  Score cumulativo: 22.30\n",
            "Step 200: action=0, reward=-0.130, dead=False\n",
            "  Score cumulativo: 16.61\n",
            "Step 0: action=3, reward=1.951, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 40.28\n",
            "UPDATING WEIGHTS AT EPISODE =  354\n",
            "Step 100: action=2, reward=0.000, dead=False\n",
            "  Score cumulativo: 38.48\n",
            "Step 150: action=2, reward=0.000, dead=False\n",
            "  Score cumulativo: 37.18\n",
            "Step 200: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 36.18\n",
            "Step 0: action=3, reward=1.769, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=3, reward=0.910, dead=False\n",
            "  Score cumulativo: 36.25\n",
            "Step 100: action=0, reward=-0.107, dead=False\n",
            "  Score cumulativo: 62.89\n",
            "Step 150: action=0, reward=-0.130, dead=False\n",
            "  Score cumulativo: 60.08\n",
            "Step 200: action=0, reward=-0.115, dead=False\n",
            "  Score cumulativo: 54.23\n",
            "Step 0: action=3, reward=1.726, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 23.92\n",
            "UPDATING WEIGHTS AT EPISODE =  356\n",
            "Step 100: action=0, reward=-0.107, dead=False\n",
            "  Score cumulativo: 20.35\n",
            "Step 150: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 14.86\n",
            "Step 200: action=0, reward=-0.130, dead=False\n",
            "  Score cumulativo: 9.29\n",
            "Step 0: action=3, reward=1.582, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=2, reward=0.000, dead=False\n",
            "  Score cumulativo: 23.32\n",
            "Step 100: action=2, reward=1.592, dead=False\n",
            "  Score cumulativo: 65.35\n",
            "Step 150: action=2, reward=-0.000, dead=False\n",
            "  Score cumulativo: 106.53\n",
            "Step 200: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 132.35\n",
            "Step 0: action=3, reward=1.651, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=3, reward=0.851, dead=False\n",
            "  Score cumulativo: 33.70\n",
            "UPDATING WEIGHTS AT EPISODE =  358\n",
            "-----------------------------------------\n",
            "SAVING AT EPISODE 358\n",
            "-----------------------------------------\n",
            "Step 100: action=2, reward=0.831, dead=False\n",
            "  Score cumulativo: 87.98\n",
            "Step 150: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 117.86\n",
            "Step 200: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 112.19\n",
            "Step 0: action=3, reward=1.587, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=0, reward=-0.107, dead=False\n",
            "  Score cumulativo: 22.77\n",
            "Step 100: action=0, reward=-0.130, dead=False\n",
            "  Score cumulativo: 17.10\n",
            "Step 150: action=0, reward=-0.115, dead=False\n",
            "  Score cumulativo: 11.38\n",
            "Step 200: action=0, reward=-0.107, dead=False\n",
            "  Score cumulativo: 5.79\n",
            "Step 0: action=3, reward=1.528, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 22.67\n",
            "UPDATING WEIGHTS AT EPISODE =  360\n",
            "Step 100: action=0, reward=0.669, dead=False\n",
            "  Score cumulativo: 53.27\n",
            "Step 150: action=0, reward=-0.122, dead=False\n",
            "  Score cumulativo: 58.93\n",
            "Step 200: action=0, reward=-0.107, dead=False\n",
            "  Score cumulativo: 53.57\n",
            "Step 0: action=3, reward=1.702, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=1, reward=0.856, dead=False\n",
            "  Score cumulativo: 30.57\n",
            "Step 100: action=0, reward=-0.130, dead=False\n",
            "  Score cumulativo: 64.71\n",
            "Step 150: action=0, reward=-0.130, dead=False\n",
            "  Score cumulativo: 59.16\n",
            "Step 200: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 53.57\n",
            "Step 0: action=0, reward=1.839, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=1, reward=0.984, dead=False\n",
            "  Score cumulativo: 40.72\n",
            "Step 100: action=0, reward=-0.130, dead=False\n",
            "  Score cumulativo: 47.72\n",
            "UPDATING WEIGHTS AT EPISODE =  362\n",
            "Step 150: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 42.71\n",
            "Step 200: action=4, reward=0.000, dead=False\n",
            "  Score cumulativo: 38.41\n",
            "Step 0: action=3, reward=1.898, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 23.61\n",
            "Step 100: action=0, reward=-0.130, dead=False\n",
            "  Score cumulativo: 17.66\n",
            "Step 150: action=0, reward=-0.130, dead=False\n",
            "  Score cumulativo: 11.56\n",
            "Step 200: action=0, reward=-0.130, dead=False\n",
            "  Score cumulativo: 5.46\n",
            "Step 0: action=3, reward=1.461, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=0, reward=-0.130, dead=False\n",
            "  Score cumulativo: 15.18\n",
            "Step 100: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 10.24\n",
            "UPDATING WEIGHTS AT EPISODE =  364\n",
            "Step 150: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 5.34\n",
            "Step 200: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 0.34\n",
            "Step 0: action=3, reward=1.685, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=2, reward=0.000, dead=False\n",
            "  Score cumulativo: 33.09\n",
            "Step 100: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 31.63\n",
            "Step 150: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 28.42\n",
            "Step 200: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 23.42\n",
            "Step 0: action=3, reward=1.674, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=0, reward=-0.130, dead=False\n",
            "  Score cumulativo: 19.32\n",
            "Step 100: action=0, reward=-0.123, dead=False\n",
            "  Score cumulativo: 13.89\n",
            "UPDATING WEIGHTS AT EPISODE =  366\n",
            "Step 150: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 8.44\n",
            "Step 200: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 3.00\n",
            "Step 0: action=0, reward=1.722, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=4, reward=0.000, dead=False\n",
            "  Score cumulativo: 39.08\n",
            "Step 100: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 34.74\n",
            "Step 150: action=2, reward=-24.975, dead=True\n",
            "  Score cumulativo: 29.21\n",
            "Step 0: action=3, reward=1.662, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=0, reward=-0.130, dead=False\n",
            "  Score cumulativo: 28.25\n",
            "Step 100: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 25.34\n",
            "Step 150: action=0, reward=-0.123, dead=False\n",
            "  Score cumulativo: 19.36\n",
            "Step 200: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 13.76\n",
            "UPDATING WEIGHTS AT EPISODE =  368\n",
            "Step 0: action=3, reward=1.456, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=3, reward=1.486, dead=False\n",
            "  Score cumulativo: 29.74\n",
            "Step 100: action=2, reward=0.733, dead=False\n",
            "  Score cumulativo: 48.17\n",
            "Step 150: action=0, reward=-0.107, dead=False\n",
            "  Score cumulativo: 44.39\n",
            "Step 200: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 38.75\n",
            "Step 0: action=3, reward=1.943, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=0, reward=-0.107, dead=False\n",
            "  Score cumulativo: 24.54\n",
            "Step 100: action=0, reward=-0.108, dead=False\n",
            "  Score cumulativo: 19.01\n",
            "Step 0: action=3, reward=1.572, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=0, reward=-0.115, dead=False\n",
            "  Score cumulativo: 28.25\n",
            "Step 100: action=0, reward=-0.107, dead=False\n",
            "  Score cumulativo: 22.60\n",
            "UPDATING WEIGHTS AT EPISODE =  371\n",
            "-----------------------------------------\n",
            "SAVING AT EPISODE 371\n",
            "-----------------------------------------\n",
            "Step 150: action=1, reward=0.791, dead=False\n",
            "  Score cumulativo: 30.44\n",
            "Step 200: action=0, reward=-0.107, dead=False\n",
            "  Score cumulativo: 27.05\n",
            "Step 0: action=3, reward=1.789, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=3, reward=0.919, dead=False\n",
            "  Score cumulativo: 35.50\n",
            "Step 100: action=3, reward=0.919, dead=False\n",
            "  Score cumulativo: 81.50\n",
            "Step 150: action=3, reward=0.919, dead=False\n",
            "  Score cumulativo: 120.65\n",
            "Step 200: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 159.54\n",
            "Step 0: action=3, reward=1.891, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=3, reward=1.921, dead=False\n",
            "  Score cumulativo: 39.42\n",
            "Step 100: action=2, reward=0.951, dead=False\n",
            "  Score cumulativo: 109.67\n",
            "UPDATING WEIGHTS AT EPISODE =  373\n",
            "Step 150: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 105.46\n",
            "Step 200: action=0, reward=-0.130, dead=False\n",
            "  Score cumulativo: 99.98\n",
            "Step 0: action=3, reward=1.415, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=3, reward=0.732, dead=False\n",
            "  Score cumulativo: 22.96\n",
            "Step 100: action=0, reward=-0.130, dead=False\n",
            "  Score cumulativo: 54.04\n",
            "Step 150: action=0, reward=-0.107, dead=False\n",
            "  Score cumulativo: 51.17\n",
            "Step 200: action=0, reward=-0.107, dead=False\n",
            "  Score cumulativo: 45.65\n",
            "Step 0: action=3, reward=1.452, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=3, reward=0.751, dead=False\n",
            "  Score cumulativo: 30.95\n",
            "Step 100: action=2, reward=0.731, dead=False\n",
            "  Score cumulativo: 65.79\n",
            "UPDATING WEIGHTS AT EPISODE =  375\n",
            "Step 150: action=3, reward=0.020, dead=False\n",
            "  Score cumulativo: 74.64\n",
            "Step 200: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 94.02\n",
            "Step 0: action=3, reward=1.577, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=3, reward=0.814, dead=False\n",
            "  Score cumulativo: 25.59\n",
            "Step 100: action=4, reward=0.000, dead=False\n",
            "  Score cumulativo: 62.28\n",
            "Step 150: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 59.48\n",
            "Step 200: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 56.68\n",
            "Step 0: action=3, reward=1.685, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 16.18\n",
            "Step 100: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 13.18\n",
            "UPDATING WEIGHTS AT EPISODE =  377\n",
            "Step 150: action=4, reward=0.000, dead=False\n",
            "  Score cumulativo: 10.78\n",
            "Step 200: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 9.38\n",
            "Step 0: action=0, reward=1.736, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=0, reward=2.699, dead=False\n",
            "  Score cumulativo: 38.11\n",
            "Step 100: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 40.97\n",
            "Step 150: action=4, reward=0.000, dead=False\n",
            "  Score cumulativo: 38.87\n",
            "Step 200: action=4, reward=0.000, dead=False\n",
            "  Score cumulativo: 37.37\n",
            "Step 0: action=3, reward=1.877, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=4, reward=-0.000, dead=False\n",
            "  Score cumulativo: 16.11\n",
            "Step 100: action=2, reward=-0.000, dead=False\n",
            "  Score cumulativo: 13.41\n",
            "UPDATING WEIGHTS AT EPISODE =  379\n",
            "Step 150: action=4, reward=0.000, dead=False\n",
            "  Score cumulativo: 10.51\n",
            "Step 200: action=4, reward=0.000, dead=False\n",
            "  Score cumulativo: 8.31\n",
            "Step 0: action=3, reward=1.782, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=4, reward=-0.000, dead=False\n",
            "  Score cumulativo: 17.77\n",
            "Step 100: action=4, reward=-0.000, dead=False\n",
            "  Score cumulativo: 15.67\n",
            "Step 150: action=4, reward=0.000, dead=False\n",
            "  Score cumulativo: 13.27\n",
            "Step 200: action=4, reward=0.000, dead=False\n",
            "  Score cumulativo: 11.17\n",
            "Step 0: action=3, reward=1.870, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 17.95\n",
            "Step 100: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 14.99\n",
            "UPDATING WEIGHTS AT EPISODE =  381\n",
            "Step 150: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 10.59\n",
            "Step 200: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 6.29\n",
            "Step 0: action=3, reward=1.849, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=3, reward=0.949, dead=False\n",
            "  Score cumulativo: 29.04\n",
            "Step 100: action=4, reward=0.929, dead=False\n",
            "  Score cumulativo: 86.64\n",
            "Step 150: action=0, reward=0.829, dead=False\n",
            "  Score cumulativo: 121.01\n",
            "Step 200: action=0, reward=-0.108, dead=False\n",
            "  Score cumulativo: 122.60\n",
            "Step 0: action=3, reward=1.452, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=3, reward=0.751, dead=False\n",
            "  Score cumulativo: 22.87\n",
            "Step 100: action=3, reward=0.751, dead=False\n",
            "  Score cumulativo: 58.18\n",
            "UPDATING WEIGHTS AT EPISODE =  383\n",
            "-----------------------------------------\n",
            "SAVING AT EPISODE 383\n",
            "-----------------------------------------\n",
            "Step 150: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 94.71\n",
            "Step 200: action=4, reward=-0.000, dead=False\n",
            "  Score cumulativo: 91.71\n",
            "Step 0: action=3, reward=1.751, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=0, reward=-0.107, dead=False\n",
            "  Score cumulativo: 26.54\n",
            "Step 100: action=0, reward=-0.130, dead=False\n",
            "  Score cumulativo: 20.30\n",
            "Step 150: action=0, reward=-0.130, dead=False\n",
            "  Score cumulativo: 14.10\n",
            "Step 200: action=0, reward=-0.130, dead=False\n",
            "  Score cumulativo: 7.90\n",
            "Step 0: action=3, reward=1.815, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=3, reward=0.932, dead=False\n",
            "  Score cumulativo: 38.45\n",
            "Step 100: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 68.81\n",
            "UPDATING WEIGHTS AT EPISODE =  385\n",
            "Step 150: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 64.21\n",
            "Step 200: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 59.21\n",
            "Step 0: action=3, reward=1.835, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=1, reward=0.923, dead=False\n",
            "  Score cumulativo: 27.79\n",
            "Step 100: action=0, reward=-0.108, dead=False\n",
            "  Score cumulativo: 78.19\n",
            "Step 150: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 78.58\n",
            "Step 0: action=3, reward=1.782, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=2, reward=-0.000, dead=False\n",
            "  Score cumulativo: 19.76\n",
            "Step 100: action=2, reward=-0.000, dead=False\n",
            "  Score cumulativo: 19.76\n",
            "Step 150: action=2, reward=-0.000, dead=False\n",
            "  Score cumulativo: 19.76\n",
            "Step 200: action=2, reward=-0.000, dead=False\n",
            "  Score cumulativo: 19.76\n",
            "UPDATING WEIGHTS AT EPISODE =  387\n",
            "Step 0: action=3, reward=1.714, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=3, reward=0.020, dead=False\n",
            "  Score cumulativo: 26.67\n",
            "Step 100: action=0, reward=0.762, dead=False\n",
            "  Score cumulativo: 76.01\n",
            "Step 150: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 78.08\n",
            "Step 200: action=4, reward=0.000, dead=False\n",
            "  Score cumulativo: 74.98\n",
            "Step 0: action=3, reward=1.726, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 19.22\n",
            "Step 100: action=4, reward=-0.000, dead=False\n",
            "  Score cumulativo: 16.82\n",
            "Step 150: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 14.52\n",
            "Step 200: action=4, reward=0.000, dead=False\n",
            "  Score cumulativo: 12.22\n",
            "UPDATING WEIGHTS AT EPISODE =  389\n",
            "Step 0: action=3, reward=2.155, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=2, reward=0.000, dead=False\n",
            "  Score cumulativo: 26.31\n",
            "Step 100: action=4, reward=0.000, dead=False\n",
            "  Score cumulativo: 25.41\n",
            "Step 150: action=4, reward=-0.000, dead=False\n",
            "  Score cumulativo: 24.11\n",
            "Step 200: action=4, reward=-0.000, dead=False\n",
            "  Score cumulativo: 22.81\n",
            "Step 0: action=0, reward=1.688, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=2, reward=0.909, dead=False\n",
            "  Score cumulativo: 39.40\n",
            "Step 100: action=0, reward=-0.107, dead=False\n",
            "  Score cumulativo: 45.31\n",
            "Step 0: action=0, reward=1.894, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=2, reward=1.012, dead=False\n",
            "  Score cumulativo: 40.58\n",
            "UPDATING WEIGHTS AT EPISODE =  392\n",
            "Step 100: action=3, reward=1.032, dead=False\n",
            "  Score cumulativo: 109.74\n",
            "Step 150: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 122.91\n",
            "Step 200: action=0, reward=-0.130, dead=False\n",
            "  Score cumulativo: 117.32\n",
            "Step 0: action=3, reward=1.782, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=4, reward=-0.000, dead=False\n",
            "  Score cumulativo: 20.46\n",
            "Step 100: action=4, reward=-0.000, dead=False\n",
            "  Score cumulativo: 20.16\n",
            "Step 150: action=4, reward=-0.000, dead=False\n",
            "  Score cumulativo: 19.56\n",
            "Step 200: action=4, reward=-0.000, dead=False\n",
            "  Score cumulativo: 19.26\n",
            "Step 0: action=0, reward=1.637, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=3, reward=0.903, dead=False\n",
            "  Score cumulativo: 25.75\n",
            "UPDATING WEIGHTS AT EPISODE =  394\n",
            "-----------------------------------------\n",
            "SAVING AT EPISODE 394\n",
            "-----------------------------------------\n",
            "Step 100: action=3, reward=0.020, dead=False\n",
            "  Score cumulativo: 66.57\n",
            "Step 150: action=2, reward=0.883, dead=False\n",
            "  Score cumulativo: 113.87\n",
            "Step 200: action=2, reward=0.883, dead=False\n",
            "  Score cumulativo: 157.98\n",
            "Step 0: action=3, reward=1.708, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=3, reward=0.879, dead=False\n",
            "  Score cumulativo: 28.46\n",
            "Step 100: action=3, reward=1.738, dead=False\n",
            "  Score cumulativo: 83.70\n",
            "Step 150: action=0, reward=0.759, dead=False\n",
            "  Score cumulativo: 128.72\n",
            "Step 200: action=0, reward=0.759, dead=False\n",
            "  Score cumulativo: 157.43\n",
            "Step 0: action=3, reward=1.696, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=4, reward=-0.000, dead=False\n",
            "  Score cumulativo: 18.80\n",
            "UPDATING WEIGHTS AT EPISODE =  396\n",
            "Step 100: action=4, reward=-0.000, dead=False\n",
            "  Score cumulativo: 18.80\n",
            "Step 150: action=4, reward=0.000, dead=False\n",
            "  Score cumulativo: 18.80\n",
            "Step 200: action=4, reward=0.000, dead=False\n",
            "  Score cumulativo: 18.70\n",
            "Step 0: action=0, reward=1.682, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=1, reward=0.906, dead=False\n",
            "  Score cumulativo: 35.65\n",
            "Step 100: action=2, reward=0.906, dead=False\n",
            "  Score cumulativo: 73.61\n",
            "Step 150: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 74.33\n",
            "Step 200: action=0, reward=-0.107, dead=False\n",
            "  Score cumulativo: 68.75\n",
            "Step 0: action=3, reward=1.835, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=4, reward=-0.000, dead=False\n",
            "  Score cumulativo: 16.52\n",
            "UPDATING WEIGHTS AT EPISODE =  398\n",
            "Step 100: action=2, reward=-0.000, dead=False\n",
            "  Score cumulativo: 16.42\n",
            "Step 150: action=2, reward=0.000, dead=False\n",
            "  Score cumulativo: 16.42\n",
            "Step 200: action=2, reward=0.000, dead=False\n",
            "  Score cumulativo: 16.42\n",
            "Step 0: action=3, reward=1.802, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=3, reward=0.926, dead=False\n",
            "  Score cumulativo: 29.16\n",
            "Step 100: action=2, reward=-0.000, dead=False\n",
            "  Score cumulativo: 85.15\n",
            "Step 150: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 80.01\n",
            "Step 200: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 75.01\n",
            "Step 0: action=3, reward=1.782, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=3, reward=0.916, dead=False\n",
            "  Score cumulativo: 28.03\n",
            "UPDATING WEIGHTS AT EPISODE =  400\n",
            "Step 100: action=0, reward=-0.115, dead=False\n",
            "  Score cumulativo: 58.75\n",
            "Step 150: action=4, reward=0.000, dead=False\n",
            "  Score cumulativo: 62.38\n",
            "Step 200: action=2, reward=0.000, dead=False\n",
            "  Score cumulativo: 62.08\n",
            "Step 0: action=3, reward=1.708, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=2, reward=-0.000, dead=False\n",
            "  Score cumulativo: 15.75\n",
            "Step 100: action=2, reward=-0.000, dead=False\n",
            "  Score cumulativo: 15.75\n",
            "Step 150: action=2, reward=-0.000, dead=False\n",
            "  Score cumulativo: 15.75\n",
            "Step 200: action=2, reward=-0.000, dead=False\n",
            "  Score cumulativo: 15.75\n",
            "Step 0: action=0, reward=1.437, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=1, reward=0.784, dead=False\n",
            "  Score cumulativo: 32.30\n",
            "UPDATING WEIGHTS AT EPISODE =  402\n",
            "Step 100: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 32.72\n",
            "Step 150: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 27.58\n",
            "Step 200: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 22.10\n",
            "Step 0: action=3, reward=1.789, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=3, reward=0.919, dead=False\n",
            "  Score cumulativo: 39.98\n",
            "Step 100: action=0, reward=-0.108, dead=False\n",
            "  Score cumulativo: 86.83\n",
            "Step 150: action=0, reward=-0.108, dead=False\n",
            "  Score cumulativo: 81.29\n",
            "Step 200: action=2, reward=0.899, dead=False\n",
            "  Score cumulativo: 80.87\n",
            "Step 0: action=3, reward=1.514, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=3, reward=0.782, dead=False\n",
            "  Score cumulativo: 22.66\n",
            "UPDATING WEIGHTS AT EPISODE =  404\n",
            "Step 100: action=0, reward=0.662, dead=False\n",
            "  Score cumulativo: 61.05\n",
            "Step 150: action=0, reward=-0.115, dead=False\n",
            "  Score cumulativo: 63.64\n",
            "Step 200: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 61.17\n",
            "Step 0: action=3, reward=1.696, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=3, reward=0.873, dead=False\n",
            "  Score cumulativo: 26.48\n",
            "Step 100: action=0, reward=0.753, dead=False\n",
            "  Score cumulativo: 61.13\n",
            "Step 150: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 69.43\n",
            "Step 200: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 63.97\n",
            "Step 0: action=3, reward=1.691, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=0, reward=0.750, dead=False\n",
            "  Score cumulativo: 31.41\n",
            "UPDATING WEIGHTS AT EPISODE =  406\n",
            "-----------------------------------------\n",
            "SAVING AT EPISODE 406\n",
            "-----------------------------------------\n",
            "Step 100: action=3, reward=1.721, dead=False\n",
            "  Score cumulativo: 73.60\n",
            "Step 150: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 85.55\n",
            "Step 200: action=1, reward=-0.000, dead=False\n",
            "  Score cumulativo: 82.75\n",
            "Step 0: action=3, reward=1.884, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 22.69\n",
            "Step 100: action=0, reward=-0.130, dead=False\n",
            "  Score cumulativo: 16.85\n",
            "Step 150: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 11.08\n",
            "Step 200: action=4, reward=0.000, dead=False\n",
            "  Score cumulativo: 33.09\n",
            "Step 0: action=3, reward=1.582, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 16.97\n",
            "UPDATING WEIGHTS AT EPISODE =  408\n",
            "Step 100: action=2, reward=-0.000, dead=False\n",
            "  Score cumulativo: 15.15\n",
            "Step 150: action=3, reward=0.816, dead=False\n",
            "  Score cumulativo: 46.89\n",
            "Step 200: action=1, reward=0.796, dead=False\n",
            "  Score cumulativo: 80.17\n",
            "Step 0: action=3, reward=1.657, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=2, reward=0.000, dead=False\n",
            "  Score cumulativo: 33.87\n",
            "Step 100: action=0, reward=-0.130, dead=False\n",
            "  Score cumulativo: 29.55\n",
            "Step 150: action=0, reward=-0.130, dead=False\n",
            "  Score cumulativo: 23.79\n",
            "Step 0: action=3, reward=1.679, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=3, reward=0.865, dead=False\n",
            "  Score cumulativo: 26.21\n",
            "Step 100: action=0, reward=-0.130, dead=False\n",
            "  Score cumulativo: 66.45\n",
            "Step 150: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 63.66\n",
            "UPDATING WEIGHTS AT EPISODE =  410\n",
            "Step 200: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 59.66\n",
            "Step 0: action=3, reward=1.714, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=3, reward=0.020, dead=False\n",
            "  Score cumulativo: 26.65\n",
            "Step 100: action=2, reward=0.000, dead=False\n",
            "  Score cumulativo: 62.46\n",
            "Step 150: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 61.05\n",
            "Step 200: action=3, reward=0.882, dead=False\n",
            "  Score cumulativo: 77.75\n",
            "Step 0: action=0, reward=1.656, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=1, reward=0.000, dead=False\n",
            "  Score cumulativo: 34.92\n",
            "Step 100: action=1, reward=0.000, dead=False\n",
            "  Score cumulativo: 34.92\n",
            "Step 150: action=1, reward=0.000, dead=False\n",
            "  Score cumulativo: 34.92\n",
            "UPDATING WEIGHTS AT EPISODE =  412\n",
            "Step 200: action=4, reward=-0.000, dead=False\n",
            "  Score cumulativo: 32.92\n",
            "Step 0: action=3, reward=1.913, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=3, reward=0.982, dead=False\n",
            "  Score cumulativo: 28.66\n",
            "Step 100: action=0, reward=1.823, dead=False\n",
            "  Score cumulativo: 88.03\n",
            "Step 150: action=0, reward=0.862, dead=False\n",
            "  Score cumulativo: 129.54\n",
            "Step 200: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 163.29\n",
            "Step 0: action=3, reward=1.708, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 19.21\n",
            "Step 100: action=3, reward=0.879, dead=False\n",
            "  Score cumulativo: 25.62\n",
            "Step 150: action=1, reward=0.000, dead=False\n",
            "  Score cumulativo: 58.16\n",
            "UPDATING WEIGHTS AT EPISODE =  414\n",
            "Step 200: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 75.34\n",
            "Step 0: action=3, reward=1.795, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=2, reward=0.000, dead=False\n",
            "  Score cumulativo: 22.54\n",
            "Step 100: action=2, reward=-0.000, dead=False\n",
            "  Score cumulativo: 19.44\n",
            "Step 150: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 16.34\n",
            "Step 200: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 13.44\n",
            "Step 0: action=3, reward=1.776, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=3, reward=0.913, dead=False\n",
            "  Score cumulativo: 27.83\n",
            "Step 100: action=0, reward=0.793, dead=False\n",
            "  Score cumulativo: 84.38\n",
            "Step 150: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 101.26\n",
            "UPDATING WEIGHTS AT EPISODE =  416\n",
            "Step 200: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 96.36\n",
            "Step 0: action=0, reward=1.729, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=0, reward=0.829, dead=False\n",
            "  Score cumulativo: 36.55\n",
            "Step 100: action=0, reward=-0.107, dead=False\n",
            "  Score cumulativo: 40.80\n",
            "Step 0: action=3, reward=1.624, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=3, reward=0.837, dead=False\n",
            "  Score cumulativo: 23.12\n",
            "Step 100: action=2, reward=0.817, dead=False\n",
            "  Score cumulativo: 65.47\n",
            "Step 150: action=0, reward=-0.130, dead=False\n",
            "  Score cumulativo: 80.26\n",
            "Step 200: action=0, reward=-0.130, dead=False\n",
            "  Score cumulativo: 75.01\n",
            "Step 0: action=3, reward=1.842, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "UPDATING WEIGHTS AT EPISODE =  419\n",
            "-----------------------------------------\n",
            "SAVING AT EPISODE 419\n",
            "-----------------------------------------\n",
            "Step 50: action=0, reward=0.826, dead=False\n",
            "  Score cumulativo: 39.31\n",
            "Step 100: action=4, reward=0.000, dead=False\n",
            "  Score cumulativo: 41.09\n",
            "Step 150: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 40.69\n",
            "Step 200: action=4, reward=0.000, dead=False\n",
            "  Score cumulativo: 39.79\n",
            "Step 0: action=4, reward=1.676, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=0, reward=0.753, dead=False\n",
            "  Score cumulativo: 33.53\n",
            "Step 100: action=2, reward=0.000, dead=False\n",
            "  Score cumulativo: 40.92\n",
            "Step 150: action=2, reward=0.000, dead=False\n",
            "  Score cumulativo: 40.12\n",
            "Step 200: action=4, reward=0.000, dead=False\n",
            "  Score cumulativo: 39.22\n",
            "Step 0: action=3, reward=1.577, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "UPDATING WEIGHTS AT EPISODE =  421\n",
            "Step 50: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 13.19\n",
            "Step 100: action=3, reward=0.814, dead=False\n",
            "  Score cumulativo: 23.85\n",
            "Step 150: action=0, reward=-0.107, dead=False\n",
            "  Score cumulativo: 53.71\n",
            "Step 200: action=2, reward=0.000, dead=False\n",
            "  Score cumulativo: 51.85\n",
            "Step 0: action=3, reward=1.603, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=2, reward=0.806, dead=False\n",
            "  Score cumulativo: 20.66\n",
            "Step 100: action=0, reward=0.706, dead=False\n",
            "  Score cumulativo: 40.65\n",
            "Step 150: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 57.42\n",
            "Step 200: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 52.37\n",
            "Step 0: action=3, reward=1.822, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "UPDATING WEIGHTS AT EPISODE =  423\n",
            "Step 50: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 13.44\n",
            "Step 100: action=4, reward=-0.000, dead=False\n",
            "  Score cumulativo: 13.24\n",
            "Step 150: action=4, reward=-0.000, dead=False\n",
            "  Score cumulativo: 13.24\n",
            "Step 200: action=4, reward=-0.000, dead=False\n",
            "  Score cumulativo: 13.24\n",
            "Step 0: action=0, reward=1.394, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=2, reward=0.762, dead=False\n",
            "  Score cumulativo: 30.04\n",
            "Step 100: action=1, reward=0.000, dead=False\n",
            "  Score cumulativo: 38.64\n",
            "Step 150: action=3, reward=0.020, dead=False\n",
            "  Score cumulativo: 54.41\n",
            "Step 200: action=1, reward=0.000, dead=False\n",
            "  Score cumulativo: 70.68\n",
            "Step 0: action=3, reward=1.510, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "UPDATING WEIGHTS AT EPISODE =  425\n",
            "Step 50: action=0, reward=-0.130, dead=False\n",
            "  Score cumulativo: 20.34\n",
            "Step 100: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 15.34\n",
            "Step 150: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 10.34\n",
            "Step 200: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 5.54\n",
            "Step 0: action=3, reward=1.651, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=4, reward=0.000, dead=False\n",
            "  Score cumulativo: 28.76\n",
            "Step 100: action=4, reward=0.000, dead=False\n",
            "  Score cumulativo: 28.76\n",
            "Step 150: action=4, reward=-0.000, dead=False\n",
            "  Score cumulativo: 28.76\n",
            "Step 200: action=4, reward=-0.000, dead=False\n",
            "  Score cumulativo: 28.76\n",
            "Step 0: action=0, reward=1.418, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "UPDATING WEIGHTS AT EPISODE =  427\n",
            "Step 50: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 21.18\n",
            "Step 100: action=4, reward=-0.000, dead=False\n",
            "  Score cumulativo: 22.60\n",
            "Step 150: action=4, reward=-0.000, dead=False\n",
            "  Score cumulativo: 22.60\n",
            "Step 200: action=4, reward=0.000, dead=False\n",
            "  Score cumulativo: 22.60\n",
            "Step 0: action=3, reward=1.635, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=0, reward=0.722, dead=False\n",
            "  Score cumulativo: 29.58\n",
            "Step 100: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 49.99\n",
            "Step 150: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 44.55\n",
            "Step 200: action=1, reward=0.822, dead=False\n",
            "  Score cumulativo: 56.53\n",
            "Step 0: action=3, reward=1.587, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "UPDATING WEIGHTS AT EPISODE =  429\n",
            "Step 50: action=4, reward=-0.000, dead=False\n",
            "  Score cumulativo: 12.65\n",
            "Step 100: action=4, reward=-0.000, dead=False\n",
            "  Score cumulativo: 12.65\n",
            "Step 150: action=4, reward=-0.000, dead=False\n",
            "  Score cumulativo: 12.55\n",
            "Step 200: action=4, reward=-0.000, dead=False\n",
            "  Score cumulativo: 12.55\n",
            "Step 0: action=3, reward=1.519, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=0, reward=-0.130, dead=False\n",
            "  Score cumulativo: 24.01\n",
            "Step 100: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 18.04\n",
            "Step 150: action=2, reward=-0.000, dead=False\n",
            "  Score cumulativo: 15.54\n",
            "Step 200: action=2, reward=-0.000, dead=False\n",
            "  Score cumulativo: 15.54\n",
            "Step 0: action=3, reward=1.849, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "UPDATING WEIGHTS AT EPISODE =  431\n",
            "-----------------------------------------\n",
            "SAVING AT EPISODE 431\n",
            "-----------------------------------------\n",
            "Step 50: action=2, reward=0.000, dead=False\n",
            "  Score cumulativo: 39.36\n",
            "Step 100: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 34.25\n",
            "Step 150: action=0, reward=-0.130, dead=False\n",
            "  Score cumulativo: 28.45\n",
            "Step 200: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 23.48\n",
            "Step 0: action=3, reward=1.822, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 23.96\n",
            "Step 100: action=0, reward=-0.130, dead=False\n",
            "  Score cumulativo: 18.45\n",
            "Step 0: action=1, reward=1.857, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=4, reward=0.000, dead=False\n",
            "  Score cumulativo: 31.87\n",
            "Step 100: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 27.10\n",
            "Step 150: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 22.10\n",
            "UPDATING WEIGHTS AT EPISODE =  433\n",
            "Step 200: action=4, reward=-0.000, dead=False\n",
            "  Score cumulativo: 18.10\n",
            "Step 0: action=3, reward=1.640, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 22.45\n",
            "Step 100: action=4, reward=-0.000, dead=False\n",
            "  Score cumulativo: 18.95\n",
            "Step 150: action=4, reward=-0.000, dead=False\n",
            "  Score cumulativo: 16.15\n",
            "Step 200: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 12.75\n",
            "Step 0: action=3, reward=1.720, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=1, reward=0.865, dead=False\n",
            "  Score cumulativo: 33.51\n",
            "Step 100: action=4, reward=0.000, dead=False\n",
            "  Score cumulativo: 38.70\n",
            "Step 150: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 36.00\n",
            "UPDATING WEIGHTS AT EPISODE =  435\n",
            "Step 200: action=4, reward=0.000, dead=False\n",
            "  Score cumulativo: 34.20\n",
            "Step 0: action=3, reward=1.708, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=4, reward=-0.030, dead=False\n",
            "  Score cumulativo: 24.28\n",
            "Step 100: action=4, reward=-0.030, dead=False\n",
            "  Score cumulativo: 22.78\n",
            "Step 150: action=4, reward=-0.030, dead=False\n",
            "  Score cumulativo: 21.28\n",
            "Step 200: action=4, reward=-0.030, dead=False\n",
            "  Score cumulativo: 19.78\n",
            "Step 0: action=3, reward=1.691, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=0, reward=0.750, dead=False\n",
            "  Score cumulativo: 34.55\n",
            "Step 100: action=4, reward=0.000, dead=False\n",
            "  Score cumulativo: 38.96\n",
            "Step 150: action=4, reward=0.000, dead=False\n",
            "  Score cumulativo: 38.96\n",
            "UPDATING WEIGHTS AT EPISODE =  437\n",
            "Step 200: action=4, reward=0.000, dead=False\n",
            "  Score cumulativo: 38.96\n",
            "Step 0: action=3, reward=1.898, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=3, reward=1.928, dead=False\n",
            "  Score cumulativo: 41.42\n",
            "Step 100: action=0, reward=1.808, dead=False\n",
            "  Score cumulativo: 111.51\n",
            "Step 150: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 163.13\n",
            "Step 200: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 158.13\n",
            "Step 0: action=3, reward=1.696, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=0, reward=0.753, dead=False\n",
            "  Score cumulativo: 33.27\n",
            "Step 100: action=4, reward=0.000, dead=False\n",
            "  Score cumulativo: 57.07\n",
            "Step 150: action=4, reward=0.000, dead=False\n",
            "  Score cumulativo: 57.07\n",
            "UPDATING WEIGHTS AT EPISODE =  439\n",
            "Step 200: action=4, reward=0.000, dead=False\n",
            "  Score cumulativo: 57.07\n",
            "Step 0: action=0, reward=1.390, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=0, reward=1.420, dead=False\n",
            "  Score cumulativo: 28.73\n",
            "Step 100: action=2, reward=0.000, dead=False\n",
            "  Score cumulativo: 30.88\n",
            "Step 150: action=1, reward=0.000, dead=False\n",
            "  Score cumulativo: 30.18\n",
            "Step 200: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 28.38\n",
            "Step 0: action=3, reward=1.974, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=3, reward=1.012, dead=False\n",
            "  Score cumulativo: 42.67\n",
            "Step 100: action=0, reward=0.892, dead=False\n",
            "  Score cumulativo: 107.94\n",
            "Step 150: action=3, reward=1.012, dead=False\n",
            "  Score cumulativo: 172.38\n",
            "UPDATING WEIGHTS AT EPISODE =  441\n",
            "Step 200: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 185.95\n",
            "Step 0: action=3, reward=1.629, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=1, reward=0.820, dead=False\n",
            "  Score cumulativo: 31.94\n",
            "Step 100: action=3, reward=1.659, dead=False\n",
            "  Score cumulativo: 75.88\n",
            "Step 150: action=1, reward=1.639, dead=False\n",
            "  Score cumulativo: 136.75\n",
            "Step 200: action=4, reward=-0.030, dead=False\n",
            "  Score cumulativo: 159.02\n",
            "Step 0: action=3, reward=1.657, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=3, reward=0.853, dead=False\n",
            "  Score cumulativo: 35.98\n",
            "Step 100: action=3, reward=1.687, dead=False\n",
            "  Score cumulativo: 89.20\n",
            "Step 150: action=0, reward=0.733, dead=False\n",
            "  Score cumulativo: 125.26\n",
            "UPDATING WEIGHTS AT EPISODE =  443\n",
            "-----------------------------------------\n",
            "SAVING AT EPISODE 443\n",
            "-----------------------------------------\n",
            "Step 200: action=4, reward=-0.000, dead=False\n",
            "  Score cumulativo: 158.42\n",
            "Step 0: action=3, reward=1.776, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=3, reward=0.913, dead=False\n",
            "  Score cumulativo: 39.70\n",
            "Step 100: action=2, reward=0.893, dead=False\n",
            "  Score cumulativo: 94.49\n",
            "Step 150: action=4, reward=-0.030, dead=False\n",
            "  Score cumulativo: 100.68\n",
            "Step 200: action=4, reward=-0.030, dead=False\n",
            "  Score cumulativo: 96.78\n",
            "Step 0: action=3, reward=1.863, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=3, reward=0.956, dead=False\n",
            "  Score cumulativo: 39.58\n",
            "Step 100: action=0, reward=-0.130, dead=False\n",
            "  Score cumulativo: 90.36\n",
            "Step 150: action=4, reward=-0.030, dead=False\n",
            "  Score cumulativo: 86.56\n",
            "UPDATING WEIGHTS AT EPISODE =  445\n",
            "Step 200: action=0, reward=-0.130, dead=False\n",
            "  Score cumulativo: 81.46\n",
            "Step 0: action=3, reward=1.582, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=0, reward=0.696, dead=False\n",
            "  Score cumulativo: 29.46\n",
            "Step 100: action=4, reward=0.000, dead=False\n",
            "  Score cumulativo: 34.43\n",
            "Step 150: action=4, reward=0.000, dead=False\n",
            "  Score cumulativo: 34.43\n",
            "Step 200: action=4, reward=0.000, dead=False\n",
            "  Score cumulativo: 34.43\n",
            "Step 0: action=3, reward=1.524, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=0, reward=0.667, dead=False\n",
            "  Score cumulativo: 28.79\n",
            "Step 100: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 25.99\n",
            "Step 150: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 21.19\n",
            "UPDATING WEIGHTS AT EPISODE =  447\n",
            "Step 200: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 16.29\n",
            "Step 0: action=3, reward=1.835, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=3, reward=0.943, dead=False\n",
            "  Score cumulativo: 36.79\n",
            "Step 100: action=0, reward=-0.107, dead=False\n",
            "  Score cumulativo: 58.12\n",
            "Step 150: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 52.83\n",
            "Step 200: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 58.63\n",
            "Step 0: action=3, reward=1.835, dead=False\n",
            "  Score cumulativo: 0.00\n",
            "Step 50: action=3, reward=1.865, dead=False\n",
            "  Score cumulativo: 39.96\n",
            "Step 100: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 86.70\n",
            "Step 150: action=0, reward=-0.100, dead=False\n",
            "  Score cumulativo: 84.04\n",
            "UPDATING WEIGHTS AT EPISODE =  449\n",
            "Step 200: action=4, reward=-0.000, dead=False\n",
            "  Score cumulativo: 80.74\n",
            "Recording video for 1000 steps...\n",
            "Step 0: action=3, reward=6.394\n",
            "Step 100: action=3, reward=-0.100\n",
            "Step 200: action=1, reward=-0.100\n",
            "Step 300: action=0, reward=-0.100\n",
            "Step 400: action=4, reward=-0.100\n",
            "Step 500: action=3, reward=-0.100\n",
            "Step 600: action=1, reward=-0.100\n",
            "Step 700: action=3, reward=-0.100\n",
            "Step 800: action=4, reward=-0.100\n",
            "Step 900: action=0, reward=-0.100\n",
            "Episode ended at step 999\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Video saved to carracing_agent_test.mp4\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>episode_reward</td><td>▅▂▄▃▃▅▃▃▂▁▃▂▂▂▁▃▆▅▄▂▂▅█▂▅▃▃█▅▂▃▃▅▃▂▂▄▃▅▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>episode_reward</td><td>79.34089</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">run_310</strong> at: <a href='https://wandb.ai/vincenzo-civale-universi-degli-studi-di-firenze/CarRacing/runs/2i0e10r2' target=\"_blank\">https://wandb.ai/vincenzo-civale-universi-degli-studi-di-firenze/CarRacing/runs/2i0e10r2</a><br> View project at: <a href='https://wandb.ai/vincenzo-civale-universi-degli-studi-di-firenze/CarRacing' target=\"_blank\">https://wandb.ai/vincenzo-civale-universi-degli-studi-di-firenze/CarRacing</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250613_111719-2i0e10r2/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from src.CarRacing.agent import Agent\n",
        "from src.CarRacing.environment import Env\n",
        "from src.CarRacing.config import configure\n",
        "from src.CarRacing.run import RunManager\n",
        "\n",
        "\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "def record_video_standard_env(agent, filename=\"carracing_test.mp4\", max_steps=1000):\n",
        "    \"\"\"Record video usando l'environment gym standard\"\"\"\n",
        "    \n",
        "    # Crea environment standard con rendering\n",
        "    env_standard = gym.make(\"CarRacing-v3\", render_mode=\"rgb_array\", continuous=False)\n",
        "    \n",
        "    frames = []\n",
        "    state, _ = env_standard.reset()\n",
        "    \n",
        "    print(f\"Recording video for {max_steps} steps...\")\n",
        "    \n",
        "    for step in range(max_steps):\n",
        "        # Render frame\n",
        "        frame = env_standard.render()\n",
        "        frames.append(frame)\n",
        "        \n",
        "        # Converti lo state per il tuo agent (se necessario)\n",
        "        # Il tuo agent potrebbe aspettarsi un formato diverso\n",
        "        try:\n",
        "            with torch.no_grad():\n",
        "                action, _ = agent.select_action(state)\n",
        "        except:\n",
        "            # Se c'è incompatibilità, usa azioni casuali per il video\n",
        "            action = env_standard.action_space.sample()\n",
        "        \n",
        "        # Step environment\n",
        "        state, reward, terminated, truncated, _ = env_standard.step(action)\n",
        "        done = terminated or truncated\n",
        "        \n",
        "        if step % 100 == 0:\n",
        "            print(f\"Step {step}: action={action}, reward={reward:.3f}\")\n",
        "        \n",
        "        if done:\n",
        "            print(f\"Episode ended at step {step}\")\n",
        "            break\n",
        "    \n",
        "    env_standard.close()\n",
        "    \n",
        "    # Salva video\n",
        "    if frames:\n",
        "        height, width, _ = frames[0].shape\n",
        "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "        out = cv2.VideoWriter(filename, fourcc, 30.0, (width, height))\n",
        "        \n",
        "        for frame in frames:\n",
        "            frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
        "            out.write(frame_bgr)\n",
        "        \n",
        "        out.release()\n",
        "        print(f\"✅ Video saved to {filename}\")\n",
        "        return filename\n",
        "    else:\n",
        "        print(\"❌ No frames captured\")\n",
        "        return None\n",
        "\n",
        "\n",
        "\n",
        "configs, use_cuda,  device = configure()\n",
        "\n",
        "agent = Agent(configs.checkpoint, configs, device)\n",
        "env = Env(configs)\n",
        "\n",
        "run = RunManager(\n",
        "    project_name=\"CarRacing\",\n",
        "    run_name=f\"run_{configs.checkpoint}\",\n",
        "    config=configs,\n",
        "    device=device,\n",
        "    model=agent,\n",
        "    env=env,\n",
        "    output_dir=\"runs\",\n",
        "    save_every=100\n",
        ")\n",
        "\n",
        "max_timesteps = 500\n",
        "\n",
        "global_step = 0\n",
        "for episodeIndex in range(configs.checkpoint, 700):\n",
        "    score = 0\n",
        "    prevState = env.reset()\n",
        "    for t in range(max_timesteps):\n",
        "        action, a_logp = agent.select_action(prevState)\n",
        "        curState, reward, dead, reasonForDeath = env.step(action, t, agent)\n",
        "        \n",
        "        # Debug info ogni 50 step - PIU' DETTAGLIATO\n",
        "        if t % 50 == 0:\n",
        "            print(f\"Step {t}: action={action}, reward={reward:.3f}, dead={dead}\")\n",
        "            print(f\"  Score cumulativo: {score:.2f}\")\n",
        "            if hasattr(agent, 'get_action_probs'):\n",
        "                probs = agent.get_action_probs(prevState)\n",
        "                print(f\"  Action probabilities: {probs}\")\n",
        "        \n",
        "        if not configs.test:\n",
        "            loss, entropy = agent.update((prevState, action, a_logp, reward, curState), episodeIndex)\n",
        "            # Log training metrics ogni 100 step - SOLO quando disponibili\n",
        "            if t % 100 == 0 and loss is not None and entropy is not None:\n",
        "                print(f\"  Loss: {loss:.4f}, Entropy: {entropy:.4f}\")\n",
        "            global_step += 1\n",
        "        \n",
        "        score += reward\n",
        "        prevState = curState\n",
        "\n",
        "        if dead:\n",
        "            break\n",
        "\n",
        "    # Log episode reward con episode number\n",
        "    run.log_episode(episodeIndex, score)\n",
        "\n",
        "    if configs.test:\n",
        "        break\n",
        "\n",
        "# fine training\n",
        "if not configs.test:\n",
        "    run.save_model()\n",
        "    # run.record_video_standard_env()\n",
        "    video_file = record_video_standard_env(agent, \"carracing_agent_test.mp4\")\n",
        "    run.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8059e51c",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
