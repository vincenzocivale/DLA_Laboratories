{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f6ae2782-57ea-48f0-b948-b412d0076ffc",
      "metadata": {
        "id": "f6ae2782-57ea-48f0-b948-b412d0076ffc"
      },
      "source": [
        "# Deep Reinforcement Learning Laboratory\n",
        "\n",
        "In this laboratory session we will work on getting more advanced versions of Deep Reinforcement Learning algorithms up and running. Deep Reinforcement Learning is **hard**, and getting agents to stably train can be frustrating and requires quite a bit of subtlety in analysis of intermediate results. We will start by refactoring (a bit) my implementation of `REINFORCE` on the [Cartpole environment](https://gymnasium.farama.org/environments/classic_control/cart_pole/)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3Cvjww4Spf4j",
      "metadata": {
        "id": "3Cvjww4Spf4j"
      },
      "source": [
        "### Import the packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ZJfqVOJpyiW",
      "metadata": {
        "id": "3ZJfqVOJpyiW"
      },
      "outputs": [],
      "source": [
        "# %%capture\n",
        "# !apt install python-opengl\n",
        "# !apt install ffmpeg\n",
        "# !apt install xvfb\n",
        "# !pip install pyvirtualdisplay\n",
        "# !pip install pyglet==1.5.1\n",
        "# !pip install gym_pygame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7AYhIWIGprT5",
      "metadata": {
        "id": "7AYhIWIGprT5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "from collections import deque\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import imageio\n",
        "from PIL import Image\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "# Gym\n",
        "import gymnasium as gym\n",
        "\n",
        "import wandb\n",
        "from wandb.integration.sb3 import WandbCallback\n",
        "import copy\n",
        "import random\n",
        "import cv2\n",
        "\n",
        "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
        "from stable_baselines3.common.vec_env import VecTransposeImage, DummyVecEnv, VecFrameStack, VecNormalize, SubprocVecEnv\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.vec_env import VecFrameStack"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fadaf0c-f9b2-4680-8456-0eadb5eb8c2f",
      "metadata": {
        "id": "3fadaf0c-f9b2-4680-8456-0eadb5eb8c2f"
      },
      "source": [
        "## Exercise 1: Improving my `REINFORCE` Implementation (warm up)\n",
        "\n",
        "In this exercise we will refactor a bit and improve some aspects of my `REINFORCE` implementation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "riPqQhsEqhV7",
      "metadata": {
        "id": "riPqQhsEqhV7"
      },
      "source": [
        "### The CartPole environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "M58K8c372-kF",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M58K8c372-kF",
        "outputId": "2d045f4c-9add-47f8-c65f-21828572d32d"
      },
      "outputs": [],
      "source": [
        "# Virtual display\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython.display import Video, display\n",
        "\n",
        "virtual_display = Display(visible=0, size=(1400, 900))\n",
        "virtual_display.start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yPyG-LNTqZeq",
      "metadata": {
        "id": "yPyG-LNTqZeq"
      },
      "outputs": [],
      "source": [
        "env_id = \"CartPole-v1\"\n",
        "\n",
        "env = gym.make(env_id)\n",
        "\n",
        "eval_env = gym.make(env_id)\n",
        "\n",
        "s_size = env.observation_space.shape[0]\n",
        "a_size = env.action_space.n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5oZubromqv0e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5oZubromqv0e",
        "outputId": "0b55cd73-da04-48ca-f6dc-045b9b631f4a"
      },
      "outputs": [],
      "source": [
        "print(\"_____OBSERVATION SPACE_____ \\n\")\n",
        "print(\"The State Space is: \", s_size)\n",
        "print(\"Sample observation\", env.observation_space.sample()) # Get a random observation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RD7TPeGkqxva",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RD7TPeGkqxva",
        "outputId": "bb21487f-0333-4b09-d0e0-220d9bfcee45"
      },
      "outputs": [],
      "source": [
        "print(\"\\n _____ACTION SPACE_____ \\n\")\n",
        "print(\"The Action Space is: \", a_size)\n",
        "print(\"Action Space Sample\", env.action_space.sample()) # Take a random action"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "v1NcD7CPq7tn",
      "metadata": {
        "id": "v1NcD7CPq7tn"
      },
      "source": [
        "### Reinforce Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BzIc4ySKoHw0",
      "metadata": {
        "id": "BzIc4ySKoHw0"
      },
      "source": [
        "> ⚠️ **Disclaimer**  \n",
        "> L'implementazione riportata di seguito si basa su una combinazione di risorse pubblicamente disponibili e suggerite dal corso **Deep Reinforcement Learning with Hugging Face**. In particolare, il codice e i concetti sono stati ispirati e adattati da:\n",
        ">\n",
        "> - [Esempio ufficiale PyTorch REINFORCE](https://github.com/pytorch/examples/blob/main/reinforcement_learning/reinforce.py)  \n",
        "> - [Implementazione REINFORCE di Udacity](https://github.com/udacity/deep-reinforcement-learning/blob/master/reinforce/REINFORCE.ipynb)  \n",
        "> - [Pull Request di ottimizzazione di Chris1nexus](https://github.com/huggingface/deep-rl-class/pull/95)\n",
        ">\n",
        "> In particolare, l'efficiente calcolo del **reward-to-go** è stato adattato dal lavoro di [Chris1nexus](https://github.com/Chris1nexus), il cui codice e spiegazioni sono ben documentati nella [pull request](https://github.com/huggingface/deep-rl-class/pull/95) al corso Hugging Face.\n",
        ">\n",
        "> Queste fonti sono state selezionate tra quelle raccomandate nel modulo dedicato alla policy gradient del corso, e hanno fornito una base solida sia dal punto di vista teorico che implementativo.\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/reinforce.png\" alt=\"Reinforce\"/>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RlCw4IjCoBrz",
      "metadata": {
        "id": "RlCw4IjCoBrz"
      },
      "outputs": [],
      "source": [
        "class Policy(nn.Module):\n",
        "    def __init__(self, s_size, a_size, h_size):\n",
        "        super(Policy, self).__init__()\n",
        "        self.fc1 = nn.Linear(s_size, h_size)\n",
        "        self.fc2 = nn.Linear(h_size, a_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return F.softmax(x, dim=1)\n",
        "\n",
        "    def act(self, state):\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "        probs = self.forward(state).cpu()\n",
        "        m = Categorical(probs)\n",
        "        action = m.sample()\n",
        "        return action.item(), m.log_prob(action)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6T0UPJS_rWF9",
      "metadata": {
        "id": "6T0UPJS_rWF9"
      },
      "source": [
        "### Reinforce Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FleRr65QrZj3",
      "metadata": {
        "id": "FleRr65QrZj3"
      },
      "outputs": [],
      "source": [
        "class ReinforceTrainer:\n",
        "    def __init__(self, policy, optimizer, env, gamma=0.99, max_t=1000, project_name=None):\n",
        "        self.policy = policy\n",
        "        self.optimizer = optimizer\n",
        "        self.env = env\n",
        "        self.gamma = gamma\n",
        "        self.max_t = max_t\n",
        "        self.project_name = project_name\n",
        "\n",
        "        self.scores = []\n",
        "        self.scores_deque = deque(maxlen=100)\n",
        "        self.best_score = -float('inf')\n",
        "\n",
        "        if self.project_name:\n",
        "            wandb.init(project=self.project_name)\n",
        "\n",
        "    def compute_returns(self, rewards):\n",
        "        returns = deque(maxlen=self.max_t)\n",
        "        for t in reversed(range(len(rewards))):\n",
        "            R = rewards[t] + self.gamma * (returns[0] if returns else 0.0)\n",
        "            returns.appendleft(R)\n",
        "        returns = torch.tensor(returns)\n",
        "        eps = np.finfo(np.float32).eps.item()\n",
        "        return (returns - returns.mean()) / (returns.std() + eps)\n",
        "\n",
        "    def reinforce(self, num_episodes=500, print_every=50, save_path=\"best_reinforce.pt\"):\n",
        "        for i_episode in range(1, num_episodes + 1):\n",
        "            saved_log_probs = []\n",
        "            rewards = []\n",
        "            # Gymnasium reset returns a tuple (observation, info), we only need the observation\n",
        "            state, _ = self.env.reset()\n",
        "\n",
        "            for t in range(self.max_t):\n",
        "                action, log_prob = self.policy.act(state)\n",
        "                saved_log_probs.append(log_prob)\n",
        "                # Gymnasium step returns (observation, reward, terminated, truncated, info)\n",
        "                next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
        "                rewards.append(reward)\n",
        "                done = terminated or truncated # In Gymnasium, done is terminated or truncated\n",
        "                state = next_state # Update state for the next iteration\n",
        "                if done:\n",
        "                    break\n",
        "\n",
        "            total_reward = sum(rewards)\n",
        "            self.scores.append(total_reward)\n",
        "            self.scores_deque.append(total_reward)\n",
        "\n",
        "            returns = self.compute_returns(rewards)\n",
        "\n",
        "            policy_loss = [-log_prob * R for log_prob, R in zip(saved_log_probs, returns)]\n",
        "            policy_loss = torch.cat(policy_loss).sum()\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            policy_loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            avg_score = np.mean(self.scores_deque)\n",
        "\n",
        "            if self.project_name:\n",
        "                wandb.log({\n",
        "                    'episode': i_episode,\n",
        "                    'reward': total_reward,\n",
        "                    'avg_reward': avg_score\n",
        "                })\n",
        "\n",
        "            if avg_score > self.best_score:\n",
        "                self.best_score = avg_score\n",
        "                torch.save(self.policy.state_dict(), save_path)\n",
        "                if self.project_name:\n",
        "                    wandb.run.summary[\"best_avg_reward\"] = avg_score\n",
        "\n",
        "            if i_episode % print_every == 0:\n",
        "                print(f\"Episode {i_episode}\\tAverage Score: {avg_score:.2f}\")\n",
        "\n",
        "        self.plot_rewards(self.scores) # Use self.scores which is accumulated over all episodes\n",
        "\n",
        "        return self.scores\n",
        "\n",
        "    def plot_rewards(self, reward_list, window=100):\n",
        "          plt.figure(figsize=(12, 6))\n",
        "          plt.plot(reward_list, label='Reward per Episode')\n",
        "\n",
        "          if len(reward_list) >= window:\n",
        "              moving_avg = np.convolve(reward_list, np.ones(window)/window, mode='valid')\n",
        "              plt.plot(range(window - 1, len(reward_list)), moving_avg, label=f'{window}-Episode Moving Avg', linewidth=2)\n",
        "\n",
        "          plt.xlabel('Episode')\n",
        "          plt.ylabel('Reward')\n",
        "          plt.title('Training Progress')\n",
        "          plt.legend()\n",
        "          plt.grid(True)\n",
        "          plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aEpVADzW2s-T",
      "metadata": {
        "id": "aEpVADzW2s-T"
      },
      "outputs": [],
      "source": [
        "def evaluate_agent(env, max_steps, n_eval_episodes, policy):\n",
        "    \"\"\"\n",
        "    Evaluate the agent for `n_eval_episodes` episodes and return the average and standard deviation of rewards.\n",
        "\n",
        "    :param env: The evaluation environment (gym.Env)\n",
        "    :param max_steps: Maximum number of steps per episode\n",
        "    :param n_eval_episodes: Number of episodes to evaluate the agent\n",
        "    :param policy: The Reinforce policy with an `act(state)` method\n",
        "    :return: Tuple of (mean_reward, std_reward)\n",
        "    \"\"\"\n",
        "    episode_rewards = []\n",
        "\n",
        "    for episode in range(n_eval_episodes):\n",
        "        state, _ = env.reset()  # Gymnasium reset returns (obs, info)\n",
        "        total_rewards_ep = 0\n",
        "\n",
        "        for step in range(max_steps):\n",
        "            action, _ = policy.act(state)\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            total_rewards_ep += reward\n",
        "            done = terminated or truncated\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "            state = next_state\n",
        "\n",
        "        episode_rewards.append(total_rewards_ep)\n",
        "\n",
        "    mean_reward = np.mean(episode_rewards)\n",
        "    std_reward = np.std(episode_rewards)\n",
        "\n",
        "    return mean_reward, std_reward\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ql0TeZuSuDeQ",
      "metadata": {
        "id": "ql0TeZuSuDeQ"
      },
      "outputs": [],
      "source": [
        "def record_video(env, policy, out_path, fps=30):\n",
        "    \"\"\"\n",
        "    Record a video of an agent acting in an environment.\n",
        "    :param env: the environment with render_mode='rgb_array'\n",
        "    :param policy: the agent, must implement policy.act(state)\n",
        "    :param out_path: full path to output .mp4 or .gif\n",
        "    :param fps: frames per second\n",
        "    \"\"\"\n",
        "    images = []\n",
        "    state, _ = env.reset()\n",
        "    done = False\n",
        "\n",
        "    img = env.render()\n",
        "    images.append(img)\n",
        "\n",
        "    while not done:\n",
        "        action, _ = policy.act(state)\n",
        "        state, reward, terminated, truncated, _ = env.step(action)\n",
        "        done = terminated or truncated\n",
        "        img = env.render()\n",
        "        images.append(img)\n",
        "\n",
        "    imageio.mimsave(out_path, [np.array(img) for img in images], fps=fps)\n",
        "    print(f\"Video saved to {out_path}\")\n",
        "    display(Video(out_path, embed=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nRL2ik9lw_Tx",
      "metadata": {
        "id": "nRL2ik9lw_Tx"
      },
      "outputs": [],
      "source": [
        "cartpole_hyperparameters = {\n",
        "    \"h_size\": 16,\n",
        "    \"n_training_episodes\": 600,\n",
        "    \"n_evaluation_episodes\": 10,\n",
        "    \"max_t\": 1000,\n",
        "    \"gamma\": 1.0,\n",
        "    \"lr\": 1e-2,\n",
        "    \"env_id\": env_id,\n",
        "    \"state_space\": s_size,\n",
        "    \"action_space\": a_size,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kppKkmSsrMnY",
      "metadata": {
        "id": "kppKkmSsrMnY"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88smzENVuF4i",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "88smzENVuF4i",
        "outputId": "f650f342-ed62-416d-e5e2-f1554247bad9"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.n\n",
        "policy = Policy(cartpole_hyperparameters[\"state_space\"], cartpole_hyperparameters[\"action_space\"], cartpole_hyperparameters[\"h_size\"]).to(device)\n",
        "optimizer = optim.Adam(policy.parameters(), lr=1e-2)\n",
        "\n",
        "trainer = ReinforceTrainer(policy, optimizer, env, gamma=0.99, max_t=cartpole_hyperparameters['max_t'])\n",
        "\n",
        "scores = trainer.reinforce(cartpole_hyperparameters['n_training_episodes'], print_every=10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BvakuGwIVnES",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "id": "BvakuGwIVnES",
        "outputId": "3e03b787-9a48-4c28-8f29-4abe8e83b930"
      },
      "outputs": [],
      "source": [
        "record_video(env, policy, \"Cart_Pole.mp4\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a5adad7-759b-4000-925b-701f41fe6e97",
      "metadata": {
        "id": "2a5adad7-759b-4000-925b-701f41fe6e97"
      },
      "source": [
        "-----\n",
        "## Exercise 2: `REINFORCE` with a Value Baseline (warm up)\n",
        "\n",
        "\n",
        "In questa variante dell’algoritmo REINFORCE, invece di aggiornare la policy solo in base al ritorno totale \\( G_t \\), si introduce una **baseline** per ridurre la varianza della stima del gradiente. Una scelta comune è usare una **value function** \\( V(s_t) \\), appresa tramite regressione sui ritorni osservati.\n",
        "\n",
        "L’aggiornamento della policy diventa:\n",
        "\n",
        "\\[\n",
        "\\theta \\leftarrow \\theta + \\alpha \\, (G_t - V(s_t)) \\, \\nabla_\\theta \\log \\pi(a_t | s_t)\n",
        "\\]\n",
        "\n",
        "Dove:\n",
        "- \\( G_t \\) è il ritorno cumulato dall’istante \\( t \\)\n",
        "- \\( V(s_t) \\) è il valore stimato dello stato corrente\n",
        "- \\( A_t = G_t - V(s_t) \\) è il **vantaggio**: quanto il risultato effettivo è migliore del previsto\n",
        "\n",
        "In pratica:\n",
        "- Addestriamo una seconda rete (`value_net`) per approssimare \\( V(s) \\)\n",
        "- Usiamo \\( A_t \\) come pesatura per il gradiente della policy (invece di usare direttamente \\( G_t \\))\n",
        "- Questo riduce la varianza dell’aggiornamento, stabilizzando il training\n",
        "\n",
        "**Nota:** La rete del valore viene ottimizzata separatamente minimizzando l’errore quadratico tra \\( V(s_t) \\) e \\( G_t \\).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Vzk_vGwhYatv",
      "metadata": {
        "id": "Vzk_vGwhYatv"
      },
      "outputs": [],
      "source": [
        "class ValueNetwork(nn.Module):\n",
        "    def __init__(self, state_size, hidden_size=128):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(state_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, state):\n",
        "      return self.net(state)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8b8cf76-5d39-45e9-8d7d-324125c04b4a",
      "metadata": {
        "id": "d8b8cf76-5d39-45e9-8d7d-324125c04b4a"
      },
      "outputs": [],
      "source": [
        "class ReinforceWithBaselineTrainer(ReinforceTrainer):\n",
        "    def __init__(self, policy, value_network, policy_optimizer, value_optimizer,\n",
        "                 env, gamma=0.99, max_t=1000, project_name=None):\n",
        "        super().__init__(policy, policy_optimizer, env, gamma, max_t, project_name)\n",
        "        self.value_network = value_network\n",
        "        self.value_optimizer = value_optimizer\n",
        "\n",
        "    def reinforce(self, num_episodes=500, print_every=50, save_path=\"best_reinforce_baseline.pt\"):\n",
        "        for i_episode in range(1, num_episodes + 1):\n",
        "            saved_log_probs = []\n",
        "            rewards = []\n",
        "            states = []\n",
        "\n",
        "            state, _ = self.env.reset()\n",
        "\n",
        "            for t in range(self.max_t):\n",
        "                action, log_prob = self.policy.act(state)\n",
        "                saved_log_probs.append(log_prob)\n",
        "                states.append(state)\n",
        "                next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
        "                rewards.append(reward)\n",
        "                done = terminated or truncated\n",
        "                state = next_state\n",
        "                if done:\n",
        "                    break\n",
        "\n",
        "            returns = self.compute_returns(rewards)\n",
        "            total_reward = sum(rewards)\n",
        "            self.scores.append(total_reward)\n",
        "            self.scores_deque.append(total_reward)\n",
        "\n",
        "            policy_losses = []\n",
        "            value_losses = []\n",
        "            for log_prob, R, state in zip(saved_log_probs, returns, states):\n",
        "                state_tensor = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "                baseline = self.value_network(state_tensor)\n",
        "                advantage = R - baseline.item()\n",
        "                policy_losses.append(-log_prob * advantage)\n",
        "                value_losses.append(nn.functional.mse_loss(baseline.squeeze(), torch.tensor(R).to(device)))\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            policy_loss = torch.stack(policy_losses).sum()\n",
        "            policy_loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            self.value_optimizer.zero_grad()\n",
        "            value_loss = torch.stack(value_losses).sum()\n",
        "            value_loss.backward()\n",
        "            self.value_optimizer.step()\n",
        "\n",
        "            avg_score = np.mean(self.scores_deque)\n",
        "\n",
        "            if self.project_name:\n",
        "                wandb.log({\n",
        "                    'episode': i_episode,\n",
        "                    'reward': total_reward,\n",
        "                    'avg_reward': avg_score,\n",
        "                    'policy_loss': policy_loss.item(),\n",
        "                    'value_loss': value_loss.item()\n",
        "                })\n",
        "\n",
        "            if avg_score > self.best_score:\n",
        "                self.best_score = avg_score\n",
        "                torch.save(self.policy.state_dict(), save_path)\n",
        "                if self.project_name:\n",
        "                    wandb.run.summary[\"best_avg_reward\"] = avg_score\n",
        "\n",
        "            if i_episode % print_every == 0:\n",
        "                print(f\"Episode {i_episode}\\tAverage Score: {avg_score:.2f}\")\n",
        "\n",
        "        self.plot_rewards(self.scores)\n",
        "        return self.scores\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8cdd5bc3-a439-4d84-891b-f4a840331e07",
      "metadata": {
        "id": "8cdd5bc3-a439-4d84-891b-f4a840331e07"
      },
      "source": [
        "**First Things First**: Recall from the slides on Deep Reinforcement Learning that we can **subtract** any function that doesn't depend on the current action from the q-value without changing the (maximum of our) objecttive function $J$:  \n",
        "\n",
        "$$ \\nabla J(\\boldsymbol{\\theta}) \\propto \\sum_{s} \\mu(s) \\sum_a \\left( q_{\\pi}(s, a) - b(s) \\right) \\nabla \\pi(a \\mid s, \\boldsymbol{\\theta}) $$\n",
        "\n",
        "In `REINFORCE` this means we can subtract from our target $G_t$:\n",
        "\n",
        "$$ \\boldsymbol{\\theta}_{t+1} \\triangleq \\boldsymbol{\\theta}_t + \\alpha (G_t - b(S_t)) \\frac{\\nabla \\pi(A_t \\mid s, \\boldsymbol{\\theta})}{\\pi(A_t \\mid s, \\boldsymbol{\\theta})} $$\n",
        "\n",
        "Since we are only interested in the **maximum** of our objective, we can also **rescale** our target by any function that also doesn't depend on the action. A **simple baseline** which is even independent of the state -- that is, it is **constant** for each episode -- is to just **standardize rewards within the episode**. So, we **subtract** the average return and **divide** by the variance of returns:\n",
        "\n",
        "$$ \\boldsymbol{\\theta}_{t+1} \\triangleq \\boldsymbol{\\theta}_t + \\alpha \\left(\\frac{G_t - \\bar{G}}{\\sigma_G}\\right) \\nabla  \\pi(A_t \\mid s, \\boldsymbol{\\theta}) $$\n",
        "\n",
        "This baseline is **already** implemented in my implementation of `REINFORCE`. Experiment with and without this standardization baseline and compare the performance. We are going to do something more interesting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "faf5b245-9369-4ed9-b5f4-3a52bb975d3f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "faf5b245-9369-4ed9-b5f4-3a52bb975d3f",
        "outputId": "dc0f3e5f-4673-4892-b7a6-942a520cd571"
      },
      "outputs": [],
      "source": [
        "seed = 123\n",
        "state, _ = env.reset(seed=seed)\n",
        "env.action_space.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "\n",
        "state_size = env.observation_space.shape[0]\n",
        "action_size = env.action_space.n\n",
        "hidden_size = 128\n",
        "\n",
        "policy1 = Policy(cartpole_hyperparameters[\"state_space\"], cartpole_hyperparameters[\"action_space\"], cartpole_hyperparameters[\"h_size\"]).to(device)\n",
        "policy2 = copy.deepcopy(policy1)\n",
        "\n",
        "policy_optimizer1 = torch.optim.Adam(policy1.parameters(), lr=1e-2)\n",
        "policy_optimizer2 = torch.optim.Adam(policy2.parameters(), lr=1e-2)\n",
        "\n",
        "# Istanzia rete valore + ottimizzatore per baseline trainer\n",
        "value_net = ValueNetwork(state_size, hidden_size).to(device)\n",
        "value_optimizer = torch.optim.Adam(value_net.parameters(), lr=1e-2)\n",
        "\n",
        "\n",
        "trainer_no_baseline = ReinforceTrainer(policy1, policy_optimizer1, env, gamma=0.99, max_t=1000)\n",
        "trainer_with_baseline = ReinforceWithBaselineTrainer(policy2, value_net, policy_optimizer2, value_optimizer,\n",
        "                                                    env, gamma=0.99, max_t=1000)\n",
        "\n",
        "\n",
        "scores_no_baseline = trainer_no_baseline.reinforce(cartpole_hyperparameters['n_training_episodes'], print_every=10),\n",
        "scores_with_baseline = trainer_with_baseline.reinforce(cartpole_hyperparameters['n_training_episodes'], print_every=10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46cfde8e-f1c7-4d96-85e0-268f687e09df",
      "metadata": {
        "id": "46cfde8e-f1c7-4d96-85e0-268f687e09df"
      },
      "source": [
        "**The Real Exercise**: Standard practice is to use the state-value function $v(s)$ as a baseline. This is intuitively appealing -- we are more interested in updating out policy for returns that estimate the current **value** worse. Our new update becomes:\n",
        "\n",
        "$$ \\boldsymbol{\\theta}_{t+1} \\triangleq \\boldsymbol{\\theta}_t + \\alpha (G_t - \\tilde{v}(S_t \\mid \\mathbf{w})) \\frac{\\nabla \\pi(A_t \\mid s, \\boldsymbol{\\theta})}{\\pi(A_t \\mid s, \\boldsymbol{\\theta})} $$\n",
        "\n",
        "where $\\tilde{v}(s \\mid \\mathbf{w})$ is a **deep neural network** with parameters $w$ that estimates $v_\\pi(s)$. What neural network? Typically, we use the **same** network architecture as that of the Policy.\n",
        "\n",
        "**Your Task**: Modify your implementation to fit a second, baseline network to estimate the value function and use it as **baseline**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4de3a2a8-05c3-44e2-9858-8cc8f431325f",
      "metadata": {
        "id": "4de3a2a8-05c3-44e2-9858-8cc8f431325f"
      },
      "outputs": [],
      "source": [
        "# Your code here."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64bf1447-d222-4b24-a357-5b7f9824390c",
      "metadata": {
        "id": "64bf1447-d222-4b24-a357-5b7f9824390c"
      },
      "source": [
        "-----\n",
        "## Exercise 3: Going Deeper\n",
        "\n",
        "As usual, pick **AT LEAST ONE** of the following exercises to complete.\n",
        "\n",
        "### Exercise 3.1: Solving Lunar Lander with `REINFORCE` (easy)\n",
        "\n",
        "Use my (or even better, improve on my) implementation of `REINFORCE` to solve the [Lunar Lander Environment](https://gymnasium.farama.org/environments/box2d/lunar_lander/). This environment is a little bit harder than Cartpole, but not much. Make sure you perform the same types of analyses we did during the lab session to quantify and qualify the performance of your agents.\n",
        "\n",
        "### Exercise 3.2: Solving Cartpole and Lunar Lander with `Deep Q-Learning` (harder)\n",
        "\n",
        "On policy Deep Reinforcement Learning tends to be **very unstable**. Write an implementation (or adapt an existing one) of `Deep Q-Learning` to solve our two environments (Cartpole and Lunar Lander). To do this you will need to implement a **Replay Buffer** and use a second, slow-moving **target Q-Network** to stabilize learning.\n",
        "\n",
        "### Exercise 3.3: Solving the OpenAI CarRacing environment (hardest)\n",
        "\n",
        "Use `Deep Q-Learning` -- or even better, an off-the-shelf implementation of **Proximal Policy Optimization (PPO)** -- to train an agent to solve the [OpenAI CarRacing](https://github.com/andywu0913/OpenAI-GYM-CarRacing-DQN) environment. This will be the most *fun*, but also the most *difficult*. Some tips:\n",
        "\n",
        "1. Make sure you use the `continuous=False` argument to the environment constructor. This ensures that the action space is **discrete** (we haven't seen how to work with continuous action spaces).\n",
        "2. Your Q-Network will need to be a CNN. A simple one should do, with two convolutional + maxpool layers, folowed by a two dense layers. You will **definitely** want to use a GPU to train your agents.\n",
        "3. The observation space of the environment is a single **color image** (a single frame of the game). Most implementations stack multiple frames (e.g. 3) after converting them to grayscale images as an observation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5wbp-6StptyZ",
      "metadata": {
        "id": "5wbp-6StptyZ"
      },
      "source": [
        "#### Modifiche all'Ambiente e Preprocessamento delle Osservazioni"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fc18dda",
      "metadata": {},
      "outputs": [],
      "source": [
        "class CarRacingPreprocessing(gym.ObservationWrapper):\n",
        "    def __init__(self, env, height=84):\n",
        "        super(CarRacingPreprocessing, self).__init__(env)\n",
        "        self.height = height\n",
        "        # IMPORTANTE: observation space per scala di grigi, normalizzato a float32 0-1\n",
        "        # L'output sarà (height, width, 1) per il wrapper, ma VecTransposeImage lo trasformerà in (1, height, width)\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=0.0, high=1.0, shape=(self.height, 96, 1), dtype=np.float32 # Modificato dtype e low/high\n",
        "        )\n",
        "\n",
        "    def observation(self, obs):\n",
        "        # Converti in scala di grigi\n",
        "        gray = cv2.cvtColor(obs, cv2.COLOR_RGB2GRAY)\n",
        "        # Crop dalla parte superiore (rimuove dashboard)\n",
        "        gray = gray[:self.height, :]\n",
        "        # Aggiungi dimensione canale\n",
        "        gray = np.expand_dims(gray, axis=-1)\n",
        "        # Normalizzazione: Cruciale per reti neurali\n",
        "        gray = gray.astype(np.float32) / 255.0\n",
        "        return gray"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1223db48",
      "metadata": {},
      "source": [
        "#### Features Extractor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1952bb92",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class CNN(BaseFeaturesExtractor):\n",
        "    def __init__(self, observation_space: gym.spaces.Box, features_dim: int = 512):\n",
        "        super(CNN, self).__init__(observation_space, features_dim)\n",
        "\n",
        "        n_input_channels = observation_space.shape[0] # Questo sarà 3 se n_stack=3\n",
        "\n",
        "        self.cnn = nn.Sequential(\n",
        "            # Layer 1: Kernel più piccolo, stride 2, con padding per preservare i bordi\n",
        "            # Input: (C, 84, 96) -> Output: (32, 42, 48) circa\n",
        "            nn.Conv2d(n_input_channels, 32, kernel_size=5, stride=2, padding=1), # Modificato kernel_size e aggiunto padding\n",
        "            nn.ReLU(),\n",
        "            # Layer 2: Input: (32, 42, 48) -> Output: (64, 21, 24) circa\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1), # Modificato kernel_size e aggiunto padding\n",
        "            nn.ReLU(),\n",
        "            # Layer 3: Input: (64, 21, 24) -> Output: (64, 11, 12) circa\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1), # Modificato kernel_size e aggiunto padding\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten()\n",
        "        )\n",
        "\n",
        "        with torch.no_grad():\n",
        "            sample_input = torch.as_tensor(observation_space.sample()[None]).float()\n",
        "            # Se la forma è (H, W, C), PyTorch Conv2d si aspetta (C, H, W)\n",
        "            # In questo caso VecFrameStack già output (C, H, W)\n",
        "            # Quindi sample_input sarà (1, n_stack, H, W)\n",
        "            n_flatten = self.cnn(sample_input).shape[1]\n",
        "\n",
        "        self.linear = nn.Sequential(\n",
        "            nn.Linear(n_flatten, features_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, observations: torch.Tensor) -> torch.Tensor:\n",
        "        return self.linear(self.cnn(observations))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "3f10948f",
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_env():\n",
        "    def _init():\n",
        "        try:\n",
        "            env = gym.make(\"CarRacing-v3\", render_mode=\"rgb_array\", continuous=False)\n",
        "            env = CarRacingPreprocessing(env, height=84)\n",
        "            return env\n",
        "        except Exception as e:\n",
        "            print(f\"Error initializing environment: {e}\")\n",
        "            raise\n",
        "    return _init\n",
        "\n",
        "\n",
        "n_envs = 8\n",
        "env = SubprocVecEnv([make_env() for _ in range(n_envs)])\n",
        "env = VecFrameStack(env, n_stack=4)\n",
        "env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_reward=10.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77beb0c0",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvincenzo-civale\u001b[0m (\u001b[33mvincenzo-civale-universi-degli-studi-di-firenze\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/vcivale/prova/Esercitazione_2/wandb/run-20250607_151741-o8x6psow</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/vincenzo-civale-universi-degli-studi-di-firenze/ppo-carracing/runs/o8x6psow' target=\"_blank\">olive-meadow-7</a></strong> to <a href='https://wandb.ai/vincenzo-civale-universi-degli-studi-di-firenze/ppo-carracing' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/vincenzo-civale-universi-degli-studi-di-firenze/ppo-carracing' target=\"_blank\">https://wandb.ai/vincenzo-civale-universi-degli-studi-di-firenze/ppo-carracing</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/vincenzo-civale-universi-degli-studi-di-firenze/ppo-carracing/runs/o8x6psow' target=\"_blank\">https://wandb.ai/vincenzo-civale-universi-degli-studi-di-firenze/ppo-carracing/runs/o8x6psow</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda device\n"
          ]
        }
      ],
      "source": [
        "# Inizializza W&B\n",
        "wandb.init(\n",
        "    project=\"ppo-carracing\",\n",
        "    config={\n",
        "        \"policy_type\": \"CnnPolicy\",\n",
        "        \"total_timesteps\": 1_000_000,\n",
        "        \"learning_rate\": 5e-4,\n",
        "        \"n_steps\": 2048,\n",
        "        \"batch_size\": 64,\n",
        "        \"n_epochs\": 10,\n",
        "        \"gamma\": 0.99,\n",
        "        \"gae_lambda\": 0.95,\n",
        "        \"clip_range\": 0.2,\n",
        "        \"ent_coef\": 0.05,\n",
        "    },\n",
        "    sync_tensorboard=True,  # sincronizza TB e W&B\n",
        "    monitor_gym=True,       # logga anche l’ambiente se possibile\n",
        "    save_code=True,\n",
        ")\n",
        "\n",
        "# Policy kwargs\n",
        "policy_kwargs = dict(\n",
        "    features_extractor_class=CNN,\n",
        "    features_extractor_kwargs=dict(features_dim=256),\n",
        ")\n",
        "\n",
        "# Inizializza il modello\n",
        "model = PPO(\n",
        "    \"CnnPolicy\",\n",
        "    env,\n",
        "    policy_kwargs=policy_kwargs,\n",
        "    learning_rate=1e-4,\n",
        "    n_steps=2048,\n",
        "    batch_size=64,\n",
        "    n_epochs=10,\n",
        "    gamma=0.99,\n",
        "    gae_lambda=0.95,\n",
        "    clip_range=0.2,\n",
        "    ent_coef=0.04,\n",
        "    verbose=1,\n",
        "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "695dd84e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training...\n",
            "Logging to ./ppo_tb/PPO_1\n",
            "------------------------------\n",
            "| time/              |       |\n",
            "|    fps             | 170   |\n",
            "|    iterations      | 1     |\n",
            "|    time_elapsed    | 96    |\n",
            "|    total_timesteps | 16384 |\n",
            "------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 140         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 233         |\n",
            "|    total_timesteps      | 32768       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010232618 |\n",
            "|    clip_fraction        | 0.0877      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.6        |\n",
            "|    explained_variance   | 0.0489      |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.0721     |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.00944    |\n",
            "|    value_loss           | 0.0587      |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 133         |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 368         |\n",
            "|    total_timesteps      | 49152       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013271993 |\n",
            "|    clip_fraction        | 0.171       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.58       |\n",
            "|    explained_variance   | 0.488       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.0846     |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.0198     |\n",
            "|    value_loss           | 0.0222      |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 129         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 505         |\n",
            "|    total_timesteps      | 65536       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015632464 |\n",
            "|    clip_fraction        | 0.193       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.54       |\n",
            "|    explained_variance   | 0.465       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.0674     |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.0253     |\n",
            "|    value_loss           | 0.0304      |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 128         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 639         |\n",
            "|    total_timesteps      | 81920       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.019365665 |\n",
            "|    clip_fraction        | 0.208       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.5        |\n",
            "|    explained_variance   | 0.627       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.0924     |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.0286     |\n",
            "|    value_loss           | 0.0325      |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 127         |\n",
            "|    iterations           | 6           |\n",
            "|    time_elapsed         | 773         |\n",
            "|    total_timesteps      | 98304       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.019949501 |\n",
            "|    clip_fraction        | 0.213       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.49       |\n",
            "|    explained_variance   | 0.716       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.0875     |\n",
            "|    n_updates            | 50          |\n",
            "|    policy_gradient_loss | -0.026      |\n",
            "|    value_loss           | 0.0285      |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 126         |\n",
            "|    iterations           | 7           |\n",
            "|    time_elapsed         | 908         |\n",
            "|    total_timesteps      | 114688      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.020756524 |\n",
            "|    clip_fraction        | 0.21        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.48       |\n",
            "|    explained_variance   | 0.86        |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.0982     |\n",
            "|    n_updates            | 60          |\n",
            "|    policy_gradient_loss | -0.0243     |\n",
            "|    value_loss           | 0.0179      |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 125         |\n",
            "|    iterations           | 8           |\n",
            "|    time_elapsed         | 1044        |\n",
            "|    total_timesteps      | 131072      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.019554403 |\n",
            "|    clip_fraction        | 0.2         |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.49       |\n",
            "|    explained_variance   | 0.939       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.0789     |\n",
            "|    n_updates            | 70          |\n",
            "|    policy_gradient_loss | -0.0257     |\n",
            "|    value_loss           | 0.011       |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 124         |\n",
            "|    iterations           | 9           |\n",
            "|    time_elapsed         | 1186        |\n",
            "|    total_timesteps      | 147456      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018770391 |\n",
            "|    clip_fraction        | 0.215       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.49       |\n",
            "|    explained_variance   | 0.968       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.0954     |\n",
            "|    n_updates            | 80          |\n",
            "|    policy_gradient_loss | -0.0257     |\n",
            "|    value_loss           | 0.00811     |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 123         |\n",
            "|    iterations           | 10          |\n",
            "|    time_elapsed         | 1323        |\n",
            "|    total_timesteps      | 163840      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.020197934 |\n",
            "|    clip_fraction        | 0.225       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.47       |\n",
            "|    explained_variance   | 0.971       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.106      |\n",
            "|    n_updates            | 90          |\n",
            "|    policy_gradient_loss | -0.0269     |\n",
            "|    value_loss           | 0.00839     |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 123         |\n",
            "|    iterations           | 11          |\n",
            "|    time_elapsed         | 1458        |\n",
            "|    total_timesteps      | 180224      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.019993085 |\n",
            "|    clip_fraction        | 0.204       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.43       |\n",
            "|    explained_variance   | 0.929       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.0823     |\n",
            "|    n_updates            | 100         |\n",
            "|    policy_gradient_loss | -0.0202     |\n",
            "|    value_loss           | 0.0314      |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 122         |\n",
            "|    iterations           | 12          |\n",
            "|    time_elapsed         | 1600        |\n",
            "|    total_timesteps      | 196608      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.021663524 |\n",
            "|    clip_fraction        | 0.218       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.42       |\n",
            "|    explained_variance   | 0.979       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.0965     |\n",
            "|    n_updates            | 110         |\n",
            "|    policy_gradient_loss | -0.0257     |\n",
            "|    value_loss           | 0.00774     |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 122         |\n",
            "|    iterations           | 13          |\n",
            "|    time_elapsed         | 1735        |\n",
            "|    total_timesteps      | 212992      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.019217448 |\n",
            "|    clip_fraction        | 0.198       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.4        |\n",
            "|    explained_variance   | 0.949       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.11       |\n",
            "|    n_updates            | 120         |\n",
            "|    policy_gradient_loss | -0.0215     |\n",
            "|    value_loss           | 0.0151      |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 122         |\n",
            "|    iterations           | 14          |\n",
            "|    time_elapsed         | 1871        |\n",
            "|    total_timesteps      | 229376      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.022590559 |\n",
            "|    clip_fraction        | 0.209       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.37       |\n",
            "|    explained_variance   | 0.969       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.0903     |\n",
            "|    n_updates            | 130         |\n",
            "|    policy_gradient_loss | -0.0269     |\n",
            "|    value_loss           | 0.0135      |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 121         |\n",
            "|    iterations           | 15          |\n",
            "|    time_elapsed         | 2016        |\n",
            "|    total_timesteps      | 245760      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.024704147 |\n",
            "|    clip_fraction        | 0.228       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.31       |\n",
            "|    explained_variance   | 0.943       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.115      |\n",
            "|    n_updates            | 140         |\n",
            "|    policy_gradient_loss | -0.0309     |\n",
            "|    value_loss           | 0.0238      |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 121        |\n",
            "|    iterations           | 16         |\n",
            "|    time_elapsed         | 2152       |\n",
            "|    total_timesteps      | 262144     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02606789 |\n",
            "|    clip_fraction        | 0.219      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.37      |\n",
            "|    explained_variance   | 0.967      |\n",
            "|    learning_rate        | 0.0001     |\n",
            "|    loss                 | -0.108     |\n",
            "|    n_updates            | 150        |\n",
            "|    policy_gradient_loss | -0.0316    |\n",
            "|    value_loss           | 0.0159     |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 121         |\n",
            "|    iterations           | 17          |\n",
            "|    time_elapsed         | 2290        |\n",
            "|    total_timesteps      | 278528      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.028998587 |\n",
            "|    clip_fraction        | 0.284       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.32       |\n",
            "|    explained_variance   | 0.936       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.0895     |\n",
            "|    n_updates            | 160         |\n",
            "|    policy_gradient_loss | -0.0305     |\n",
            "|    value_loss           | 0.0174      |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 121        |\n",
            "|    iterations           | 18         |\n",
            "|    time_elapsed         | 2426       |\n",
            "|    total_timesteps      | 294912     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02721861 |\n",
            "|    clip_fraction        | 0.29       |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.32      |\n",
            "|    explained_variance   | 0.925      |\n",
            "|    learning_rate        | 0.0001     |\n",
            "|    loss                 | -0.0618    |\n",
            "|    n_updates            | 170        |\n",
            "|    policy_gradient_loss | -0.0311    |\n",
            "|    value_loss           | 0.014      |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 121        |\n",
            "|    iterations           | 19         |\n",
            "|    time_elapsed         | 2563       |\n",
            "|    total_timesteps      | 311296     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02792129 |\n",
            "|    clip_fraction        | 0.284      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.3       |\n",
            "|    explained_variance   | 0.942      |\n",
            "|    learning_rate        | 0.0001     |\n",
            "|    loss                 | -0.0928    |\n",
            "|    n_updates            | 180        |\n",
            "|    policy_gradient_loss | -0.0318    |\n",
            "|    value_loss           | 0.0141     |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 121        |\n",
            "|    iterations           | 20         |\n",
            "|    time_elapsed         | 2703       |\n",
            "|    total_timesteps      | 327680     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02736053 |\n",
            "|    clip_fraction        | 0.281      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.31      |\n",
            "|    explained_variance   | 0.947      |\n",
            "|    learning_rate        | 0.0001     |\n",
            "|    loss                 | -0.0725    |\n",
            "|    n_updates            | 190        |\n",
            "|    policy_gradient_loss | -0.0354    |\n",
            "|    value_loss           | 0.0172     |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 121         |\n",
            "|    iterations           | 21          |\n",
            "|    time_elapsed         | 2840        |\n",
            "|    total_timesteps      | 344064      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.028594576 |\n",
            "|    clip_fraction        | 0.287       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.22       |\n",
            "|    explained_variance   | 0.934       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.116      |\n",
            "|    n_updates            | 200         |\n",
            "|    policy_gradient_loss | -0.0386     |\n",
            "|    value_loss           | 0.0264      |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 121        |\n",
            "|    iterations           | 22         |\n",
            "|    time_elapsed         | 2977       |\n",
            "|    total_timesteps      | 360448     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.03111228 |\n",
            "|    clip_fraction        | 0.306      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.27      |\n",
            "|    explained_variance   | 0.923      |\n",
            "|    learning_rate        | 0.0001     |\n",
            "|    loss                 | -0.0525    |\n",
            "|    n_updates            | 210        |\n",
            "|    policy_gradient_loss | -0.0391    |\n",
            "|    value_loss           | 0.0185     |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 120         |\n",
            "|    iterations           | 23          |\n",
            "|    time_elapsed         | 3117        |\n",
            "|    total_timesteps      | 376832      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.031269033 |\n",
            "|    clip_fraction        | 0.304       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.25       |\n",
            "|    explained_variance   | 0.935       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.1        |\n",
            "|    n_updates            | 220         |\n",
            "|    policy_gradient_loss | -0.0424     |\n",
            "|    value_loss           | 0.0191      |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 120         |\n",
            "|    iterations           | 24          |\n",
            "|    time_elapsed         | 3263        |\n",
            "|    total_timesteps      | 393216      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.032828398 |\n",
            "|    clip_fraction        | 0.312       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.24       |\n",
            "|    explained_variance   | 0.918       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.0907     |\n",
            "|    n_updates            | 230         |\n",
            "|    policy_gradient_loss | -0.0391     |\n",
            "|    value_loss           | 0.0191      |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 120         |\n",
            "|    iterations           | 25          |\n",
            "|    time_elapsed         | 3403        |\n",
            "|    total_timesteps      | 409600      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.031920847 |\n",
            "|    clip_fraction        | 0.304       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.21       |\n",
            "|    explained_variance   | 0.923       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.0868     |\n",
            "|    n_updates            | 240         |\n",
            "|    policy_gradient_loss | -0.0375     |\n",
            "|    value_loss           | 0.0191      |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 120         |\n",
            "|    iterations           | 26          |\n",
            "|    time_elapsed         | 3537        |\n",
            "|    total_timesteps      | 425984      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.032573417 |\n",
            "|    clip_fraction        | 0.311       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.18       |\n",
            "|    explained_variance   | 0.92        |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.104      |\n",
            "|    n_updates            | 250         |\n",
            "|    policy_gradient_loss | -0.0395     |\n",
            "|    value_loss           | 0.0181      |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 120         |\n",
            "|    iterations           | 27          |\n",
            "|    time_elapsed         | 3679        |\n",
            "|    total_timesteps      | 442368      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.033804223 |\n",
            "|    clip_fraction        | 0.318       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.23       |\n",
            "|    explained_variance   | 0.922       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.106      |\n",
            "|    n_updates            | 260         |\n",
            "|    policy_gradient_loss | -0.0395     |\n",
            "|    value_loss           | 0.0155      |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 119         |\n",
            "|    iterations           | 28          |\n",
            "|    time_elapsed         | 3823        |\n",
            "|    total_timesteps      | 458752      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.033309057 |\n",
            "|    clip_fraction        | 0.326       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.19       |\n",
            "|    explained_variance   | 0.907       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.0729     |\n",
            "|    n_updates            | 270         |\n",
            "|    policy_gradient_loss | -0.0407     |\n",
            "|    value_loss           | 0.0185      |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 120         |\n",
            "|    iterations           | 29          |\n",
            "|    time_elapsed         | 3956        |\n",
            "|    total_timesteps      | 475136      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.034100275 |\n",
            "|    clip_fraction        | 0.325       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.18       |\n",
            "|    explained_variance   | 0.932       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.0804     |\n",
            "|    n_updates            | 280         |\n",
            "|    policy_gradient_loss | -0.0433     |\n",
            "|    value_loss           | 0.0177      |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 119         |\n",
            "|    iterations           | 30          |\n",
            "|    time_elapsed         | 4100        |\n",
            "|    total_timesteps      | 491520      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.034867126 |\n",
            "|    clip_fraction        | 0.323       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.18       |\n",
            "|    explained_variance   | 0.913       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.145      |\n",
            "|    n_updates            | 290         |\n",
            "|    policy_gradient_loss | -0.0417     |\n",
            "|    value_loss           | 0.0189      |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 120         |\n",
            "|    iterations           | 31          |\n",
            "|    time_elapsed         | 4231        |\n",
            "|    total_timesteps      | 507904      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.034438677 |\n",
            "|    clip_fraction        | 0.332       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.17       |\n",
            "|    explained_variance   | 0.89        |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.109      |\n",
            "|    n_updates            | 300         |\n",
            "|    policy_gradient_loss | -0.0392     |\n",
            "|    value_loss           | 0.0172      |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 120         |\n",
            "|    iterations           | 32          |\n",
            "|    time_elapsed         | 4363        |\n",
            "|    total_timesteps      | 524288      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.036378488 |\n",
            "|    clip_fraction        | 0.327       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.18       |\n",
            "|    explained_variance   | 0.91        |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.0989     |\n",
            "|    n_updates            | 310         |\n",
            "|    policy_gradient_loss | -0.0388     |\n",
            "|    value_loss           | 0.0133      |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 120         |\n",
            "|    iterations           | 33          |\n",
            "|    time_elapsed         | 4496        |\n",
            "|    total_timesteps      | 540672      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.035828196 |\n",
            "|    clip_fraction        | 0.325       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.15       |\n",
            "|    explained_variance   | 0.908       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.0928     |\n",
            "|    n_updates            | 320         |\n",
            "|    policy_gradient_loss | -0.0411     |\n",
            "|    value_loss           | 0.0151      |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 120        |\n",
            "|    iterations           | 34         |\n",
            "|    time_elapsed         | 4638       |\n",
            "|    total_timesteps      | 557056     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.03720585 |\n",
            "|    clip_fraction        | 0.336      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.17      |\n",
            "|    explained_variance   | 0.897      |\n",
            "|    learning_rate        | 0.0001     |\n",
            "|    loss                 | -0.105     |\n",
            "|    n_updates            | 330        |\n",
            "|    policy_gradient_loss | -0.0402    |\n",
            "|    value_loss           | 0.0162     |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 120         |\n",
            "|    iterations           | 35          |\n",
            "|    time_elapsed         | 4774        |\n",
            "|    total_timesteps      | 573440      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.038322814 |\n",
            "|    clip_fraction        | 0.325       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.09       |\n",
            "|    explained_variance   | 0.92        |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.108      |\n",
            "|    n_updates            | 340         |\n",
            "|    policy_gradient_loss | -0.0427     |\n",
            "|    value_loss           | 0.0185      |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 120         |\n",
            "|    iterations           | 36          |\n",
            "|    time_elapsed         | 4913        |\n",
            "|    total_timesteps      | 589824      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.040805083 |\n",
            "|    clip_fraction        | 0.334       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.13       |\n",
            "|    explained_variance   | 0.916       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.127      |\n",
            "|    n_updates            | 350         |\n",
            "|    policy_gradient_loss | -0.0413     |\n",
            "|    value_loss           | 0.0164      |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 119         |\n",
            "|    iterations           | 37          |\n",
            "|    time_elapsed         | 5055        |\n",
            "|    total_timesteps      | 606208      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.040516093 |\n",
            "|    clip_fraction        | 0.329       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.08       |\n",
            "|    explained_variance   | 0.909       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.09       |\n",
            "|    n_updates            | 360         |\n",
            "|    policy_gradient_loss | -0.0449     |\n",
            "|    value_loss           | 0.0203      |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 119         |\n",
            "|    iterations           | 38          |\n",
            "|    time_elapsed         | 5191        |\n",
            "|    total_timesteps      | 622592      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.042556785 |\n",
            "|    clip_fraction        | 0.345       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.1        |\n",
            "|    explained_variance   | 0.926       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.116      |\n",
            "|    n_updates            | 370         |\n",
            "|    policy_gradient_loss | -0.0453     |\n",
            "|    value_loss           | 0.018       |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 119        |\n",
            "|    iterations           | 39         |\n",
            "|    time_elapsed         | 5327       |\n",
            "|    total_timesteps      | 638976     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.04340783 |\n",
            "|    clip_fraction        | 0.358      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.14      |\n",
            "|    explained_variance   | 0.904      |\n",
            "|    learning_rate        | 0.0001     |\n",
            "|    loss                 | -0.117     |\n",
            "|    n_updates            | 380        |\n",
            "|    policy_gradient_loss | -0.0461    |\n",
            "|    value_loss           | 0.0169     |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 119        |\n",
            "|    iterations           | 40         |\n",
            "|    time_elapsed         | 5464       |\n",
            "|    total_timesteps      | 655360     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.04626951 |\n",
            "|    clip_fraction        | 0.369      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.11      |\n",
            "|    explained_variance   | 0.904      |\n",
            "|    learning_rate        | 0.0001     |\n",
            "|    loss                 | -0.138     |\n",
            "|    n_updates            | 390        |\n",
            "|    policy_gradient_loss | -0.0472    |\n",
            "|    value_loss           | 0.0148     |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 120         |\n",
            "|    iterations           | 41          |\n",
            "|    time_elapsed         | 5597        |\n",
            "|    total_timesteps      | 671744      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.043217078 |\n",
            "|    clip_fraction        | 0.358       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.13       |\n",
            "|    explained_variance   | 0.904       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.132      |\n",
            "|    n_updates            | 400         |\n",
            "|    policy_gradient_loss | -0.0455     |\n",
            "|    value_loss           | 0.0164      |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 120        |\n",
            "|    iterations           | 42         |\n",
            "|    time_elapsed         | 5730       |\n",
            "|    total_timesteps      | 688128     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.04600232 |\n",
            "|    clip_fraction        | 0.357      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.11      |\n",
            "|    explained_variance   | 0.93       |\n",
            "|    learning_rate        | 0.0001     |\n",
            "|    loss                 | -0.0759    |\n",
            "|    n_updates            | 410        |\n",
            "|    policy_gradient_loss | -0.0425    |\n",
            "|    value_loss           | 0.0149     |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 120        |\n",
            "|    iterations           | 43         |\n",
            "|    time_elapsed         | 5858       |\n",
            "|    total_timesteps      | 704512     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.05071123 |\n",
            "|    clip_fraction        | 0.374      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.13      |\n",
            "|    explained_variance   | 0.909      |\n",
            "|    learning_rate        | 0.0001     |\n",
            "|    loss                 | -0.122     |\n",
            "|    n_updates            | 420        |\n",
            "|    policy_gradient_loss | -0.0436    |\n",
            "|    value_loss           | 0.0132     |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 120         |\n",
            "|    iterations           | 44          |\n",
            "|    time_elapsed         | 5989        |\n",
            "|    total_timesteps      | 720896      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.052912015 |\n",
            "|    clip_fraction        | 0.391       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.1        |\n",
            "|    explained_variance   | 0.884       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.0807     |\n",
            "|    n_updates            | 430         |\n",
            "|    policy_gradient_loss | -0.0437     |\n",
            "|    value_loss           | 0.0116      |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 120         |\n",
            "|    iterations           | 45          |\n",
            "|    time_elapsed         | 6120        |\n",
            "|    total_timesteps      | 737280      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.048066907 |\n",
            "|    clip_fraction        | 0.364       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.11       |\n",
            "|    explained_variance   | 0.925       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.12       |\n",
            "|    n_updates            | 440         |\n",
            "|    policy_gradient_loss | -0.0434     |\n",
            "|    value_loss           | 0.0136      |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 120         |\n",
            "|    iterations           | 46          |\n",
            "|    time_elapsed         | 6262        |\n",
            "|    total_timesteps      | 753664      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.051342614 |\n",
            "|    clip_fraction        | 0.379       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.11       |\n",
            "|    explained_variance   | 0.92        |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.0979     |\n",
            "|    n_updates            | 450         |\n",
            "|    policy_gradient_loss | -0.0432     |\n",
            "|    value_loss           | 0.0127      |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 120         |\n",
            "|    iterations           | 47          |\n",
            "|    time_elapsed         | 6398        |\n",
            "|    total_timesteps      | 770048      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.047814365 |\n",
            "|    clip_fraction        | 0.361       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.09       |\n",
            "|    explained_variance   | 0.917       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.11       |\n",
            "|    n_updates            | 460         |\n",
            "|    policy_gradient_loss | -0.0417     |\n",
            "|    value_loss           | 0.0137      |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 120        |\n",
            "|    iterations           | 48         |\n",
            "|    time_elapsed         | 6542       |\n",
            "|    total_timesteps      | 786432     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.05650707 |\n",
            "|    clip_fraction        | 0.377      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.07      |\n",
            "|    explained_variance   | 0.94       |\n",
            "|    learning_rate        | 0.0001     |\n",
            "|    loss                 | -0.134     |\n",
            "|    n_updates            | 470        |\n",
            "|    policy_gradient_loss | -0.0464    |\n",
            "|    value_loss           | 0.016      |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 120         |\n",
            "|    iterations           | 49          |\n",
            "|    time_elapsed         | 6679        |\n",
            "|    total_timesteps      | 802816      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.052321058 |\n",
            "|    clip_fraction        | 0.363       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.09       |\n",
            "|    explained_variance   | 0.937       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.128      |\n",
            "|    n_updates            | 480         |\n",
            "|    policy_gradient_loss | -0.0427     |\n",
            "|    value_loss           | 0.0174      |\n",
            "-----------------------------------------\n",
            "---------------------------------------\n",
            "| time/                   |           |\n",
            "|    fps                  | 120       |\n",
            "|    iterations           | 50        |\n",
            "|    time_elapsed         | 6821      |\n",
            "|    total_timesteps      | 819200    |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0502716 |\n",
            "|    clip_fraction        | 0.362     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -1.08     |\n",
            "|    explained_variance   | 0.914     |\n",
            "|    learning_rate        | 0.0001    |\n",
            "|    loss                 | -0.0884   |\n",
            "|    n_updates            | 490       |\n",
            "|    policy_gradient_loss | -0.0448   |\n",
            "|    value_loss           | 0.0174    |\n",
            "---------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 120        |\n",
            "|    iterations           | 51         |\n",
            "|    time_elapsed         | 6961       |\n",
            "|    total_timesteps      | 835584     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.05642126 |\n",
            "|    clip_fraction        | 0.372      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.03      |\n",
            "|    explained_variance   | 0.915      |\n",
            "|    learning_rate        | 0.0001     |\n",
            "|    loss                 | -0.125     |\n",
            "|    n_updates            | 500        |\n",
            "|    policy_gradient_loss | -0.0493    |\n",
            "|    value_loss           | 0.019      |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 120        |\n",
            "|    iterations           | 52         |\n",
            "|    time_elapsed         | 7099       |\n",
            "|    total_timesteps      | 851968     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.06280429 |\n",
            "|    clip_fraction        | 0.386      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.1       |\n",
            "|    explained_variance   | 0.947      |\n",
            "|    learning_rate        | 0.0001     |\n",
            "|    loss                 | -0.117     |\n",
            "|    n_updates            | 510        |\n",
            "|    policy_gradient_loss | -0.0427    |\n",
            "|    value_loss           | 0.0145     |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 120        |\n",
            "|    iterations           | 53         |\n",
            "|    time_elapsed         | 7231       |\n",
            "|    total_timesteps      | 868352     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.05813282 |\n",
            "|    clip_fraction        | 0.371      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.06      |\n",
            "|    explained_variance   | 0.927      |\n",
            "|    learning_rate        | 0.0001     |\n",
            "|    loss                 | -0.104     |\n",
            "|    n_updates            | 520        |\n",
            "|    policy_gradient_loss | -0.0418    |\n",
            "|    value_loss           | 0.0158     |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 119         |\n",
            "|    iterations           | 54          |\n",
            "|    time_elapsed         | 7375        |\n",
            "|    total_timesteps      | 884736      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.058587603 |\n",
            "|    clip_fraction        | 0.363       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.04       |\n",
            "|    explained_variance   | 0.947       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.112      |\n",
            "|    n_updates            | 530         |\n",
            "|    policy_gradient_loss | -0.0422     |\n",
            "|    value_loss           | 0.0155      |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 119         |\n",
            "|    iterations           | 55          |\n",
            "|    time_elapsed         | 7509        |\n",
            "|    total_timesteps      | 901120      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.055395924 |\n",
            "|    clip_fraction        | 0.366       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.06       |\n",
            "|    explained_variance   | 0.942       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.0684     |\n",
            "|    n_updates            | 540         |\n",
            "|    policy_gradient_loss | -0.04       |\n",
            "|    value_loss           | 0.0157      |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 119        |\n",
            "|    iterations           | 56         |\n",
            "|    time_elapsed         | 7651       |\n",
            "|    total_timesteps      | 917504     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.06530324 |\n",
            "|    clip_fraction        | 0.378      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.989     |\n",
            "|    explained_variance   | 0.921      |\n",
            "|    learning_rate        | 0.0001     |\n",
            "|    loss                 | -0.112     |\n",
            "|    n_updates            | 550        |\n",
            "|    policy_gradient_loss | -0.0485    |\n",
            "|    value_loss           | 0.0222     |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 119        |\n",
            "|    iterations           | 57         |\n",
            "|    time_elapsed         | 7788       |\n",
            "|    total_timesteps      | 933888     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.06163242 |\n",
            "|    clip_fraction        | 0.401      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.1       |\n",
            "|    explained_variance   | 0.935      |\n",
            "|    learning_rate        | 0.0001     |\n",
            "|    loss                 | -0.119     |\n",
            "|    n_updates            | 560        |\n",
            "|    policy_gradient_loss | -0.0462    |\n",
            "|    value_loss           | 0.018      |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 119        |\n",
            "|    iterations           | 58         |\n",
            "|    time_elapsed         | 7926       |\n",
            "|    total_timesteps      | 950272     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.06568067 |\n",
            "|    clip_fraction        | 0.383      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.988     |\n",
            "|    explained_variance   | 0.936      |\n",
            "|    learning_rate        | 0.0001     |\n",
            "|    loss                 | -0.0823    |\n",
            "|    n_updates            | 570        |\n",
            "|    policy_gradient_loss | -0.048     |\n",
            "|    value_loss           | 0.0194     |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 119         |\n",
            "|    iterations           | 59          |\n",
            "|    time_elapsed         | 8065        |\n",
            "|    total_timesteps      | 966656      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.060228735 |\n",
            "|    clip_fraction        | 0.393       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -1.15       |\n",
            "|    explained_variance   | 0.946       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | -0.11       |\n",
            "|    n_updates            | 580         |\n",
            "|    policy_gradient_loss | -0.0415     |\n",
            "|    value_loss           | 0.0157      |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 119        |\n",
            "|    iterations           | 60         |\n",
            "|    time_elapsed         | 8199       |\n",
            "|    total_timesteps      | 983040     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.05715896 |\n",
            "|    clip_fraction        | 0.397      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.14      |\n",
            "|    explained_variance   | 0.935      |\n",
            "|    learning_rate        | 0.0001     |\n",
            "|    loss                 | -0.115     |\n",
            "|    n_updates            | 590        |\n",
            "|    policy_gradient_loss | -0.0416    |\n",
            "|    value_loss           | 0.0138     |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 119        |\n",
            "|    iterations           | 61         |\n",
            "|    time_elapsed         | 8344       |\n",
            "|    total_timesteps      | 999424     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.06096393 |\n",
            "|    clip_fraction        | 0.366      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.1       |\n",
            "|    explained_variance   | 0.95       |\n",
            "|    learning_rate        | 0.0001     |\n",
            "|    loss                 | -0.127     |\n",
            "|    n_updates            | 600        |\n",
            "|    policy_gradient_loss | -0.0412    |\n",
            "|    value_loss           | 0.0166     |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 119        |\n",
            "|    iterations           | 62         |\n",
            "|    time_elapsed         | 8479       |\n",
            "|    total_timesteps      | 1015808    |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.06182477 |\n",
            "|    clip_fraction        | 0.379      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -1.11      |\n",
            "|    explained_variance   | 0.96       |\n",
            "|    learning_rate        | 0.0001     |\n",
            "|    loss                 | -0.0965    |\n",
            "|    n_updates            | 610        |\n",
            "|    policy_gradient_loss | -0.0419    |\n",
            "|    value_loss           | 0.0163     |\n",
            "----------------------------------------\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<stable_baselines3.ppo.ppo.PPO at 0x7f1dbc78dbd0>"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Callback W&B (puoi combinare con altri callback come EvalCallback)\n",
        "wandb_callback = WandbCallback(\n",
        "    gradient_save_freq=100,\n",
        "    model_save_path=\"./models/\",\n",
        "    verbose=2,\n",
        ")\n",
        "\n",
        "# Avvia il training\n",
        "print(\"Starting training...\")\n",
        "model.learn(\n",
        "    total_timesteps=1_000_000,\n",
        "    callback=wandb_callback\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "d6b0dc41",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (600, 400) to (608, 400) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total reward: -98.11304451525211\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>global_step</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇█</td></tr><tr><td>time/fps</td><td>█▄▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/approx_kl</td><td>▁▁▂▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▄▄▅▅▆▅▆▆▇▆▆█▇██▇▇█</td></tr><tr><td>train/clip_fraction</td><td>▁▃▃▄▄▄▄▄▄▄▄▄▅▆▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇█▇▇▇▇█▇███▇</td></tr><tr><td>train/clip_range</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/entropy_loss</td><td>▁▁▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▆▅▆▆▆▆▇▆▆▆▇▆▇▇▇▇█▇▇█▇█▇</td></tr><tr><td>train/explained_variance</td><td>▁▄▄▅▆█████████████████▇██▇█▇████████████</td></tr><tr><td>train/learning_rate</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/loss</td><td>▆▅▇▅▅▆▄▄▆▃▇▅▆▃█▅▄▄▆▆▄▅▄▄▂▃▁▂▆▂▃▁▅▂▃▃▃▆▃▄</td></tr><tr><td>train/policy_gradient_loss</td><td>█▆▅▅▅▅▅▅▄▄▄▃▃▃▃▃▃▂▂▃▂▃▂▂▂▂▂▂▂▂▂▁▂▂▂▁▁▂▂▂</td></tr><tr><td>train/value_loss</td><td>▅▇█▇▄▁▁█▁▃▄▃▃▆▄▃▄▄▄▃▄▅▄▄▃▃▂▃▂▃▄▄▄▃▃▅▄▄▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>global_step</td><td>1015808</td></tr><tr><td>time/fps</td><td>119</td></tr><tr><td>train/approx_kl</td><td>0.06182</td></tr><tr><td>train/clip_fraction</td><td>0.37885</td></tr><tr><td>train/clip_range</td><td>0.2</td></tr><tr><td>train/entropy_loss</td><td>-1.10802</td></tr><tr><td>train/explained_variance</td><td>0.95991</td></tr><tr><td>train/learning_rate</td><td>0.0001</td></tr><tr><td>train/loss</td><td>-0.09655</td></tr><tr><td>train/policy_gradient_loss</td><td>-0.04193</td></tr><tr><td>train/value_loss</td><td>0.01635</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">olive-meadow-7</strong> at: <a href='https://wandb.ai/vincenzo-civale-universi-degli-studi-di-firenze/ppo-carracing/runs/o8x6psow' target=\"_blank\">https://wandb.ai/vincenzo-civale-universi-degli-studi-di-firenze/ppo-carracing/runs/o8x6psow</a><br> View project at: <a href='https://wandb.ai/vincenzo-civale-universi-degli-studi-di-firenze/ppo-carracing' target=\"_blank\">https://wandb.ai/vincenzo-civale-universi-degli-studi-di-firenze/ppo-carracing</a><br>Synced 6 W&B file(s), 0 media file(s), 20 artifact file(s) and 2 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250607_151741-o8x6psow/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/vcivale/prova/Esercitazione_2/wandb/run-20250607_174654-ye9svn86</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/vincenzo-civale-universi-degli-studi-di-firenze/ppo-carracing/runs/ye9svn86' target=\"_blank\">evaluation-run</a></strong> to <a href='https://wandb.ai/vincenzo-civale-universi-degli-studi-di-firenze/ppo-carracing' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/vincenzo-civale-universi-degli-studi-di-firenze/ppo-carracing' target=\"_blank\">https://wandb.ai/vincenzo-civale-universi-degli-studi-di-firenze/ppo-carracing</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/vincenzo-civale-universi-degli-studi-di-firenze/ppo-carracing/runs/ye9svn86' target=\"_blank\">https://wandb.ai/vincenzo-civale-universi-degli-studi-di-firenze/ppo-carracing/runs/ye9svn86</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `fps` argument does not affect the frame rate of the video when providing a file path or raw bytes.\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>evaluation_reward</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>evaluation_reward</td><td>-98.11304</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">evaluation-run</strong> at: <a href='https://wandb.ai/vincenzo-civale-universi-degli-studi-di-firenze/ppo-carracing/runs/ye9svn86' target=\"_blank\">https://wandb.ai/vincenzo-civale-universi-degli-studi-di-firenze/ppo-carracing/runs/ye9svn86</a><br> View project at: <a href='https://wandb.ai/vincenzo-civale-universi-degli-studi-di-firenze/ppo-carracing' target=\"_blank\">https://wandb.ai/vincenzo-civale-universi-degli-studi-di-firenze/ppo-carracing</a><br>Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250607_174654-ye9svn86/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def make_eval_env():\n",
        "    def _init():\n",
        "        env = gym.make(\"CarRacing-v3\", render_mode=\"rgb_array\", continuous=False)\n",
        "        env = CarRacingPreprocessing(env, height=84)\n",
        "        return env\n",
        "    return _init\n",
        "\n",
        "def record_agent_video(model, steps=1000, save_path=\"carracing_eval.mp4\"):\n",
        "    # 1. Ricrea env come nel training\n",
        "    eval_env = DummyVecEnv([make_eval_env()])\n",
        "    eval_env = VecFrameStack(eval_env, n_stack=4)\n",
        "\n",
        "    obs = eval_env.reset()  # shape: (1, 84, 96, 4)\n",
        "    total_reward = 0.0\n",
        "    images = []\n",
        "\n",
        "    for _ in range(steps):\n",
        "        # Non serve transporre, il modello è già stato addestrato su questo formato\n",
        "        action, _ = model.predict(obs, deterministic=True)\n",
        "        obs, rewards, dones, infos = eval_env.step(action)\n",
        "        total_reward += float(rewards[0])\n",
        "\n",
        "        frame = eval_env.envs[0].render()\n",
        "        images.append(frame)\n",
        "\n",
        "        if dones[0]:\n",
        "            break\n",
        "\n",
        "    eval_env.close()\n",
        "    imageio.mimsave(save_path, images, fps=30)\n",
        "    return save_path, total_reward\n",
        "\n",
        "# Esegui valutazione\n",
        "video_path, total_reward = record_agent_video(model, steps=1000)\n",
        "print(\"Total reward:\", total_reward)\n",
        "\n",
        "# Log su W&B (opzionale)\n",
        "import wandb\n",
        "wandb.init(project=\"ppo-carracing\", name=\"evaluation-run\")\n",
        "wandb.log({\n",
        "    \"evaluation_reward\": total_reward,\n",
        "    \"agent_video\": wandb.Video(video_path, fps=30, format=\"mp4\")\n",
        "})\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b78960e",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
