{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f6ae2782-57ea-48f0-b948-b412d0076ffc",
      "metadata": {
        "id": "f6ae2782-57ea-48f0-b948-b412d0076ffc"
      },
      "source": [
        "# Deep Reinforcement Learning Laboratory\n",
        "\n",
        "In this laboratory session we will work on getting more advanced versions of Deep Reinforcement Learning algorithms up and running. Deep Reinforcement Learning is **hard**, and getting agents to stably train can be frustrating and requires quite a bit of subtlety in analysis of intermediate results. We will start by refactoring (a bit) my implementation of `REINFORCE` on the [Cartpole environment](https://gymnasium.farama.org/environments/classic_control/cart_pole/)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3Cvjww4Spf4j",
      "metadata": {
        "id": "3Cvjww4Spf4j"
      },
      "source": [
        "### Import the packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ZJfqVOJpyiW",
      "metadata": {
        "id": "3ZJfqVOJpyiW"
      },
      "outputs": [],
      "source": [
        "# %%capture\n",
        "# !apt install python-opengl\n",
        "# !apt install ffmpeg\n",
        "# !apt install xvfb\n",
        "# !pip install pyvirtualdisplay\n",
        "# !pip install pyglet==1.5.1\n",
        "# !pip install gym_pygame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7AYhIWIGprT5",
      "metadata": {
        "id": "7AYhIWIGprT5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "from collections import deque\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import imageio\n",
        "from PIL import Image\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "# Gym\n",
        "import gymnasium as gym\n",
        "\n",
        "import wandb\n",
        "from wandb.integration.sb3 import WandbCallback\n",
        "import copy\n",
        "import random\n",
        "import cv2\n",
        "\n",
        "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
        "from stable_baselines3.common.vec_env import VecTransposeImage, DummyVecEnv, VecFrameStack, VecNormalize, SubprocVecEnv\n",
        "from stable_baselines3.common.monitor import Monitor\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.vec_env import VecFrameStack"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fadaf0c-f9b2-4680-8456-0eadb5eb8c2f",
      "metadata": {
        "id": "3fadaf0c-f9b2-4680-8456-0eadb5eb8c2f"
      },
      "source": [
        "## Exercise 1: Improving my `REINFORCE` Implementation (warm up)\n",
        "\n",
        "In this exercise we will refactor a bit and improve some aspects of my `REINFORCE` implementation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "riPqQhsEqhV7",
      "metadata": {
        "id": "riPqQhsEqhV7"
      },
      "source": [
        "### The CartPole environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "M58K8c372-kF",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M58K8c372-kF",
        "outputId": "2d045f4c-9add-47f8-c65f-21828572d32d"
      },
      "outputs": [],
      "source": [
        "# Virtual display\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython.display import Video, display\n",
        "\n",
        "virtual_display = Display(visible=0, size=(1400, 900))\n",
        "virtual_display.start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yPyG-LNTqZeq",
      "metadata": {
        "id": "yPyG-LNTqZeq"
      },
      "outputs": [],
      "source": [
        "env_id = \"CartPole-v1\"\n",
        "\n",
        "env = gym.make(env_id)\n",
        "\n",
        "eval_env = gym.make(env_id)\n",
        "\n",
        "s_size = env.observation_space.shape[0]\n",
        "a_size = env.action_space.n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5oZubromqv0e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5oZubromqv0e",
        "outputId": "0b55cd73-da04-48ca-f6dc-045b9b631f4a"
      },
      "outputs": [],
      "source": [
        "print(\"_____OBSERVATION SPACE_____ \\n\")\n",
        "print(\"The State Space is: \", s_size)\n",
        "print(\"Sample observation\", env.observation_space.sample()) # Get a random observation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RD7TPeGkqxva",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RD7TPeGkqxva",
        "outputId": "bb21487f-0333-4b09-d0e0-220d9bfcee45"
      },
      "outputs": [],
      "source": [
        "print(\"\\n _____ACTION SPACE_____ \\n\")\n",
        "print(\"The Action Space is: \", a_size)\n",
        "print(\"Action Space Sample\", env.action_space.sample()) # Take a random action"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "v1NcD7CPq7tn",
      "metadata": {
        "id": "v1NcD7CPq7tn"
      },
      "source": [
        "### Reinforce Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BzIc4ySKoHw0",
      "metadata": {
        "id": "BzIc4ySKoHw0"
      },
      "source": [
        "> ⚠️ **Disclaimer**  \n",
        "> L'implementazione riportata di seguito si basa su una combinazione di risorse pubblicamente disponibili e suggerite dal corso **Deep Reinforcement Learning with Hugging Face**. In particolare, il codice e i concetti sono stati ispirati e adattati da:\n",
        ">\n",
        "> - [Esempio ufficiale PyTorch REINFORCE](https://github.com/pytorch/examples/blob/main/reinforcement_learning/reinforce.py)  \n",
        "> - [Implementazione REINFORCE di Udacity](https://github.com/udacity/deep-reinforcement-learning/blob/master/reinforce/REINFORCE.ipynb)  \n",
        "> - [Pull Request di ottimizzazione di Chris1nexus](https://github.com/huggingface/deep-rl-class/pull/95)\n",
        ">\n",
        "> In particolare, l'efficiente calcolo del **reward-to-go** è stato adattato dal lavoro di [Chris1nexus](https://github.com/Chris1nexus), il cui codice e spiegazioni sono ben documentati nella [pull request](https://github.com/huggingface/deep-rl-class/pull/95) al corso Hugging Face.\n",
        ">\n",
        "> Queste fonti sono state selezionate tra quelle raccomandate nel modulo dedicato alla policy gradient del corso, e hanno fornito una base solida sia dal punto di vista teorico che implementativo.\n",
        "\n",
        "<img src=\"https://huggingface.co/datasets/huggingface-deep-rl-course/course-images/resolve/main/en/unit6/reinforce.png\" alt=\"Reinforce\"/>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RlCw4IjCoBrz",
      "metadata": {
        "id": "RlCw4IjCoBrz"
      },
      "outputs": [],
      "source": [
        "class Policy(nn.Module):\n",
        "    def __init__(self, s_size, a_size, h_size):\n",
        "        super(Policy, self).__init__()\n",
        "        self.fc1 = nn.Linear(s_size, h_size)\n",
        "        self.fc2 = nn.Linear(h_size, a_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return F.softmax(x, dim=1)\n",
        "\n",
        "    def act(self, state):\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "        probs = self.forward(state).cpu()\n",
        "        m = Categorical(probs)\n",
        "        action = m.sample()\n",
        "        return action.item(), m.log_prob(action)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6T0UPJS_rWF9",
      "metadata": {
        "id": "6T0UPJS_rWF9"
      },
      "source": [
        "### Reinforce Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FleRr65QrZj3",
      "metadata": {
        "id": "FleRr65QrZj3"
      },
      "outputs": [],
      "source": [
        "class ReinforceTrainer:\n",
        "    def __init__(self, policy, optimizer, env, gamma=0.99, max_t=1000, project_name=None):\n",
        "        self.policy = policy\n",
        "        self.optimizer = optimizer\n",
        "        self.env = env\n",
        "        self.gamma = gamma\n",
        "        self.max_t = max_t\n",
        "        self.project_name = project_name\n",
        "\n",
        "        self.scores = []\n",
        "        self.scores_deque = deque(maxlen=100)\n",
        "        self.best_score = -float('inf')\n",
        "\n",
        "        if self.project_name:\n",
        "            wandb.init(project=self.project_name)\n",
        "\n",
        "    def compute_returns(self, rewards):\n",
        "        returns = deque(maxlen=self.max_t)\n",
        "        for t in reversed(range(len(rewards))):\n",
        "            R = rewards[t] + self.gamma * (returns[0] if returns else 0.0)\n",
        "            returns.appendleft(R)\n",
        "        returns = torch.tensor(returns)\n",
        "        eps = np.finfo(np.float32).eps.item()\n",
        "        return (returns - returns.mean()) / (returns.std() + eps)\n",
        "\n",
        "    def reinforce(self, num_episodes=500, print_every=50, save_path=\"best_reinforce.pt\"):\n",
        "        for i_episode in range(1, num_episodes + 1):\n",
        "            saved_log_probs = []\n",
        "            rewards = []\n",
        "            # Gymnasium reset returns a tuple (observation, info), we only need the observation\n",
        "            state, _ = self.env.reset()\n",
        "\n",
        "            for t in range(self.max_t):\n",
        "                action, log_prob = self.policy.act(state)\n",
        "                saved_log_probs.append(log_prob)\n",
        "                # Gymnasium step returns (observation, reward, terminated, truncated, info)\n",
        "                next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
        "                rewards.append(reward)\n",
        "                done = terminated or truncated # In Gymnasium, done is terminated or truncated\n",
        "                state = next_state # Update state for the next iteration\n",
        "                if done:\n",
        "                    break\n",
        "\n",
        "            total_reward = sum(rewards)\n",
        "            self.scores.append(total_reward)\n",
        "            self.scores_deque.append(total_reward)\n",
        "\n",
        "            returns = self.compute_returns(rewards)\n",
        "\n",
        "            policy_loss = [-log_prob * R for log_prob, R in zip(saved_log_probs, returns)]\n",
        "            policy_loss = torch.cat(policy_loss).sum()\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            policy_loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            avg_score = np.mean(self.scores_deque)\n",
        "\n",
        "            if self.project_name:\n",
        "                wandb.log({\n",
        "                    'episode': i_episode,\n",
        "                    'reward': total_reward,\n",
        "                    'avg_reward': avg_score\n",
        "                })\n",
        "\n",
        "            if avg_score > self.best_score:\n",
        "                self.best_score = avg_score\n",
        "                torch.save(self.policy.state_dict(), save_path)\n",
        "                if self.project_name:\n",
        "                    wandb.run.summary[\"best_avg_reward\"] = avg_score\n",
        "\n",
        "            if i_episode % print_every == 0:\n",
        "                print(f\"Episode {i_episode}\\tAverage Score: {avg_score:.2f}\")\n",
        "\n",
        "        self.plot_rewards(self.scores) # Use self.scores which is accumulated over all episodes\n",
        "\n",
        "        return self.scores\n",
        "\n",
        "    def plot_rewards(self, reward_list, window=100):\n",
        "          plt.figure(figsize=(12, 6))\n",
        "          plt.plot(reward_list, label='Reward per Episode')\n",
        "\n",
        "          if len(reward_list) >= window:\n",
        "              moving_avg = np.convolve(reward_list, np.ones(window)/window, mode='valid')\n",
        "              plt.plot(range(window - 1, len(reward_list)), moving_avg, label=f'{window}-Episode Moving Avg', linewidth=2)\n",
        "\n",
        "          plt.xlabel('Episode')\n",
        "          plt.ylabel('Reward')\n",
        "          plt.title('Training Progress')\n",
        "          plt.legend()\n",
        "          plt.grid(True)\n",
        "          plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aEpVADzW2s-T",
      "metadata": {
        "id": "aEpVADzW2s-T"
      },
      "outputs": [],
      "source": [
        "def evaluate_agent(env, max_steps, n_eval_episodes, policy):\n",
        "    \"\"\"\n",
        "    Evaluate the agent for `n_eval_episodes` episodes and return the average and standard deviation of rewards.\n",
        "\n",
        "    :param env: The evaluation environment (gym.Env)\n",
        "    :param max_steps: Maximum number of steps per episode\n",
        "    :param n_eval_episodes: Number of episodes to evaluate the agent\n",
        "    :param policy: The Reinforce policy with an `act(state)` method\n",
        "    :return: Tuple of (mean_reward, std_reward)\n",
        "    \"\"\"\n",
        "    episode_rewards = []\n",
        "\n",
        "    for episode in range(n_eval_episodes):\n",
        "        state, _ = env.reset()  # Gymnasium reset returns (obs, info)\n",
        "        total_rewards_ep = 0\n",
        "\n",
        "        for step in range(max_steps):\n",
        "            action, _ = policy.act(state)\n",
        "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "            total_rewards_ep += reward\n",
        "            done = terminated or truncated\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "            state = next_state\n",
        "\n",
        "        episode_rewards.append(total_rewards_ep)\n",
        "\n",
        "    mean_reward = np.mean(episode_rewards)\n",
        "    std_reward = np.std(episode_rewards)\n",
        "\n",
        "    return mean_reward, std_reward\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ql0TeZuSuDeQ",
      "metadata": {
        "id": "ql0TeZuSuDeQ"
      },
      "outputs": [],
      "source": [
        "def record_video(env, policy, out_path, fps=30):\n",
        "    \"\"\"\n",
        "    Record a video of an agent acting in an environment.\n",
        "    :param env: the environment with render_mode='rgb_array'\n",
        "    :param policy: the agent, must implement policy.act(state)\n",
        "    :param out_path: full path to output .mp4 or .gif\n",
        "    :param fps: frames per second\n",
        "    \"\"\"\n",
        "    images = []\n",
        "    state, _ = env.reset()\n",
        "    done = False\n",
        "\n",
        "    img = env.render()\n",
        "    images.append(img)\n",
        "\n",
        "    while not done:\n",
        "        action, _ = policy.act(state)\n",
        "        state, reward, terminated, truncated, _ = env.step(action)\n",
        "        done = terminated or truncated\n",
        "        img = env.render()\n",
        "        images.append(img)\n",
        "\n",
        "    imageio.mimsave(out_path, [np.array(img) for img in images], fps=fps)\n",
        "    print(f\"Video saved to {out_path}\")\n",
        "    display(Video(out_path, embed=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nRL2ik9lw_Tx",
      "metadata": {
        "id": "nRL2ik9lw_Tx"
      },
      "outputs": [],
      "source": [
        "cartpole_hyperparameters = {\n",
        "    \"h_size\": 16,\n",
        "    \"n_training_episodes\": 600,\n",
        "    \"n_evaluation_episodes\": 10,\n",
        "    \"max_t\": 1000,\n",
        "    \"gamma\": 1.0,\n",
        "    \"lr\": 1e-2,\n",
        "    \"env_id\": env_id,\n",
        "    \"state_space\": s_size,\n",
        "    \"action_space\": a_size,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kppKkmSsrMnY",
      "metadata": {
        "id": "kppKkmSsrMnY"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88smzENVuF4i",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "88smzENVuF4i",
        "outputId": "f650f342-ed62-416d-e5e2-f1554247bad9"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.n\n",
        "policy = Policy(cartpole_hyperparameters[\"state_space\"], cartpole_hyperparameters[\"action_space\"], cartpole_hyperparameters[\"h_size\"]).to(device)\n",
        "optimizer = optim.Adam(policy.parameters(), lr=1e-2)\n",
        "\n",
        "trainer = ReinforceTrainer(policy, optimizer, env, gamma=0.99, max_t=cartpole_hyperparameters['max_t'])\n",
        "\n",
        "scores = trainer.reinforce(cartpole_hyperparameters['n_training_episodes'], print_every=10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BvakuGwIVnES",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "id": "BvakuGwIVnES",
        "outputId": "3e03b787-9a48-4c28-8f29-4abe8e83b930"
      },
      "outputs": [],
      "source": [
        "record_video(env, policy, \"Cart_Pole.mp4\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a5adad7-759b-4000-925b-701f41fe6e97",
      "metadata": {
        "id": "2a5adad7-759b-4000-925b-701f41fe6e97"
      },
      "source": [
        "-----\n",
        "## Exercise 2: `REINFORCE` with a Value Baseline (warm up)\n",
        "\n",
        "\n",
        "In questa variante dell’algoritmo REINFORCE, invece di aggiornare la policy solo in base al ritorno totale \\( G_t \\), si introduce una **baseline** per ridurre la varianza della stima del gradiente. Una scelta comune è usare una **value function** \\( V(s_t) \\), appresa tramite regressione sui ritorni osservati.\n",
        "\n",
        "L’aggiornamento della policy diventa:\n",
        "\n",
        "\\[\n",
        "\\theta \\leftarrow \\theta + \\alpha \\, (G_t - V(s_t)) \\, \\nabla_\\theta \\log \\pi(a_t | s_t)\n",
        "\\]\n",
        "\n",
        "Dove:\n",
        "- \\( G_t \\) è il ritorno cumulato dall’istante \\( t \\)\n",
        "- \\( V(s_t) \\) è il valore stimato dello stato corrente\n",
        "- \\( A_t = G_t - V(s_t) \\) è il **vantaggio**: quanto il risultato effettivo è migliore del previsto\n",
        "\n",
        "In pratica:\n",
        "- Addestriamo una seconda rete (`value_net`) per approssimare \\( V(s) \\)\n",
        "- Usiamo \\( A_t \\) come pesatura per il gradiente della policy (invece di usare direttamente \\( G_t \\))\n",
        "- Questo riduce la varianza dell’aggiornamento, stabilizzando il training\n",
        "\n",
        "**Nota:** La rete del valore viene ottimizzata separatamente minimizzando l’errore quadratico tra \\( V(s_t) \\) e \\( G_t \\).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Vzk_vGwhYatv",
      "metadata": {
        "id": "Vzk_vGwhYatv"
      },
      "outputs": [],
      "source": [
        "class ValueNetwork(nn.Module):\n",
        "    def __init__(self, state_size, hidden_size=128):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(state_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, state):\n",
        "      return self.net(state)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8b8cf76-5d39-45e9-8d7d-324125c04b4a",
      "metadata": {
        "id": "d8b8cf76-5d39-45e9-8d7d-324125c04b4a"
      },
      "outputs": [],
      "source": [
        "class ReinforceWithBaselineTrainer(ReinforceTrainer):\n",
        "    def __init__(self, policy, value_network, policy_optimizer, value_optimizer,\n",
        "                 env, gamma=0.99, max_t=1000, project_name=None):\n",
        "        super().__init__(policy, policy_optimizer, env, gamma, max_t, project_name)\n",
        "        self.value_network = value_network\n",
        "        self.value_optimizer = value_optimizer\n",
        "\n",
        "    def reinforce(self, num_episodes=500, print_every=50, save_path=\"best_reinforce_baseline.pt\"):\n",
        "        for i_episode in range(1, num_episodes + 1):\n",
        "            saved_log_probs = []\n",
        "            rewards = []\n",
        "            states = []\n",
        "\n",
        "            state, _ = self.env.reset()\n",
        "\n",
        "            for t in range(self.max_t):\n",
        "                action, log_prob = self.policy.act(state)\n",
        "                saved_log_probs.append(log_prob)\n",
        "                states.append(state)\n",
        "                next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
        "                rewards.append(reward)\n",
        "                done = terminated or truncated\n",
        "                state = next_state\n",
        "                if done:\n",
        "                    break\n",
        "\n",
        "            returns = self.compute_returns(rewards)\n",
        "            total_reward = sum(rewards)\n",
        "            self.scores.append(total_reward)\n",
        "            self.scores_deque.append(total_reward)\n",
        "\n",
        "            policy_losses = []\n",
        "            value_losses = []\n",
        "            for log_prob, R, state in zip(saved_log_probs, returns, states):\n",
        "                state_tensor = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "                baseline = self.value_network(state_tensor)\n",
        "                advantage = R - baseline.item()\n",
        "                policy_losses.append(-log_prob * advantage)\n",
        "                value_losses.append(nn.functional.mse_loss(baseline.squeeze(), torch.tensor(R).to(device)))\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            policy_loss = torch.stack(policy_losses).sum()\n",
        "            policy_loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            self.value_optimizer.zero_grad()\n",
        "            value_loss = torch.stack(value_losses).sum()\n",
        "            value_loss.backward()\n",
        "            self.value_optimizer.step()\n",
        "\n",
        "            avg_score = np.mean(self.scores_deque)\n",
        "\n",
        "            if self.project_name:\n",
        "                wandb.log({\n",
        "                    'episode': i_episode,\n",
        "                    'reward': total_reward,\n",
        "                    'avg_reward': avg_score,\n",
        "                    'policy_loss': policy_loss.item(),\n",
        "                    'value_loss': value_loss.item()\n",
        "                })\n",
        "\n",
        "            if avg_score > self.best_score:\n",
        "                self.best_score = avg_score\n",
        "                torch.save(self.policy.state_dict(), save_path)\n",
        "                if self.project_name:\n",
        "                    wandb.run.summary[\"best_avg_reward\"] = avg_score\n",
        "\n",
        "            if i_episode % print_every == 0:\n",
        "                print(f\"Episode {i_episode}\\tAverage Score: {avg_score:.2f}\")\n",
        "\n",
        "        self.plot_rewards(self.scores)\n",
        "        return self.scores\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8cdd5bc3-a439-4d84-891b-f4a840331e07",
      "metadata": {
        "id": "8cdd5bc3-a439-4d84-891b-f4a840331e07"
      },
      "source": [
        "**First Things First**: Recall from the slides on Deep Reinforcement Learning that we can **subtract** any function that doesn't depend on the current action from the q-value without changing the (maximum of our) objecttive function $J$:  \n",
        "\n",
        "$$ \\nabla J(\\boldsymbol{\\theta}) \\propto \\sum_{s} \\mu(s) \\sum_a \\left( q_{\\pi}(s, a) - b(s) \\right) \\nabla \\pi(a \\mid s, \\boldsymbol{\\theta}) $$\n",
        "\n",
        "In `REINFORCE` this means we can subtract from our target $G_t$:\n",
        "\n",
        "$$ \\boldsymbol{\\theta}_{t+1} \\triangleq \\boldsymbol{\\theta}_t + \\alpha (G_t - b(S_t)) \\frac{\\nabla \\pi(A_t \\mid s, \\boldsymbol{\\theta})}{\\pi(A_t \\mid s, \\boldsymbol{\\theta})} $$\n",
        "\n",
        "Since we are only interested in the **maximum** of our objective, we can also **rescale** our target by any function that also doesn't depend on the action. A **simple baseline** which is even independent of the state -- that is, it is **constant** for each episode -- is to just **standardize rewards within the episode**. So, we **subtract** the average return and **divide** by the variance of returns:\n",
        "\n",
        "$$ \\boldsymbol{\\theta}_{t+1} \\triangleq \\boldsymbol{\\theta}_t + \\alpha \\left(\\frac{G_t - \\bar{G}}{\\sigma_G}\\right) \\nabla  \\pi(A_t \\mid s, \\boldsymbol{\\theta}) $$\n",
        "\n",
        "This baseline is **already** implemented in my implementation of `REINFORCE`. Experiment with and without this standardization baseline and compare the performance. We are going to do something more interesting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "faf5b245-9369-4ed9-b5f4-3a52bb975d3f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "faf5b245-9369-4ed9-b5f4-3a52bb975d3f",
        "outputId": "dc0f3e5f-4673-4892-b7a6-942a520cd571"
      },
      "outputs": [],
      "source": [
        "seed = 123\n",
        "state, _ = env.reset(seed=seed)\n",
        "env.action_space.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "\n",
        "state_size = env.observation_space.shape[0]\n",
        "action_size = env.action_space.n\n",
        "hidden_size = 128\n",
        "\n",
        "policy1 = Policy(cartpole_hyperparameters[\"state_space\"], cartpole_hyperparameters[\"action_space\"], cartpole_hyperparameters[\"h_size\"]).to(device)\n",
        "policy2 = copy.deepcopy(policy1)\n",
        "\n",
        "policy_optimizer1 = torch.optim.Adam(policy1.parameters(), lr=1e-2)\n",
        "policy_optimizer2 = torch.optim.Adam(policy2.parameters(), lr=1e-2)\n",
        "\n",
        "# Istanzia rete valore + ottimizzatore per baseline trainer\n",
        "value_net = ValueNetwork(state_size, hidden_size).to(device)\n",
        "value_optimizer = torch.optim.Adam(value_net.parameters(), lr=1e-2)\n",
        "\n",
        "\n",
        "trainer_no_baseline = ReinforceTrainer(policy1, policy_optimizer1, env, gamma=0.99, max_t=1000)\n",
        "trainer_with_baseline = ReinforceWithBaselineTrainer(policy2, value_net, policy_optimizer2, value_optimizer,\n",
        "                                                    env, gamma=0.99, max_t=1000)\n",
        "\n",
        "\n",
        "scores_no_baseline = trainer_no_baseline.reinforce(cartpole_hyperparameters['n_training_episodes'], print_every=10),\n",
        "scores_with_baseline = trainer_with_baseline.reinforce(cartpole_hyperparameters['n_training_episodes'], print_every=10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46cfde8e-f1c7-4d96-85e0-268f687e09df",
      "metadata": {
        "id": "46cfde8e-f1c7-4d96-85e0-268f687e09df"
      },
      "source": [
        "**The Real Exercise**: Standard practice is to use the state-value function $v(s)$ as a baseline. This is intuitively appealing -- we are more interested in updating out policy for returns that estimate the current **value** worse. Our new update becomes:\n",
        "\n",
        "$$ \\boldsymbol{\\theta}_{t+1} \\triangleq \\boldsymbol{\\theta}_t + \\alpha (G_t - \\tilde{v}(S_t \\mid \\mathbf{w})) \\frac{\\nabla \\pi(A_t \\mid s, \\boldsymbol{\\theta})}{\\pi(A_t \\mid s, \\boldsymbol{\\theta})} $$\n",
        "\n",
        "where $\\tilde{v}(s \\mid \\mathbf{w})$ is a **deep neural network** with parameters $w$ that estimates $v_\\pi(s)$. What neural network? Typically, we use the **same** network architecture as that of the Policy.\n",
        "\n",
        "**Your Task**: Modify your implementation to fit a second, baseline network to estimate the value function and use it as **baseline**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4de3a2a8-05c3-44e2-9858-8cc8f431325f",
      "metadata": {
        "id": "4de3a2a8-05c3-44e2-9858-8cc8f431325f"
      },
      "outputs": [],
      "source": [
        "# Your code here."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64bf1447-d222-4b24-a357-5b7f9824390c",
      "metadata": {
        "id": "64bf1447-d222-4b24-a357-5b7f9824390c"
      },
      "source": [
        "-----\n",
        "## Exercise 3: Going Deeper\n",
        "\n",
        "As usual, pick **AT LEAST ONE** of the following exercises to complete.\n",
        "\n",
        "### Exercise 3.1: Solving Lunar Lander with `REINFORCE` (easy)\n",
        "\n",
        "Use my (or even better, improve on my) implementation of `REINFORCE` to solve the [Lunar Lander Environment](https://gymnasium.farama.org/environments/box2d/lunar_lander/). This environment is a little bit harder than Cartpole, but not much. Make sure you perform the same types of analyses we did during the lab session to quantify and qualify the performance of your agents.\n",
        "\n",
        "### Exercise 3.2: Solving Cartpole and Lunar Lander with `Deep Q-Learning` (harder)\n",
        "\n",
        "On policy Deep Reinforcement Learning tends to be **very unstable**. Write an implementation (or adapt an existing one) of `Deep Q-Learning` to solve our two environments (Cartpole and Lunar Lander). To do this you will need to implement a **Replay Buffer** and use a second, slow-moving **target Q-Network** to stabilize learning.\n",
        "\n",
        "### Exercise 3.3: Solving the OpenAI CarRacing environment (hardest)\n",
        "\n",
        "Use `Deep Q-Learning` -- or even better, an off-the-shelf implementation of **Proximal Policy Optimization (PPO)** -- to train an agent to solve the [OpenAI CarRacing](https://github.com/andywu0913/OpenAI-GYM-CarRacing-DQN) environment. This will be the most *fun*, but also the most *difficult*. Some tips:\n",
        "\n",
        "1. Make sure you use the `continuous=False` argument to the environment constructor. This ensures that the action space is **discrete** (we haven't seen how to work with continuous action spaces).\n",
        "2. Your Q-Network will need to be a CNN. A simple one should do, with two convolutional + maxpool layers, folowed by a two dense layers. You will **definitely** want to use a GPU to train your agents.\n",
        "3. The observation space of the environment is a single **color image** (a single frame of the game). Most implementations stack multiple frames (e.g. 3) after converting them to grayscale images as an observation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5wbp-6StptyZ",
      "metadata": {
        "id": "5wbp-6StptyZ"
      },
      "source": [
        "#### Modifiche all'Ambiente e Preprocessamento delle Osservazioni"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fc18dda",
      "metadata": {},
      "outputs": [],
      "source": [
        "class CarRacingPreprocessing(gym.ObservationWrapper):\n",
        "    def __init__(self, env, height=84):\n",
        "        super(CarRacingPreprocessing, self).__init__(env)\n",
        "        self.height = height\n",
        "        # IMPORTANTE: observation space per scala di grigi, normalizzato a float32 0-1\n",
        "        # L'output sarà (height, width, 1) per il wrapper, ma VecTransposeImage lo trasformerà in (1, height, width)\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=0.0, high=1.0, shape=(self.height, 96, 1), dtype=np.float32 # Modificato dtype e low/high\n",
        "        )\n",
        "\n",
        "    def observation(self, obs):\n",
        "        # Converti in scala di grigi\n",
        "        gray = cv2.cvtColor(obs, cv2.COLOR_RGB2GRAY)\n",
        "        # Crop dalla parte superiore (rimuove dashboard)\n",
        "        gray = gray[:self.height, :]\n",
        "        # Aggiungi dimensione canale\n",
        "        gray = np.expand_dims(gray, axis=-1)\n",
        "        # Normalizzazione: Cruciale per reti neurali\n",
        "        gray = gray.astype(np.float32) / 255.0\n",
        "        return gray"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1223db48",
      "metadata": {},
      "source": [
        "#### Features Extractor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1952bb92",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class CNN(BaseFeaturesExtractor):\n",
        "    def __init__(self, observation_space: gym.spaces.Box, features_dim: int = 512):\n",
        "        super(CNN, self).__init__(observation_space, features_dim)\n",
        "\n",
        "        n_input_channels = observation_space.shape[0] # Questo sarà 3 se n_stack=3\n",
        "\n",
        "        self.cnn = nn.Sequential(\n",
        "            # Layer 1: Kernel più piccolo, stride 2, con padding per preservare i bordi\n",
        "            # Input: (C, 84, 96) -> Output: (32, 42, 48) circa\n",
        "            nn.Conv2d(n_input_channels, 32, kernel_size=5, stride=2, padding=1), # Modificato kernel_size e aggiunto padding\n",
        "            nn.ReLU(),\n",
        "            # Layer 2: Input: (32, 42, 48) -> Output: (64, 21, 24) circa\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1), # Modificato kernel_size e aggiunto padding\n",
        "            nn.ReLU(),\n",
        "            # Layer 3: Input: (64, 21, 24) -> Output: (64, 11, 12) circa\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1), # Modificato kernel_size e aggiunto padding\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten()\n",
        "        )\n",
        "\n",
        "        with torch.no_grad():\n",
        "            sample_input = torch.as_tensor(observation_space.sample()[None]).float()\n",
        "            # Se la forma è (H, W, C), PyTorch Conv2d si aspetta (C, H, W)\n",
        "            # In questo caso VecFrameStack già output (C, H, W)\n",
        "            # Quindi sample_input sarà (1, n_stack, H, W)\n",
        "            n_flatten = self.cnn(sample_input).shape[1]\n",
        "\n",
        "        self.linear = nn.Sequential(\n",
        "            nn.Linear(n_flatten, features_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, observations: torch.Tensor) -> torch.Tensor:\n",
        "        return self.linear(self.cnn(observations))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "3f10948f",
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_env():\n",
        "    def _init():\n",
        "        try:\n",
        "            env = gym.make(\"CarRacing-v3\", render_mode=\"rgb_array\", continuous=False)\n",
        "            env = CarRacingPreprocessing(env, height=84)\n",
        "            return env\n",
        "        except Exception as e:\n",
        "            print(f\"Error initializing environment: {e}\")\n",
        "            raise\n",
        "    return _init\n",
        "\n",
        "\n",
        "n_envs = 8\n",
        "env = SubprocVecEnv([make_env() for _ in range(n_envs)])\n",
        "env = VecFrameStack(env, n_stack=4)\n",
        "env = VecNormalize(env, norm_obs=True, norm_reward=True, clip_reward=10.0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "77beb0c0",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvincenzo-civale\u001b[0m (\u001b[33mvincenzo-civale-universi-degli-studi-di-firenze\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/home/vcivale/prova/Esercitazione_2/wandb/run-20250607_151741-o8x6psow</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/vincenzo-civale-universi-degli-studi-di-firenze/ppo-carracing/runs/o8x6psow' target=\"_blank\">olive-meadow-7</a></strong> to <a href='https://wandb.ai/vincenzo-civale-universi-degli-studi-di-firenze/ppo-carracing' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/vincenzo-civale-universi-degli-studi-di-firenze/ppo-carracing' target=\"_blank\">https://wandb.ai/vincenzo-civale-universi-degli-studi-di-firenze/ppo-carracing</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/vincenzo-civale-universi-degli-studi-di-firenze/ppo-carracing/runs/o8x6psow' target=\"_blank\">https://wandb.ai/vincenzo-civale-universi-degli-studi-di-firenze/ppo-carracing/runs/o8x6psow</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda device\n"
          ]
        }
      ],
      "source": [
        "# Inizializza W&B\n",
        "wandb.init(\n",
        "    project=\"ppo-carracing\",\n",
        "    config={\n",
        "        \"policy_type\": \"CnnPolicy\",\n",
        "        \"total_timesteps\": 1_000_000,\n",
        "        \"learning_rate\": 5e-4,\n",
        "        \"n_steps\": 2048,\n",
        "        \"batch_size\": 64,\n",
        "        \"n_epochs\": 10,\n",
        "        \"gamma\": 0.99,\n",
        "        \"gae_lambda\": 0.95,\n",
        "        \"clip_range\": 0.2,\n",
        "        \"ent_coef\": 0.05,\n",
        "    },\n",
        "    sync_tensorboard=True,  # sincronizza TB e W&B\n",
        "    monitor_gym=True,       # logga anche l’ambiente se possibile\n",
        "    save_code=True,\n",
        ")\n",
        "\n",
        "# Policy kwargs\n",
        "policy_kwargs = dict(\n",
        "    features_extractor_class=CNN,\n",
        "    features_extractor_kwargs=dict(features_dim=256),\n",
        ")\n",
        "\n",
        "# Inizializza il modello\n",
        "model = PPO(\n",
        "    \"CnnPolicy\",\n",
        "    env,\n",
        "    policy_kwargs=policy_kwargs,\n",
        "    learning_rate=1e-4,\n",
        "    n_steps=2048,\n",
        "    batch_size=64,\n",
        "    n_epochs=10,\n",
        "    gamma=0.99,\n",
        "    gae_lambda=0.95,\n",
        "    clip_range=0.2,\n",
        "    ent_coef=0.04,\n",
        "    verbose=1,\n",
        "    tensorboard_log=\"./ppo_tb/\",\n",
        "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "695dd84e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Callback W&B (puoi combinare con altri callback come EvalCallback)\n",
        "wandb_callback = WandbCallback(\n",
        "    gradient_save_freq=100,\n",
        "    model_save_path=\"./models/\",\n",
        "    verbose=2,\n",
        ")\n",
        "\n",
        "# Avvia il training\n",
        "print(\"Starting training...\")\n",
        "model.learn(\n",
        "    total_timesteps=1_000_000,\n",
        "    callback=wandb_callback\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6b0dc41",
      "metadata": {},
      "outputs": [],
      "source": [
        "def record_agent_video(model, steps=1000, save_path=\"carracing_eval.mp4\"):\n",
        "    env = gym.make(\"CarRacing-v3\", render_mode=\"rgb_array\", continuous=False)\n",
        "    obs, _ = env.reset()\n",
        "    images = []\n",
        "    total_reward = 0\n",
        "    done = False\n",
        "\n",
        "    for _ in range(steps):\n",
        "        img = env.render()\n",
        "        images.append(img)\n",
        "\n",
        "        # Preprocessa l'osservazione per adattarla al formato atteso dal modello\n",
        "        # Il modello si aspetta (3, 84, 96)\n",
        "\n",
        "        # 1. Taglia l'immagine a 84 pixel di altezza (come nel tuo preprocessing)\n",
        "        obs_cropped = obs[:84, :, :]  # Da (96, 96, 3) a (84, 96, 3)\n",
        "\n",
        "        # 2. Trasforma da HWC a CHW\n",
        "        obs_processed = np.transpose(obs_cropped, (2, 0, 1))  # Da (84, 96, 3) a (3, 84, 96)\n",
        "\n",
        "        action, _ = model.predict(obs_processed, deterministic=True)\n",
        "        obs, reward, done, truncated, info = env.step(action)\n",
        "        total_reward += reward\n",
        "        if done or truncated:\n",
        "            break\n",
        "\n",
        "    env.close()\n",
        "    imageio.mimsave(save_path, images, fps=30)\n",
        "    return save_path, total_reward\n",
        "\n",
        "video_path, total_reward = record_agent_video(model, steps=1000)\n",
        "print(\"Total reward:\", total_reward)\n",
        "\n",
        "# Log su W&B\n",
        "wandb.init(project=\"carracing-ppo-baseline\", name=\"eval-video\")\n",
        "wandb.log({\"evaluation_reward\": total_reward})\n",
        "wandb.log({\"agent_play\": wandb.Video(video_path, fps=30, format=\"mp4\")})\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b78960e",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
