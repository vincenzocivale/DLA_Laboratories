{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a18c83b-448a-4679-8f19-0fa8d2c95508",
   "metadata": {},
   "source": [
    "# Getting up to speed with DRL\n",
    "\n",
    "In this notebook I provide a simple example of implementing a policy gradient Deep Reinforcement Learning algorithm to solve a control problem with continuous state space and discrete action space -- the venerable [CartPole environment](https://gymnasium.farama.org/environments/classic_control/cart_pole/). You should study the implementation in this notebook in preparation for the laboratory next Wednesday.\n",
    "\n",
    "This notebook should run in an environment with at least the following packages installed (the gpu version of PyTorch is not mandatory):\n",
    "\n",
    "     conda create -n DRL -c conda-forge gymnasium pytorch-gpu matplotlib pygame jupyterlab\n",
    "     \n",
    "Some background reading to get you started:\n",
    "\n",
    "1. We will be using the [Gymnasium](https://gymnasium.farama.org/) framework for all of our experiments. This framework provides a consistent interface to a broad range of reinforcement learning environments (including CartPole). You should familiarize yourself with how it works, how environments are specified, how to instantiate them, and how to interact with them.\n",
    "\n",
    "2. [This excellent blog post](http://karpathy.github.io/2016/05/31/rl/) is a great introduction to policy gradients, where they come from and how they work. Give it a read and I am sure it will help understand better what is going on in this notebook.\n",
    "\n",
    "## Preliminaries\n",
    "\n",
    "We start with our standard imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47b2a363-e3cd-4b22-8741-f3545e150281",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Standard imports.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Plus one non standard one -- we need this to sample from policies.\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "# This *might* help PyGame crash less...\n",
    "import pygame\n",
    "_ = pygame.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd56607-0c95-449c-8724-8db4e54788fd",
   "metadata": {},
   "source": [
    "And also some utility functions useful for what comes next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5c56fab-8522-40fa-b5f2-895b042c1c48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Given an environment, observation, and policy, sample from pi(a | obs). Returns the\n",
    "# selected action and the log probability of that action (needed for policy gradient).\n",
    "def select_action(env, obs, policy):\n",
    "    dist = Categorical(policy(obs))\n",
    "    action = dist.sample()\n",
    "    log_prob = dist.log_prob(action)\n",
    "    return (action.item(), log_prob.reshape(1))\n",
    "\n",
    "# Utility to compute the discounted total reward. Torch doesn't like flipped arrays, so we need to\n",
    "# .copy() the final numpy array. There's probably a better way to do this.\n",
    "def compute_returns(rewards, gamma):\n",
    "    return np.flip(np.cumsum([gamma**(i+1)*r for (i, r) in enumerate(rewards)][::-1]), 0).copy()\n",
    "\n",
    "# Given an environment and a policy, run it up to the maximum number of steps.\n",
    "def run_episode(env, policy, maxlen=500):\n",
    "    # Collect just about everything.\n",
    "    observations = []\n",
    "    actions = []\n",
    "    log_probs = []\n",
    "    rewards = []\n",
    "    \n",
    "    # Reset the environment and start the episode.\n",
    "    (obs, info) = env.reset()\n",
    "    for i in range(maxlen):\n",
    "        # Get the current observation, run the policy and select an action.\n",
    "        obs = torch.tensor(obs)\n",
    "        (action, log_prob) = select_action(env, obs, policy)\n",
    "        observations.append(obs)\n",
    "        actions.append(action)\n",
    "        log_probs.append(log_prob)\n",
    "        \n",
    "        # Advance the episode by executing the selected action.\n",
    "        (obs, reward, term, trunc, info) = env.step(action)\n",
    "        rewards.append(reward)\n",
    "        if term or trunc:\n",
    "            break\n",
    "    return (observations, actions, torch.cat(log_probs), rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62dea044-14e1-44f1-a851-ef6e19452692",
   "metadata": {},
   "source": [
    "## The Policy network\n",
    "\n",
    "Here I provide a simple policy network which should work with any environment with continuous observations and discrete action spaces. Note how it uses the *specification* of the environment to configure its input and output spaces. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a8c3360-3671-4075-8ebe-9dfd7f1d1a8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A simple, but generic, policy network with one hidden layer.\n",
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(env.observation_space.shape[0], 128)\n",
    "        self.fc2 = nn.Linear(128, env.action_space.n)\n",
    "        \n",
    "    def forward(self, s):\n",
    "        s = F.relu(self.fc1(s))\n",
    "        s = F.softmax(self.fc2(s), dim=-1)\n",
    "        return s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115e2af5-62fd-422e-8fb7-06295c49ddc7",
   "metadata": {},
   "source": [
    "## The `REINFORCE` Algorithm\n",
    "\n",
    "This is a very simple implementation of the most basic policy gradient DRL algorithm: `REINFORCE`. It is a very direct implementation of the policy gradient update (although I use Adam instead of SGD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09393c40-def8-47ca-9a3c-8c7689d5f87d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A direct, inefficient, and probably buggy of the REINFORCE policy gradient algorithm.\n",
    "def reinforce(policy, env, env_render=None, gamma=0.99, num_episodes=10):\n",
    "    # The only non-vanilla part: we use Adam instead of SGD.\n",
    "    opt = torch.optim.Adam(policy.parameters(), lr=1e-2)\n",
    "\n",
    "    # Track episode rewards in a list.\n",
    "    running_rewards = [0.0]\n",
    "    \n",
    "    # The main training loop.\n",
    "    policy.train()\n",
    "    for episode in range(num_episodes):\n",
    "        # Run an episode of the environment, collect everything needed for policy update.\n",
    "        (observations, actions, log_probs, rewards) = run_episode(env, policy)\n",
    "        \n",
    "        # Compute the discounted reward for every step of the episode. \n",
    "        returns = torch.tensor(compute_returns(rewards, gamma), dtype=torch.float32)\n",
    "        \n",
    "        # Keep a running average of total discounted rewards for the whole episode.\n",
    "        running_rewards.append(0.05 * returns[0].item() + 0.95 * running_rewards[-1])\n",
    "        \n",
    "        # Standardize returns.\n",
    "        returns = (returns - returns.mean()) / returns.std()\n",
    "        \n",
    "        # Make an optimization step\n",
    "        opt.zero_grad()\n",
    "        loss = (-log_probs * returns).mean()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        # Render an episode after every 100 policy updates.\n",
    "        if not episode % 100:\n",
    "            if env_render:\n",
    "                policy.eval()\n",
    "                run_episode(env_render, policy)\n",
    "                policy.train()\n",
    "            print(f'Running reward: {running_rewards[-1]}')\n",
    "    \n",
    "    # Return the running rewards.\n",
    "    policy.eval()\n",
    "    return running_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7086f041-a315-4f4e-9db6-01cb446fffa6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiate a (rendering) CartPole environment.\n",
    "env_render = gymnasium.make('CartPole-v1', render_mode='human')\n",
    "pygame.display.init()  # Might help PyGame not crash...\n",
    "\n",
    "# Make a policy network and run a few episodes to see how well random initialization works.\n",
    "policy = PolicyNet(env_render)\n",
    "for _ in range(10):\n",
    "    run_episode(env_render, policy)\n",
    "    \n",
    "# If you don't close the environment, the PyGame window stays visible.\n",
    "env_render.close()\n",
    "\n",
    "# Again we pray PyGame doesn't crash...\n",
    "pygame.display.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adeffa1e-d23d-46ea-a3e4-e65251592529",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# In the new version of Gymnasium you need different environments for rendering and no rendering.\n",
    "# Here we instaintiate two versions of cartpole, one that animates the episodes (which slows everything\n",
    "# down), and another that does not animate.\n",
    "seed = 2112\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "env = gymnasium.make('CartPole-v1')\n",
    "env.reset(seed=seed)\n",
    "env_render = None # gymnasium.make('CartPole-v1', render_mode='human')\n",
    "\n",
    "# PyGame, please don't crash.\n",
    "pygame.display.init()\n",
    "\n",
    "# Make a policy network.\n",
    "policy = PolicyNet(env)\n",
    "\n",
    "# Train the agent.\n",
    "plt.plot(reinforce(policy, env, env_render, num_episodes=1000))\n",
    "\n",
    "# Close up everything\n",
    "#env_render.close()\n",
    "env.close()\n",
    "pygame.display.quit()  # Fingers crossed..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e40868e-7f32-4409-9f07-1fe781de45da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# And run the final agent for a few episodes.\n",
    "env_render = gymnasium.make('CartPole-v1', render_mode='human')\n",
    "for _ in range(10):\n",
    "    run_episode(env_render, policy)\n",
    "env_render.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6202cea9-aef2-46da-ace9-ddf25bca77be",
   "metadata": {},
   "source": [
    "## For your consideration\n",
    "\n",
    "There are many things that can be improved in this example. Some things you can think about:\n",
    "\n",
    "1. **Development Environment**. If there is *one* laboratory for which you don't use a Jupyter Notebook, *this should definitely be the one*. PyGame (for visualizing episodes in the environment) interacts very badly with the Jupyter Python kernels at times. Consider using PyCharm or VSCode.\n",
    " \n",
    "2. **Better Logging**. Monitoring just a moving average return is a pretty coarse metric of performance -- one that with also *depend on the discount factor $\\gamma$*. Think about what might be better statistics to collect and report, and also *how to collect and report them*. You should think about using Weights and Biases or Tensorboard for this.\n",
    "\n",
    "3. **Checkpointing**. When training is unstable, it can be a good idea to *checkpoint* your models both periodically (so you can restart training from a specific checkpoint) and for each *best* model as determined by you running performance measure(s). \n",
    "\n",
    "4. **Exploration**. The model is probably overfitting (or perhaps remaining too *plastic*, which can explain the unstable convergence). Our policy is *always* stochastic in that we sample from the output distribution. It would be interesting to add a temperature parameter to the policy so that we can control this behavior, or even implement a deterministic policy sampler that always selects the action with max probability to evaluate the quality of the learned policy network.\n",
    "\n",
    "5. **Discount Factor**: The discount factor (default $\\gamma = 0.99$) is an important hyperparameter that has an effect on the stability of training. Try different values for $\\gamma$ and see how it affects training. Can you think of other ways to stabilize training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d2dd69-4806-44fb-acb4-1b26c0d93cf4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
