Starting training for 20 epochs
Device: cuda
Batch size: 128

Epoch 1/20
                                                                                                                
Train Loss: 2.3990, Val Metrics: {'loss': 1.2793010100722313, 'accuracy': 0.5478}
âœ“ Best model updated!

Epoch 2/20
Train Loss: 1.4616, Val Metrics: {'loss': 1.297536963224411, 'accuracy': 0.5862}
âœ“ Best model updated!

Epoch 3/20
Train Loss: 1.0950, Val Metrics: {'loss': 1.0273622795939445, 'accuracy': 0.66}
âœ“ Best model updated!

Epoch 4/20
Train Loss: 0.9036, Val Metrics: {'loss': 0.7599977433681488, 'accuracy': 0.7408}
âœ“ Best model updated!

Epoch 5/20
Train Loss: 0.7933, Val Metrics: {'loss': 0.8985732704401016, 'accuracy': 0.7082}
No improvement for 1 epochs

Epoch 6/20
Train Loss: 0.7162, Val Metrics: {'loss': 0.8282651782035828, 'accuracy': 0.7318}
No improvement for 2 epochs

Epoch 7/20
Train Loss: 0.6682, Val Metrics: {'loss': 0.6434436678886414, 'accuracy': 0.7848}
âœ“ Best model updated!

Epoch 8/20
Train Loss: 0.6315, Val Metrics: {'loss': 0.7658638849854469, 'accuracy': 0.7448}
No improvement for 1 epochs

Epoch 9/20
Train Loss: 0.5975, Val Metrics: {'loss': 0.6051010750234127, 'accuracy': 0.7932}
âœ“ Best model updated!

Epoch 10/20
Train Loss: 0.5781, Val Metrics: {'loss': 0.7339943781495094, 'accuracy': 0.7642}
No improvement for 1 epochs

Epoch 11/20
Train Loss: 0.5565, Val Metrics: {'loss': 0.5964657738804817, 'accuracy': 0.7966}
âœ“ Best model updated!

Epoch 12/20
Train Loss: 0.5432, Val Metrics: {'loss': 0.5730176396667958, 'accuracy': 0.8028}
âœ“ Best model updated!

Epoch 13/20
Train Loss: 0.5314, Val Metrics: {'loss': 0.5959741652011872, 'accuracy': 0.7922}
No improvement for 1 epochs

Epoch 14/20
Train Loss: 0.5305, Val Metrics: {'loss': 0.6343463003635407, 'accuracy': 0.7734}
No improvement for 2 epochs

Epoch 15/20
Train Loss: 0.5002, Val Metrics: {'loss': 0.577999809384346, 'accuracy': 0.8042}
âœ“ Best model updated!

Epoch 16/20
Train Loss: 0.4899, Val Metrics: {'loss': 0.5442349627614022, 'accuracy': 0.8092}
âœ“ Best model updated!

Epoch 17/20
Train Loss: 0.4832, Val Metrics: {'loss': 0.5150509931147098, 'accuracy': 0.819}
âœ“ Best model updated!

Epoch 18/20
Train Loss: 0.4750, Val Metrics: {'loss': 0.5552622392773628, 'accuracy': 0.805}
No improvement for 1 epochs

Epoch 19/20
Train Loss: 0.4696, Val Metrics: {'loss': 0.5692136004567147, 'accuracy': 0.8032}
No improvement for 2 epochs

Epoch 20/20
Train Loss: 0.4655, Val Metrics: {'loss': 0.5811493121087551, 'accuracy': 0.7996}
No improvement for 3 epochs
Loading best model weights

Final Test Metrics: {'loss': 0.549058354353603, 'accuracy': 0.8095}

Student (distilled) test accuracy: 0.8095
Model saved to models/rescnn_ch16_d2_c10_student_distilled.pth
============================================================
KNOWLEDGE DISTILLATION RESULTS
============================================================
Teacher Model (ResidualCNN depth=4):     0.8164
Student Baseline (ResidualCNN depth=2):  0.7833
Student Distilled (ResidualCNN depth=2): 0.8095
============================================================
Improvement from Knowledge Distillation: 0.0262 (2.62%)
Parameter reduction: 5.94% of teacher size

Knowledge distillation improved student performance by 0.0262!
   The distilled student achieved 0.8095 vs 0.7833 baseline

ðŸ“Š Knowledge transfer efficiency: 99.2%
   (Student achieved 99.2% of teacher performance with 5.9% of parameters)
