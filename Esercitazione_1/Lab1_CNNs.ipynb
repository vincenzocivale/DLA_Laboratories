{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d97f7c5d-46f3-4cbd-80ad-f1e50cd65096",
   "metadata": {
    "id": "d97f7c5d-46f3-4cbd-80ad-f1e50cd65096"
   },
   "source": [
    "# Deep Learning Applications: Laboratory #1\n",
    "\n",
    "In this first laboratory we will work with relatively simple architectures to get a feel for working with Deep Models. This notebook is designed to work with PyTorch, but as I said in the introductory lecture: please feel free to use and experiment with whatever tools you like.\n",
    "\n",
    "**Important Notes**:\n",
    "1. Be sure to **document** all of your decisions, as well as your intermediate and final results. Make sure your conclusions and analyses are clearly presented. Don't make us dig into your code or walls of printed results to try to draw conclusions from your code.\n",
    "2. If you use code from someone else (e.g. Github, Stack Overflow, ChatGPT, etc) you **must be transparent about it**. Document your sources and explain how you adapted any partial solutions to create **your** solution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ed8906-bd19-4b4f-8b79-4feae355ffd6",
   "metadata": {
    "id": "17ed8906-bd19-4b4f-8b79-4feae355ffd6"
   },
   "source": [
    "## Exercise 1: Warming Up\n",
    "In this series of exercises I want you to try to duplicate (on a small scale) the results of the ResNet paper:\n",
    "\n",
    "> [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385), Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun, CVPR 2016.\n",
    "\n",
    "We will do this in steps using a Multilayer Perceptron on MNIST.\n",
    "\n",
    "Recall that the main message of the ResNet paper is that **deeper** networks do not **guarantee** more reduction in training loss (or in validation accuracy). Below you will incrementally build a sequence of experiments to verify this for an MLP. A few guidelines:\n",
    "\n",
    "+ I have provided some **starter** code at the beginning. **NONE** of this code should survive in your solutions. Not only is it **very** badly written, it is also written in my functional style that also obfuscates what it's doing (in part to **discourage** your reuse!). It's just to get you *started*.\n",
    "+ These exercises ask you to compare **multiple** training runs, so it is **really** important that you factor this into your **pipeline**. Using [Tensorboard](https://pytorch.org/tutorials/recipes/recipes/tensorboard_with_pytorch.html) is a **very** good idea -- or, even better [Weights and Biases](https://wandb.ai/site).\n",
    "+ You may work and submit your solutions in **groups of at most two**. Share your ideas with everyone, but the solutions you submit *must be your own*.\n",
    "\n",
    "First some boilerplate to get you started, then on to the actual exercises!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb2b6d1-3df0-464c-9a5f-8c611257a971",
   "metadata": {
    "id": "edb2b6d1-3df0-464c-9a5f-8c611257a971"
   },
   "source": [
    "### Preface: Some code to get you started\n",
    "\n",
    "What follows is some **very simple** code for training an MLP on MNIST. The point of this code is to get you up and running (and to verify that your Python environment has all needed dependencies).\n",
    "\n",
    "**Note**: As you read through my code and execute it, this would be a good time to think about *abstracting* **your** model definition, and training and evaluation pipelines in order to make it easier to compare performance of different models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab3a8282-2322-4dca-b76e-2f3863bc75fb",
   "metadata": {
    "id": "ab3a8282-2322-4dca-b76e-2f3863bc75fb"
   },
   "outputs": [],
   "source": [
    "# Import standard libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import copy\n",
    "from functools import reduce\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List, Optional, Callable, Dict, Any\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from torch.optim import Adam, SGD\n",
    "\n",
    "# Torchvision imports\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST, CIFAR10\n",
    "\n",
    "# Third-party imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import moduli locali\n",
    "from src.models import MLP, ResidualMLP, SimpleCNN, ResidualCNN\n",
    "from src.trainer import StreamlinedTrainer\n",
    "from src.data import get_data_transforms, TransformedSubset\n",
    "from src.utils import compute_accuracy_metrics\n",
    "from src.config import TrainingConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cc12cc-8422-47bf-8d8e-0950ac05ae96",
   "metadata": {
    "id": "33cc12cc-8422-47bf-8d8e-0950ac05ae96"
   },
   "source": [
    "#### Data preparation\n",
    "\n",
    "Here is some basic dataset loading, validation splitting code to get you started working with MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272a69db-0416-444a-9be4-5f055ff48bbb",
   "metadata": {
    "id": "272a69db-0416-444a-9be4-5f055ff48bbb"
   },
   "outputs": [],
   "source": [
    "# # Standard MNIST transform.\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize((0.1307,), (0.3081,))\n",
    "# ])\n",
    "\n",
    "# # Load MNIST train and test.\n",
    "# ds_train = MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "# ds_test = MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# # Split train into train and validation.\n",
    "# val_size = 5000\n",
    "# I = np.random.permutation(len(ds_train))\n",
    "# ds_val = Subset(ds_train, I[:val_size])\n",
    "# ds_train = Subset(ds_train, I[val_size:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e05e96-7707-4490-98b8-50cb5e330af1",
   "metadata": {
    "id": "24e05e96-7707-4490-98b8-50cb5e330af1"
   },
   "source": [
    "#### Boilerplate training and evaluation code\n",
    "\n",
    "This is some **very** rough training, evaluation, and plotting code. Again, just to get you started. I will be *very* disappointed if any of this code makes it into your final submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcce348-f603-4d57-b9a8-5b1c6eba28ae",
   "metadata": {
    "id": "dbcce348-f603-4d57-b9a8-5b1c6eba28ae"
   },
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "# from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# # Function to train a model for a single epoch over the data loader.\n",
    "# def train_epoch(model, dl, opt, epoch='Unknown', device='cpu'):\n",
    "#     model.train()\n",
    "#     losses = []\n",
    "#     for (xs, ys) in tqdm(dl, desc=f'Training epoch {epoch}', leave=True):\n",
    "#         xs = xs.to(device)\n",
    "#         ys = ys.to(device)\n",
    "#         opt.zero_grad()\n",
    "#         logits = model(xs)\n",
    "#         loss = F.cross_entropy(logits, ys)\n",
    "#         loss.backward()\n",
    "#         opt.step()\n",
    "#         losses.append(loss.item())\n",
    "#     return np.mean(losses)\n",
    "\n",
    "# # Function to evaluate model over all samples in the data loader.\n",
    "# def evaluate_model(model, dl, device='cpu'):\n",
    "#     model.eval()\n",
    "#     predictions = []\n",
    "#     gts = []\n",
    "#     for (xs, ys) in tqdm(dl, desc='Evaluating', leave=False):\n",
    "#         xs = xs.to(device)\n",
    "#         preds = torch.argmax(model(xs), dim=1)\n",
    "#         gts.append(ys)\n",
    "#         predictions.append(preds.detach().cpu().numpy())\n",
    "\n",
    "#     # Return accuracy score and classification report.\n",
    "#     return (accuracy_score(np.hstack(gts), np.hstack(predictions)),\n",
    "#             classification_report(np.hstack(gts), np.hstack(predictions), zero_division=0, digits=3))\n",
    "\n",
    "# # Simple function to plot the loss curve and validation accuracy.\n",
    "# def plot_validation_curves(losses_and_accs):\n",
    "#     losses = [x for (x, _) in losses_and_accs]\n",
    "#     accs = [x for (_, x) in losses_and_accs]\n",
    "#     plt.figure(figsize=(16, 8))\n",
    "#     plt.subplot(1, 2, 1)\n",
    "#     plt.plot(losses)\n",
    "#     plt.xlabel('Epoch')\n",
    "#     plt.ylabel('Loss')\n",
    "#     plt.title('Average Training Loss per Epoch')\n",
    "#     plt.subplot(1, 2, 2)\n",
    "#     plt.plot(accs)\n",
    "#     plt.xlabel('Epoch')\n",
    "#     plt.ylabel('Validation Accuracy')\n",
    "#     plt.title(f'Best Accuracy = {np.max(accs)} @ epoch {np.argmax(accs)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875008c3-306c-4e39-a845-d7bda7862621",
   "metadata": {
    "id": "875008c3-306c-4e39-a845-d7bda7862621"
   },
   "source": [
    "#### A basic, parameterized MLP\n",
    "\n",
    "This is a very basic implementation of a Multilayer Perceptron. Don't waste too much time trying to figure out how it works -- the important detail is that it allows you to pass in a list of input, hidden layer, and output *widths*. **Your** implementation should also support this for the exercises to come."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1e503a-37df-4fb9-94e7-85d0adb494bd",
   "metadata": {
    "id": "8c1e503a-37df-4fb9-94e7-85d0adb494bd"
   },
   "outputs": [],
   "source": [
    "# class MLP(nn.Module):\n",
    "#     def __init__(self, layer_sizes):\n",
    "#         super().__init__()\n",
    "#         self.layers = nn.ModuleList([nn.Linear(nin, nout) for (nin, nout) in zip(layer_sizes[:-1], layer_sizes[1:])])\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return reduce(lambda f, g: lambda x: g(F.relu(f(x))), self.layers, lambda x: x.flatten(1))(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae06e26-8fa3-414e-a502-8d1c18ba9eb7",
   "metadata": {
    "id": "4ae06e26-8fa3-414e-a502-8d1c18ba9eb7"
   },
   "source": [
    "#### A *very* minimal training pipeline.\n",
    "\n",
    "Here is some basic training and evaluation code to get you started.\n",
    "\n",
    "**Important**: I cannot stress enough that this is a **terrible** example of how to implement a training pipeline. You can do better!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc89e48f-d8f3-4122-842d-1ff389499854",
   "metadata": {
    "id": "fc89e48f-d8f3-4122-842d-1ff389499854"
   },
   "outputs": [],
   "source": [
    "# # Training hyperparameters.\n",
    "# device = 'cuda' if torch.cuda.is_available else 'cpu'\n",
    "# epochs = 100\n",
    "# lr = 0.0001\n",
    "# batch_size = 128\n",
    "\n",
    "# # Architecture hyperparameters.\n",
    "# input_size = 28*28\n",
    "# width = 16\n",
    "# depth = 2\n",
    "\n",
    "# # Dataloaders.\n",
    "# dl_train = torch.utils.data.DataLoader(ds_train, batch_size, shuffle=True, num_workers=4)\n",
    "# dl_val   = torch.utils.data.DataLoader(ds_val, batch_size, num_workers=4)\n",
    "# dl_test  = torch.utils.data.DataLoader(ds_test, batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "# # Instantiate model and optimizer.\n",
    "# model_mlp = MLP([input_size] + [width]*depth + [10]).to(device)\n",
    "# opt = torch.optim.Adam(params=model_mlp.parameters(), lr=lr)\n",
    "\n",
    "# # Training loop.\n",
    "# losses_and_accs = []\n",
    "# for epoch in range(epochs):\n",
    "#     loss = train_epoch(model_mlp, dl_train, opt, epoch, device=device)\n",
    "#     (val_acc, _) = evaluate_model(model_mlp, dl_val, device=device)\n",
    "#     losses_and_accs.append((loss, val_acc))\n",
    "\n",
    "# # And finally plot the curves.\n",
    "# plot_validation_curves(losses_and_accs)\n",
    "# print(f'Accuracy report on TEST:\\n {evaluate_model(model_mlp, dl_test, device=device)[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7hLvteozXetr",
   "metadata": {
    "id": "7hLvteozXetr"
   },
   "source": [
    "#### My Streamlined Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GeYeYhR9TzFj",
   "metadata": {
    "id": "GeYeYhR9TzFj"
   },
   "outputs": [],
   "source": [
    "from typing import List, Optional, Callable, Any, Dict\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import os\n",
    "import copy\n",
    "\n",
    "# Le classi TrainingConfig, get_data_transforms, TransformedSubset e StreamlinedTrainer sono ora importate dai moduli locali.\n",
    "# Utilizza direttamente queste classi e funzioni per la configurazione, la preparazione dei dati e il training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2cad13-ee2c-4e43-b5c7-31760da8c2df",
   "metadata": {
    "id": "de2cad13-ee2c-4e43-b5c7-31760da8c2df"
   },
   "source": [
    "### Exercise 1.1: A baseline MLP\n",
    "\n",
    "Implement a *simple* Multilayer Perceptron to classify the 10 digits of MNIST (e.g. two *narrow* layers). Use my code above as inspiration, but implement your own training pipeline -- you will need it later. Train this model to convergence, monitoring (at least) the loss and accuracy on the training and validation sets for every epoch. Below I include a basic implementation to get you started -- remember that you should write your *own* pipeline!\n",
    "\n",
    "**Note**: This would be a good time to think about *abstracting* your model definition, and training and evaluation pipelines in order to make it easier to compare performance of different models.\n",
    "\n",
    "**Important**: Given the *many* runs you will need to do, and the need to *compare* performance between them, this would **also** be a great point to study how **Tensorboard** or **Weights and Biases** can be used for performance monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d96405-36e7-4074-803c-fb02576cd528",
   "metadata": {
    "id": "19d96405-36e7-4074-803c-fb02576cd528"
   },
   "outputs": [],
   "source": [
    "# La classe MLP è ora importata da src.models. Utilizza direttamente MLP per definire i modelli MLP personalizzati."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eXQZJvbFX3dS",
   "metadata": {
    "id": "eXQZJvbFX3dS"
   },
   "source": [
    "#### Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6eaMN-xY8RW",
   "metadata": {
    "id": "d6eaMN-xY8RW"
   },
   "outputs": [],
   "source": [
    "from src.utils import compute_accuracy_metrics\n",
    "\n",
    "# Il resto del tuo codice rimane invariato, dato che ora utilizziamo la funzione importata per il calcolo delle metriche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lWTLzNc7X2U4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 387
    },
    "id": "lWTLzNc7X2U4",
    "outputId": "4221f498-8dff-4c2f-dcfb-3e01139f2df1"
   },
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "ds_train = MNIST(root='./data', train=True, download=True)\n",
    "ds_test = MNIST(root='./data', train=False, download=True)\n",
    "\n",
    "# Create streamlined training configuration\n",
    "config = TrainingConfig(\n",
    "    num_epochs=60,\n",
    "    batch_size=256,\n",
    "    learning_rate=1e-3,\n",
    "    use_early_stopping=True,\n",
    "    early_stopping_patience=10,\n",
    "    validation_split=0.1,\n",
    "    data_augmentation=[\"RandomRotation\", \"RandomAffine\"],\n",
    "    wandb_project=\"DeepLearningApplication_Lab1\"\n",
    ")\n",
    "\n",
    "# Define model architecture\n",
    "model = MLP(\n",
    "    input_size=28 * 28,\n",
    "    output_size=10,\n",
    "    hidden_sizes=[128, 64],\n",
    "    activation_fn=nn.ReLU,\n",
    "    dropout_rate=0.1\n",
    ")\n",
    "\n",
    "# Create trainer and start training\n",
    "trainer = StreamlinedTrainer(\n",
    "    model=model,\n",
    "    config=config,\n",
    "    train_dataset=ds_train,\n",
    "    test_dataset=ds_test,\n",
    "    compute_metrics_fn=compute_accuracy_metrics\n",
    ")\n",
    "\n",
    "print(f\"Model: {model}\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69-BgruAZdec",
   "metadata": {
    "id": "69-BgruAZdec"
   },
   "outputs": [],
   "source": [
    "# Start training\n",
    "final_metrics = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb8ad9b-e3ae-4c49-9bec-35aaea149b08",
   "metadata": {
    "id": "0fb8ad9b-e3ae-4c49-9bec-35aaea149b08"
   },
   "source": [
    "### Exercise 1.2: Adding Residual Connections\n",
    "\n",
    "Implement a variant of your parameterized MLP network to support **residual** connections. Your network should be defined as a composition of **residual MLP** blocks that have one or more linear layers and add a skip connection from the block input to the output of the final linear layer.\n",
    "\n",
    "**Compare** the performance (in training/validation loss and test accuracy) of your MLP and ResidualMLP for a range of depths. Verify that deeper networks **with** residual connections are easier to train than a network of the same depth **without** residual connections.\n",
    "\n",
    "**For extra style points**: See if you can explain by analyzing the gradient magnitudes on a single training batch *why* this is the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bcff82-756a-4ffa-92ae-a939fa21f5fd",
   "metadata": {
    "id": "90bcff82-756a-4ffa-92ae-a939fa21f5fd"
   },
   "outputs": [],
   "source": [
    "# Le classi ResidualBlock e ResidualMLP sono ora importate da src.models. Utilizza direttamente ResidualMLP per i modelli MLP con connessioni residue.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1TUnzgS1n-i",
   "metadata": {
    "id": "c1TUnzgS1n-i"
   },
   "source": [
    "#### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7gk_4bqTnjXJ",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "7gk_4bqTnjXJ"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Comparison between MLP and ResidualMLP with different depths\n",
    "def compare_mlp_architectures():\n",
    "    \"\"\"Compare standard MLP vs ResidualMLP across different depths.\"\"\"\n",
    "    \n",
    "    # Load datasets\n",
    "    ds_train = MNIST(root='./data', train=True, download=True)\n",
    "    ds_test = MNIST(root='./data', train=False, download=True)\n",
    "    \n",
    "    # Training configuration\n",
    "    config = TrainingConfig(\n",
    "        num_epochs=50,\n",
    "        batch_size=256,\n",
    "        learning_rate=1e-3,\n",
    "        use_early_stopping=True,\n",
    "        early_stopping_patience=8,\n",
    "        wandb_project=\"MLP_vs_ResidualMLP_Comparison\"\n",
    "    )\n",
    "    \n",
    "    # Test different architectures\n",
    "    architectures = [\n",
    "        {\"hidden_sizes\": [128, 64], \"name\": \"shallow\"},\n",
    "        {\"hidden_sizes\": [128, 64, 64, 32], \"name\": \"medium\"},\n",
    "        {\"hidden_sizes\": [128, 128, 64, 64, 32, 32, 16, 16], \"name\": \"deep\"}\n",
    "    ]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for arch in architectures:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Testing {arch['name']} architecture\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # # Standard MLP\n",
    "        # mlp = MLP(\n",
    "        #     input_size=28 * 28,\n",
    "        #     output_size=10,\n",
    "        #     hidden_sizes=arch[\"hidden_sizes\"],\n",
    "        #     dropout_rate=0.1\n",
    "        # )\n",
    "        \n",
    "        # print(f\"\\n--- Standard MLP ---\")\n",
    "        # print(f\"Parameters: {sum(p.numel() for p in mlp.parameters()):,}\")\n",
    "        \n",
    "        # trainer_mlp = StreamlinedTrainer(\n",
    "        #     model=mlp,\n",
    "        #     config=config,\n",
    "        #     train_dataset=ds_train,\n",
    "        #     test_dataset=ds_test,\n",
    "        #     compute_metrics_fn=compute_accuracy_metrics\n",
    "        # )\n",
    "        \n",
    "        # mlp_metrics = trainer_mlp.train()\n",
    "        \n",
    "        # ResidualMLP (approximate same complexity)\n",
    "        res_mlp = ResidualMLP(\n",
    "            input_size=28 * 28,\n",
    "            output_size=10,\n",
    "            hidden_sizes=arch[\"hidden_sizes\"],  # Use first hidden size\n",
    "            # num_blocks=3,\n",
    "            layers_per_block=2,\n",
    "            dropout_rate=0.1\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n--- Residual MLP ---\")\n",
    "        print(f\"Parameters: {sum(p.numel() for p in res_mlp.parameters()):,}\")\n",
    "        \n",
    "        trainer_res = StreamlinedTrainer(\n",
    "            model=res_mlp,\n",
    "            config=config,\n",
    "            train_dataset=ds_train,\n",
    "            test_dataset=ds_test,\n",
    "            compute_metrics_fn=compute_accuracy_metrics\n",
    "        )\n",
    "        \n",
    "        res_metrics = trainer_res.train()\n",
    "        \n",
    "        # Store results\n",
    "        results[arch[\"name\"]] = {\n",
    "            # \"mlp\": mlp_metrics,\n",
    "            \"residual_mlp\": res_metrics\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n--- Results Summary for {arch['name']} ---\")\n",
    "        # print(f\"Standard MLP Test Accuracy: {mlp_metrics['accuracy']:.4f}\")\n",
    "        print(f\"Residual MLP Test Accuracy: {res_metrics['accuracy']:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run the comparison\n",
    "results = compare_mlp_architectures()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c59bdd8-3377-4311-b45f-511c2fb0b53e",
   "metadata": {
    "id": "3c59bdd8-3377-4311-b45f-511c2fb0b53e"
   },
   "source": [
    "### Exercise 1.3: Rinse and Repeat (but with a CNN)\n",
    "\n",
    "Repeat the verification you did above, but with **Convolutional** Neural Networks. If you were careful about abstracting your model and training code, this should be a simple exercise. Show that **deeper** CNNs *without* residual connections do not always work better and **even deeper** ones *with* residual connections.\n",
    "\n",
    "**Hint**: You probably should do this exercise using CIFAR-10, since MNIST is *very* easy (at least up to about 99% accuracy).\n",
    "\n",
    "**Tip**: Feel free to reuse the ResNet building blocks defined in `torchvision.models.resnet` (e.g. [BasicBlock](https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py#L59) which handles the cascade of 3x3 convolutions, skip connections, and optional downsampling). This is an excellent exercise in code diving.\n",
    "\n",
    "**Spoiler**: Depending on the optional exercises you plan to do below, you should think *very* carefully about the architectures of your CNNs here (so you can reuse them!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8baa0e-b17f-4a77-8a88-dadfdc6763ea",
   "metadata": {
    "id": "3c8baa0e-b17f-4a77-8a88-dadfdc6763ea"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple CNN with configurable depth for image classification.\n",
    "    \n",
    "    Args:\n",
    "        in_channels: Number of input channels (1 for MNIST, 3 for CIFAR)\n",
    "        num_classes: Number of output classes\n",
    "        depth: Number of convolutional blocks\n",
    "        initial_channels: Number of channels in first conv layer\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels: int = 3, num_classes: int = 10, depth: int = 3, initial_channels: int = 16):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.depth = depth\n",
    "        self.initial_channels = initial_channels\n",
    "        self.in_channels = in_channels\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Build feature extractor\n",
    "        layers = []\n",
    "        current_channels = initial_channels\n",
    "        \n",
    "        # Stem layer\n",
    "        layers.extend([\n",
    "            nn.Conv2d(in_channels, current_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(current_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ])\n",
    "        \n",
    "        # Convolutional blocks with downsampling\n",
    "        for i in range(depth):\n",
    "            next_channels = current_channels * 2\n",
    "            layers.extend([\n",
    "                nn.Conv2d(current_channels, next_channels, kernel_size=3, stride=2, padding=1),\n",
    "                nn.BatchNorm2d(next_channels),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ])\n",
    "            current_channels = next_channels\n",
    "        \n",
    "        self.features = nn.Sequential(*layers)\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.classifier = nn.Linear(current_channels, num_classes)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.features(x)\n",
    "        x = self.global_pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return self.classifier(x)\n",
    "    \n",
    "    def __str__(self) -> str:\n",
    "        return f\"simplecnn_ch{self.initial_channels}_d{self.depth}_c{self.num_classes}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4JpfrXrmyghQ",
   "metadata": {
    "id": "4JpfrXrmyghQ"
   },
   "outputs": [],
   "source": [
    "# La classe ResidualCNN è ora importata da src.models. Utilizza direttamente ResidualCNN per i modelli CNN con connessioni residue."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saYm5O381toe",
   "metadata": {
    "id": "saYm5O381toe"
   },
   "source": [
    "#### Training Loop with CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WLQJDp0Hzawn",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "WLQJDp0Hzawn",
    "outputId": "4701ec32-b136-47e4-d6b2-3d3948a44c35"
   },
   "outputs": [],
   "source": [
    "def compare_cnn_architectures():\n",
    "    \"\"\"Compare SimpleCNN vs ResidualCNN on CIFAR-10.\"\"\"\n",
    "    \n",
    "    # CIFAR-10 specific transforms\n",
    "    def get_cifar_transforms(is_training: bool = True):\n",
    "        if is_training:\n",
    "            return transforms.Compose([\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.RandomRotation(degrees=10),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "            ])\n",
    "        else:\n",
    "            return transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "            ])\n",
    "    \n",
    "    # Override transforms for CIFAR-10\n",
    "    def get_data_transforms_cifar(config, is_training=True):\n",
    "        return get_cifar_transforms(is_training)\n",
    "    \n",
    "    # Load CIFAR-10 datasets  \n",
    "    ds_train = CIFAR10(root='./data', train=True, download=True)\n",
    "    ds_test = CIFAR10(root='./data', train=False, download=True)\n",
    "    \n",
    "    # Configuration for CIFAR-10\n",
    "    config = TrainingConfig(\n",
    "        num_epochs=60,\n",
    "        batch_size=128,  # Smaller batch size for CIFAR-10\n",
    "        learning_rate=1e-3,\n",
    "        use_early_stopping=True,\n",
    "        early_stopping_patience=10,\n",
    "        validation_split=0.1,\n",
    "        data_augmentation=[],  # We handle augmentation in transforms\n",
    "        wandb_project=\"CNN_Comparison_CIFAR10\"\n",
    "    )\n",
    "    \n",
    "    depths = [2, 3, 4]\n",
    "    results = {}\n",
    "    \n",
    "    for depth in depths:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Testing depth {depth}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Simple CNN\n",
    "        simple_cnn = SimpleCNN(\n",
    "            in_channels=3,\n",
    "            num_classes=10,\n",
    "            depth=depth,\n",
    "            initial_channels=32\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n--- Simple CNN (depth {depth}) ---\")\n",
    "        print(f\"Parameters: {sum(p.numel() for p in simple_cnn.parameters()):,}\")\n",
    "        \n",
    "        # Temporarily override transform function for CIFAR-10\n",
    "        import types\n",
    "        trainer_simple = StreamlinedTrainer(\n",
    "            model=simple_cnn,\n",
    "            config=config,\n",
    "            train_dataset=ds_train,\n",
    "            test_dataset=ds_test,\n",
    "            compute_metrics_fn=compute_accuracy_metrics\n",
    "        )\n",
    "        \n",
    "        # Override the transform creation method for CIFAR-10\n",
    "        trainer_simple._setup_data_loaders = types.MethodType(\n",
    "            lambda self, train_dataset: self._setup_cifar_loaders(train_dataset), \n",
    "            trainer_simple\n",
    "        )\n",
    "        trainer_simple._setup_test_loader = types.MethodType(\n",
    "            lambda self, test_dataset: self._setup_cifar_test_loader(test_dataset), \n",
    "            trainer_simple\n",
    "        )\n",
    "        \n",
    "        # Add CIFAR-specific methods\n",
    "        def _setup_cifar_loaders(self, train_dataset):\n",
    "            targets = np.array(train_dataset.targets)\n",
    "            train_indices, val_indices = train_test_split(\n",
    "                np.arange(len(targets)), test_size=self.config.validation_split,\n",
    "                stratify=targets, random_state=self.config.seed\n",
    "            )\n",
    "            \n",
    "            train_subset = TransformedSubset(train_dataset, train_indices, get_cifar_transforms(True))\n",
    "            val_subset = TransformedSubset(train_dataset, val_indices, get_cifar_transforms(False))\n",
    "            \n",
    "            train_loader = DataLoader(train_subset, batch_size=self.config.batch_size, \n",
    "                                    shuffle=True, num_workers=self.config.num_workers, \n",
    "                                    pin_memory=self.config.pin_memory)\n",
    "            val_loader = DataLoader(val_subset, batch_size=self.config.batch_size, \n",
    "                                  shuffle=False, num_workers=self.config.num_workers, \n",
    "                                  pin_memory=self.config.pin_memory)\n",
    "            \n",
    "            print(f\"Training set size: {len(train_subset)}\")\n",
    "            print(f\"Validation set size: {len(val_subset)}\")\n",
    "            return train_loader, val_loader\n",
    "        \n",
    "        def _setup_cifar_test_loader(self, test_dataset):\n",
    "            class TransformedDataset(Dataset):\n",
    "                def __init__(self, dataset, transform):\n",
    "                    self.dataset = dataset\n",
    "                    self.transform = transform\n",
    "                def __len__(self):\n",
    "                    return len(self.dataset)\n",
    "                def __getitem__(self, idx):\n",
    "                    img, label = self.dataset[idx]\n",
    "                    return self.transform(img), label\n",
    "            \n",
    "            transformed_test = TransformedDataset(test_dataset, get_cifar_transforms(False))\n",
    "            return DataLoader(transformed_test, batch_size=self.config.batch_size, \n",
    "                            shuffle=False, num_workers=self.config.num_workers, \n",
    "                            pin_memory=self.config.pin_memory)\n",
    "        \n",
    "        trainer_simple._setup_cifar_loaders = types.MethodType(_setup_cifar_loaders, trainer_simple)\n",
    "        trainer_simple._setup_cifar_test_loader = types.MethodType(_setup_cifar_test_loader, trainer_simple)\n",
    "        \n",
    "        # Re-setup dataloaders with CIFAR transforms\n",
    "        trainer_simple.train_loader, trainer_simple.val_loader = trainer_simple._setup_cifar_loaders(ds_train)\n",
    "        trainer_simple.test_loader = trainer_simple._setup_cifar_test_loader(ds_test)\n",
    "        \n",
    "        simple_metrics = trainer_simple.train()\n",
    "        \n",
    "        # Residual CNN\n",
    "        residual_cnn = ResidualCNN(\n",
    "            in_channels=3,\n",
    "            num_classes=10,\n",
    "            depth=depth,\n",
    "            initial_channels=32\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n--- Residual CNN (depth {depth}) ---\")\n",
    "        print(f\"Parameters: {sum(p.numel() for p in residual_cnn.parameters()):,}\")\n",
    "        \n",
    "        trainer_res = StreamlinedTrainer(\n",
    "            model=residual_cnn,\n",
    "            config=config,\n",
    "            train_dataset=ds_train,\n",
    "            test_dataset=ds_test,\n",
    "            compute_metrics_fn=compute_accuracy_metrics\n",
    "        )\n",
    "        \n",
    "        # Apply same CIFAR-10 specific setup\n",
    "        trainer_res._setup_cifar_loaders = types.MethodType(_setup_cifar_loaders, trainer_res)\n",
    "        trainer_res._setup_cifar_test_loader = types.MethodType(_setup_cifar_test_loader, trainer_res)\n",
    "        trainer_res.train_loader, trainer_res.val_loader = trainer_res._setup_cifar_loaders(ds_train)\n",
    "        trainer_res.test_loader = trainer_res._setup_cifar_test_loader(ds_test)\n",
    "        \n",
    "        res_metrics = trainer_res.train()\n",
    "        \n",
    "        # Store results\n",
    "        results[depth] = {\n",
    "            \"simple_cnn\": simple_metrics,\n",
    "            \"residual_cnn\": res_metrics\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n--- Results Summary for depth {depth} ---\")\n",
    "        print(f\"Simple CNN Test Accuracy: {simple_metrics['accuracy']:.4f}\")\n",
    "        print(f\"Residual CNN Test Accuracy: {res_metrics['accuracy']:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run the comparison (uncomment to execute)\n",
    "results = compare_cnn_architectures()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4de2f2-abc5-4f98-9eaf-3497f734a022",
   "metadata": {
    "id": "ef4de2f2-abc5-4f98-9eaf-3497f734a022"
   },
   "source": [
    "-----\n",
    "## Exercise 2: Choose at Least One\n",
    "\n",
    "Below are **three** exercises that ask you to deepen your understanding of Deep Networks for visual recognition. You must choose **at least one** of the below for your final submission -- feel free to do **more**, but at least **ONE** you must submit. Each exercise is designed to require you to dig your hands **deep** into the guts of your models in order to do new and interesting things.\n",
    "\n",
    "**Note**: These exercises are designed to use your small, custom CNNs and small datasets. This is to keep training times reasonable. If you have a decent GPU, feel free to use pretrained ResNets and larger datasets (e.g. the [Imagenette](https://pytorch.org/vision/0.20/generated/torchvision.datasets.Imagenette.html#torchvision.datasets.Imagenette) dataset at 160px)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440a3a7b-2ed6-4f58-a1b7-5ab1fc432893",
   "metadata": {
    "id": "440a3a7b-2ed6-4f58-a1b7-5ab1fc432893"
   },
   "source": [
    "### Exercise 2.2: *Distill* the knowledge from a large model into a smaller one\n",
    "In this exercise you will see if you can derive a *small* model that performs comparably to a larger one on CIFAR-10. To do this, you will use [Knowledge Distillation](https://arxiv.org/abs/1503.02531):\n",
    "\n",
    "> Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the Knowledge in a Neural Network, NeurIPS 2015.\n",
    "\n",
    "To do this:\n",
    "1. Train one of your best-performing CNNs on CIFAR-10 from Exercise 1.3 above. This will be your **teacher** model.\n",
    "2. Define a *smaller* variant with about half the number of parameters (change the width and/or depth of the network). Train it on CIFAR-10 and verify that it performs *worse* than your **teacher**. This small network will be your **student** model.\n",
    "3. Train the **student** using a combination of **hard labels** from the CIFAR-10 training set (cross entropy loss) and **soft labels** from predictions of the **teacher** (Kulback-Leibler loss between teacher and student).\n",
    "\n",
    "Try to optimize training parameters in order to maximize the performance of the student. It should at least outperform the student trained only on hard labels in Setp 2.\n",
    "\n",
    "**Tip**: You can save the predictions of the trained teacher network on the training set and adapt your dataloader to provide them together with hard labels. This will **greatly** speed up training compared to performing a forward pass through the teacher for each batch of training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686a1371",
   "metadata": {},
   "source": [
    "## Exercise 2.2: Knowledge Distillation\n",
    "\n",
    "In this exercise, we'll implement knowledge distillation to transfer knowledge from a large **teacher** model to a smaller **student** model.\n",
    "\n",
    "**Steps:**\n",
    "1. Train a ResidualCNN with depth 4 as the **teacher** model\n",
    "2. Create a smaller **student** model with ~half the parameters  \n",
    "3. Train the student using both hard labels and soft labels from the teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c6b4d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Teacher model parameters: 2,793,114\n",
      "Training teacher model...\n",
      "Training set size: 45000\n",
      "Validation set size: 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvincenzo-civale\u001b[0m (\u001b[33mvincenzo-civale-universi-degli-studi-di-firenze\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/vcivale/prova/Esercitazione_1/wandb/run-20250616_105226-qury76v2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/vincenzo-civale-universi-degli-studi-di-firenze/dl-lab1-knowledge-distillation/runs/qury76v2' target=\"_blank\">rescnn_ch16_d4_c10</a></strong> to <a href='https://wandb.ai/vincenzo-civale-universi-degli-studi-di-firenze/dl-lab1-knowledge-distillation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/vincenzo-civale-universi-degli-studi-di-firenze/dl-lab1-knowledge-distillation' target=\"_blank\">https://wandb.ai/vincenzo-civale-universi-degli-studi-di-firenze/dl-lab1-knowledge-distillation</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/vincenzo-civale-universi-degli-studi-di-firenze/dl-lab1-knowledge-distillation/runs/qury76v2' target=\"_blank\">https://wandb.ai/vincenzo-civale-universi-degli-studi-di-firenze/dl-lab1-knowledge-distillation/runs/qury76v2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for 15 epochs\n",
      "Device: cuda\n",
      "Batch size: 128\n",
      "\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.5340, Val Metrics: {'loss': 1.2635868400335313, 'accuracy': 0.5462}\n",
      "✓ Best model updated!\n",
      "\n",
      "Epoch 2/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2023, Val Metrics: {'loss': 1.074976746737957, 'accuracy': 0.6298}\n",
      "✓ Best model updated!\n",
      "\n",
      "Epoch 3/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0197, Val Metrics: {'loss': 0.9488953307271004, 'accuracy': 0.6704}\n",
      "✓ Best model updated!\n",
      "\n",
      "Epoch 4/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9014, Val Metrics: {'loss': 0.7755203813314437, 'accuracy': 0.7304}\n",
      "✓ Best model updated!\n",
      "\n",
      "Epoch 5/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.8267, Val Metrics: {'loss': 0.7082339152693748, 'accuracy': 0.7504}\n",
      "✓ Best model updated!\n",
      "\n",
      "Epoch 6/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7570, Val Metrics: {'loss': 0.6670993015170097, 'accuracy': 0.7724}\n",
      "✓ Best model updated!\n",
      "\n",
      "Epoch 7/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7132, Val Metrics: {'loss': 0.6120938830077648, 'accuracy': 0.7834}\n",
      "✓ Best model updated!\n",
      "\n",
      "Epoch 8/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6761, Val Metrics: {'loss': 0.6535413131117821, 'accuracy': 0.777}\n",
      "No improvement for 1 epochs\n",
      "\n",
      "Epoch 9/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6389, Val Metrics: {'loss': 0.5610129199922085, 'accuracy': 0.8074}\n",
      "✓ Best model updated!\n",
      "\n",
      "Epoch 10/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6075, Val Metrics: {'loss': 0.6785904765129089, 'accuracy': 0.7712}\n",
      "No improvement for 1 epochs\n",
      "\n",
      "Epoch 11/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5799, Val Metrics: {'loss': 0.5659338288009167, 'accuracy': 0.8034}\n",
      "No improvement for 2 epochs\n",
      "\n",
      "Epoch 12/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5631, Val Metrics: {'loss': 0.5993236765265465, 'accuracy': 0.8004}\n",
      "No improvement for 3 epochs\n",
      "\n",
      "Epoch 13/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5373, Val Metrics: {'loss': 0.5487134583294392, 'accuracy': 0.8142}\n",
      "✓ Best model updated!\n",
      "\n",
      "Epoch 14/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5211, Val Metrics: {'loss': 0.5354428112506866, 'accuracy': 0.8228}\n",
      "✓ Best model updated!\n",
      "\n",
      "Epoch 15/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5019, Val Metrics: {'loss': 0.5323229275643826, 'accuracy': 0.8208}\n",
      "No improvement for 1 epochs\n",
      "Loading best model weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Test Metrics: {'loss': 0.5461378218252447, 'accuracy': 0.8164}\n",
      "\n",
      "Teacher model test accuracy: 0.8164\n",
      "Model saved to models/rescnn_ch16_d4_c10_teacher.pth\n"
     ]
    }
   ],
   "source": [
    "### Step 1: Train the Teacher Model (ResidualCNN depth=4)\n",
    "\n",
    "cifar10_train = CIFAR10(root='./data', train=True, download=True)\n",
    "cifar10_test = CIFAR10(root='./data', train=False, download=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create teacher model\n",
    "teacher_model = ResidualCNN(depth=4, num_classes=10)\n",
    "print(f\"Teacher model parameters: {sum(p.numel() for p in teacher_model.parameters()):,}\")\n",
    "\n",
    "# Create configuration for teacher training\n",
    "teacher_config = TrainingConfig(\n",
    "    num_epochs=15,\n",
    "    batch_size=128,\n",
    "    learning_rate=0.001,\n",
    "    weight_decay=1e-4,\n",
    "    loss_fn=nn.CrossEntropyLoss(),\n",
    "    device=device,\n",
    "    use_wandb=True,\n",
    "    wandb_project=\"dl-lab1-knowledge-distillation\",\n",
    "    use_early_stopping=True,\n",
    "    early_stopping_patience=5,\n",
    "    early_stopping_metric=\"accuracy\",\n",
    "    maximize_metric=True,\n",
    "    output_dir=\"models/\"\n",
    ")\n",
    "\n",
    "# Train teacher model\n",
    "print(\"Training teacher model...\")\n",
    "teacher_trainer = StreamlinedTrainer(\n",
    "    model=teacher_model,\n",
    "    config=teacher_config,\n",
    "    train_dataset=cifar10_train,\n",
    "    test_dataset=cifar10_test\n",
    ")\n",
    "\n",
    "teacher_results = teacher_trainer.train()\n",
    "print(f\"\\nTeacher model test accuracy: {teacher_results['accuracy']:.4f}\")\n",
    "\n",
    "# Save teacher model and store results\n",
    "teacher_path = teacher_trainer.save_model(\"teacher\")\n",
    "teacher_model_state = teacher_model.state_dict().copy()\n",
    "teacher_accuracy = teacher_results['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f40ce608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student model parameters: 165,914\n",
      "Parameter reduction: 5.94%\n",
      "Training student model (without distillation)...\n",
      "Training set size: 45000\n",
      "Validation set size: 5000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>test/accuracy</td><td>▁</td></tr><tr><td>test/loss</td><td>▁</td></tr><tr><td>train/train_loss</td><td>██▆▆▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val/accuracy</td><td>▁▃▄▆▆▇▇▇█▇█▇███</td></tr><tr><td>val/loss</td><td>█▆▅▃▃▂▂▂▁▂▁▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test/accuracy</td><td>0.8164</td></tr><tr><td>test/loss</td><td>0.54614</td></tr><tr><td>train/train_loss</td><td>0.49944</td></tr><tr><td>val/accuracy</td><td>0.8208</td></tr><tr><td>val/loss</td><td>0.53232</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">rescnn_ch16_d4_c10</strong> at: <a href='https://wandb.ai/vincenzo-civale-universi-degli-studi-di-firenze/dl-lab1-knowledge-distillation/runs/qury76v2' target=\"_blank\">https://wandb.ai/vincenzo-civale-universi-degli-studi-di-firenze/dl-lab1-knowledge-distillation/runs/qury76v2</a><br> View project at: <a href='https://wandb.ai/vincenzo-civale-universi-degli-studi-di-firenze/dl-lab1-knowledge-distillation' target=\"_blank\">https://wandb.ai/vincenzo-civale-universi-degli-studi-di-firenze/dl-lab1-knowledge-distillation</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250616_105226-qury76v2/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/vcivale/prova/Esercitazione_1/wandb/run-20250616_105629-y7nr4d3u</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/vincenzo-civale-universi-degli-studi-di-firenze/dl-lab1-knowledge-distillation/runs/y7nr4d3u' target=\"_blank\">rescnn_ch16_d2_c10</a></strong> to <a href='https://wandb.ai/vincenzo-civale-universi-degli-studi-di-firenze/dl-lab1-knowledge-distillation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/vincenzo-civale-universi-degli-studi-di-firenze/dl-lab1-knowledge-distillation' target=\"_blank\">https://wandb.ai/vincenzo-civale-universi-degli-studi-di-firenze/dl-lab1-knowledge-distillation</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/vincenzo-civale-universi-degli-studi-di-firenze/dl-lab1-knowledge-distillation/runs/y7nr4d3u' target=\"_blank\">https://wandb.ai/vincenzo-civale-universi-degli-studi-di-firenze/dl-lab1-knowledge-distillation/runs/y7nr4d3u</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for 15 epochs\n",
      "Device: cuda\n",
      "Batch size: 128\n",
      "\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.5143, Val Metrics: {'loss': 1.2407567590475082, 'accuracy': 0.5504}\n",
      "✓ Best model updated!\n",
      "\n",
      "Epoch 2/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.1961, Val Metrics: {'loss': 1.1029451370239258, 'accuracy': 0.6088}\n",
      "✓ Best model updated!\n",
      "\n",
      "Epoch 3/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0453, Val Metrics: {'loss': 1.0837183237075805, 'accuracy': 0.613}\n",
      "✓ Best model updated!\n",
      "\n",
      "Epoch 4/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9569, Val Metrics: {'loss': 0.9742089167237282, 'accuracy': 0.6594}\n",
      "✓ Best model updated!\n",
      "\n",
      "Epoch 5/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.8898, Val Metrics: {'loss': 0.8519542336463928, 'accuracy': 0.7024}\n",
      "✓ Best model updated!\n",
      "\n",
      "Epoch 6/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.8233, Val Metrics: {'loss': 0.7638009175658226, 'accuracy': 0.733}\n",
      "✓ Best model updated!\n",
      "\n",
      "Epoch 7/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7851, Val Metrics: {'loss': 0.8307042688131332, 'accuracy': 0.7124}\n",
      "No improvement for 1 epochs\n",
      "\n",
      "Epoch 8/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7479, Val Metrics: {'loss': 0.7266128897666931, 'accuracy': 0.7528}\n",
      "✓ Best model updated!\n",
      "\n",
      "Epoch 9/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7125, Val Metrics: {'loss': 0.6902000844478607, 'accuracy': 0.7646}\n",
      "✓ Best model updated!\n",
      "\n",
      "Epoch 10/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6795, Val Metrics: {'loss': 0.86166403144598, 'accuracy': 0.7192}\n",
      "No improvement for 1 epochs\n",
      "\n",
      "Epoch 11/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6551, Val Metrics: {'loss': 0.694909043610096, 'accuracy': 0.766}\n",
      "✓ Best model updated!\n",
      "\n",
      "Epoch 12/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6367, Val Metrics: {'loss': 0.691471978276968, 'accuracy': 0.7664}\n",
      "✓ Best model updated!\n",
      "\n",
      "Epoch 13/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6202, Val Metrics: {'loss': 0.5978748992085456, 'accuracy': 0.7922}\n",
      "✓ Best model updated!\n",
      "\n",
      "Epoch 14/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6011, Val Metrics: {'loss': 0.6620557852089405, 'accuracy': 0.769}\n",
      "No improvement for 1 epochs\n",
      "\n",
      "Epoch 15/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5846, Val Metrics: {'loss': 0.625201053917408, 'accuracy': 0.7804}\n",
      "No improvement for 2 epochs\n",
      "Loading best model weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Test Metrics: {'loss': 0.6288939273055596, 'accuracy': 0.7833}\n",
      "\n",
      "Student baseline test accuracy: 0.7833\n",
      "Model saved to models/rescnn_ch16_d2_c10_student_baseline.pth\n"
     ]
    }
   ],
   "source": [
    "### Step 2: Create and Train Student Model (smaller version)\n",
    "\n",
    "# Create student model with ~half the parameters\n",
    "student_model = ResidualCNN(depth=2,  num_classes=10)  # Reduced depth and width\n",
    "print(f\"Student model parameters: {sum(p.numel() for p in student_model.parameters()):,}\")\n",
    "print(f\"Parameter reduction: {sum(p.numel() for p in student_model.parameters()) / sum(p.numel() for p in teacher_model.parameters()):.2%}\")\n",
    "\n",
    "# Train student model without distillation\n",
    "student_config = TrainingConfig(\n",
    "    num_epochs=15,\n",
    "    batch_size=128,\n",
    "    learning_rate=0.001,\n",
    "    weight_decay=1e-4,\n",
    "    loss_fn=nn.CrossEntropyLoss(),\n",
    "    device=device,\n",
    "    use_wandb=True,\n",
    "    wandb_project=\"dl-lab1-knowledge-distillation\",\n",
    "    use_early_stopping=True,\n",
    "    early_stopping_patience=5,\n",
    "    early_stopping_metric=\"accuracy\",\n",
    "    maximize_metric=True,\n",
    "    output_dir=\"models/\"\n",
    ")\n",
    "\n",
    "print(\"Training student model (without distillation)...\")\n",
    "student_trainer = StreamlinedTrainer(\n",
    "    model=student_model,\n",
    "    config=student_config,\n",
    "    train_dataset=cifar10_train,\n",
    "    test_dataset=cifar10_test\n",
    ")\n",
    "\n",
    "student_baseline_results = student_trainer.train()\n",
    "print(f\"\\nStudent baseline test accuracy: {student_baseline_results['accuracy']:.4f}\")\n",
    "\n",
    "# Save student baseline\n",
    "student_baseline_path = student_trainer.save_model(\"student_baseline\")\n",
    "student_baseline_accuracy = student_baseline_results['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc33ec23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knowledge Distillation Trainer created!\n"
     ]
    }
   ],
   "source": [
    "### Step 3: Knowledge Distillation Training\n",
    "\n",
    "# Create a custom trainer for knowledge distillation\n",
    "class KnowledgeDistillationTrainer(StreamlinedTrainer):\n",
    "    def __init__(self, student_model, teacher_model, config, train_dataset, test_dataset, \n",
    "                 temperature=3.0, alpha=0.7, compute_metrics_fn=None):\n",
    "        # Initialize parent with student model\n",
    "        super().__init__(student_model, config, train_dataset, test_dataset, compute_metrics_fn)\n",
    "        \n",
    "        \n",
    "        self.teacher_model = teacher_model\n",
    "        self.teacher_model.to(config.device)\n",
    "        self.teacher_model.eval()  # Teacher is always in eval mode\n",
    "        \n",
    "        # Distillation parameters\n",
    "        self.temperature = temperature\n",
    "        self.alpha = alpha  # Weight for distillation loss\n",
    "        \n",
    "        # Loss functions\n",
    "        self.hard_loss_fn = nn.CrossEntropyLoss()\n",
    "        self.soft_loss_fn = nn.KLDivLoss(reduction='batchmean')\n",
    "        \n",
    "    def train_epoch(self):\n",
    "        self.model.train()\n",
    "        self.teacher_model.eval()\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        total_hard_loss = 0.0\n",
    "        total_soft_loss = 0.0\n",
    "        num_batches = 0\n",
    "        \n",
    "        from tqdm import tqdm\n",
    "        progress_bar = tqdm(self.train_loader, desc=\"Distillation Training\", leave=False)\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            inputs, labels = batch\n",
    "            inputs, labels = inputs.to(self.config.device), labels.to(self.config.device)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass through student\n",
    "            student_outputs = self.model(inputs)\n",
    "            \n",
    "            # Forward pass through teacher (no gradients)\n",
    "            with torch.no_grad():\n",
    "                teacher_outputs = self.teacher_model(inputs)\n",
    "            \n",
    "            # Calculate losses using the distillation_loss function from models.py\n",
    "            loss, hard_loss, soft_loss = distillation_loss(\n",
    "                student_outputs, teacher_outputs, labels,\n",
    "                temperature=self.temperature, alpha=self.alpha\n",
    "            )\n",
    "            \n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_hard_loss += hard_loss.item()\n",
    "            total_soft_loss += soft_loss.item()\n",
    "            num_batches += 1\n",
    "            self.global_step += 1\n",
    "            \n",
    "            avg_total_loss = total_loss / num_batches\n",
    "            avg_hard_loss = total_hard_loss / num_batches\n",
    "            avg_soft_loss = total_soft_loss / num_batches\n",
    "            \n",
    "            progress_bar.set_postfix({\n",
    "                \"total\": f\"{avg_total_loss:.4f}\",\n",
    "                \"hard\": f\"{avg_hard_loss:.4f}\", \n",
    "                \"soft\": f\"{avg_soft_loss:.4f}\"\n",
    "            })\n",
    "            \n",
    "            if self.config.logging_steps > 0 and self.global_step % self.config.logging_steps == 0:\n",
    "                self._log_metrics({\n",
    "                    \"train_loss\": avg_total_loss,\n",
    "                    \"train_hard_loss\": avg_hard_loss,\n",
    "                    \"train_soft_loss\": avg_soft_loss\n",
    "                }, \"train/\")\n",
    "        \n",
    "        return total_loss / num_batches\n",
    "\n",
    "print(\"Knowledge Distillation Trainer created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c81c5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training student model with knowledge distillation...\n",
      "Training set size: 45000\n",
      "Validation set size: 5000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>test/accuracy</td><td>▁</td></tr><tr><td>test/loss</td><td>▁</td></tr><tr><td>train/train_loss</td><td>█▇▇▅▅▅▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val/accuracy</td><td>▁▃▃▄▅▆▆▇▇▆▇▇█▇█</td></tr><tr><td>val/loss</td><td>█▆▆▅▄▃▄▂▂▄▂▂▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>test/accuracy</td><td>0.7833</td></tr><tr><td>test/loss</td><td>0.62889</td></tr><tr><td>train/train_loss</td><td>0.58367</td></tr><tr><td>val/accuracy</td><td>0.7804</td></tr><tr><td>val/loss</td><td>0.6252</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">rescnn_ch16_d2_c10</strong> at: <a href='https://wandb.ai/vincenzo-civale-universi-degli-studi-di-firenze/dl-lab1-knowledge-distillation/runs/y7nr4d3u' target=\"_blank\">https://wandb.ai/vincenzo-civale-universi-degli-studi-di-firenze/dl-lab1-knowledge-distillation/runs/y7nr4d3u</a><br> View project at: <a href='https://wandb.ai/vincenzo-civale-universi-degli-studi-di-firenze/dl-lab1-knowledge-distillation' target=\"_blank\">https://wandb.ai/vincenzo-civale-universi-degli-studi-di-firenze/dl-lab1-knowledge-distillation</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250616_105629-y7nr4d3u/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "creating run (0.0s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.20.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/vcivale/prova/Esercitazione_1/wandb/run-20250616_110132-309hnt67</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/vincenzo-civale-universi-degli-studi-di-firenze/dl-lab1-knowledge-distillation/runs/309hnt67' target=\"_blank\">rescnn_ch16_d2_c10</a></strong> to <a href='https://wandb.ai/vincenzo-civale-universi-degli-studi-di-firenze/dl-lab1-knowledge-distillation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/vincenzo-civale-universi-degli-studi-di-firenze/dl-lab1-knowledge-distillation' target=\"_blank\">https://wandb.ai/vincenzo-civale-universi-degli-studi-di-firenze/dl-lab1-knowledge-distillation</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/vincenzo-civale-universi-degli-studi-di-firenze/dl-lab1-knowledge-distillation/runs/309hnt67' target=\"_blank\">https://wandb.ai/vincenzo-civale-universi-degli-studi-di-firenze/dl-lab1-knowledge-distillation/runs/309hnt67</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training for 20 epochs\n",
      "Device: cuda\n",
      "Batch size: 128\n",
      "\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'distillation_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 37\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining student model with knowledge distillation...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m distillation_trainer \u001b[38;5;241m=\u001b[39m KnowledgeDistillationTrainer(\n\u001b[1;32m     28\u001b[0m     student_model\u001b[38;5;241m=\u001b[39mstudent_distilled,\n\u001b[1;32m     29\u001b[0m     teacher_model\u001b[38;5;241m=\u001b[39mteacher_for_distillation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     34\u001b[0m     alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m  \u001b[38;5;66;03m# Weight for distillation loss (0.7 distillation + 0.3 hard labels)\u001b[39;00m\n\u001b[1;32m     35\u001b[0m )\n\u001b[0;32m---> 37\u001b[0m student_distilled_results \u001b[38;5;241m=\u001b[39m \u001b[43mdistillation_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStudent (distilled) test accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstudent_distilled_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Save distilled student model\u001b[39;00m\n",
      "File \u001b[0;32m~/prova/Esercitazione_1/src/trainer.py:161\u001b[0m, in \u001b[0;36mStreamlinedTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_epochs):\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 161\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    162\u001b[0m     val_metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_loader, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Val Metrics: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_metrics\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 49\u001b[0m, in \u001b[0;36mKnowledgeDistillationTrainer.train_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     46\u001b[0m     teacher_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mteacher_model(inputs)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Calculate losses using the distillation_loss function from models.py\u001b[39;00m\n\u001b[0;32m---> 49\u001b[0m loss, hard_loss, soft_loss \u001b[38;5;241m=\u001b[39m \u001b[43mdistillation_loss\u001b[49m(\n\u001b[1;32m     50\u001b[0m     student_outputs, teacher_outputs, labels,\n\u001b[1;32m     51\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemperature, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha\n\u001b[1;32m     52\u001b[0m )\n\u001b[1;32m     54\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'distillation_loss' is not defined"
     ]
    }
   ],
   "source": [
    "# Create a fresh student model for distillation training\n",
    "student_distilled = ResidualCNN(depth=2, num_classes=10)\n",
    "\n",
    "# Load the trained teacher model\n",
    "teacher_for_distillation = ResidualCNN(depth=4,num_classes=10)\n",
    "teacher_for_distillation.load_state_dict(teacher_model_state)\n",
    "\n",
    "# Configuration for distillation training\n",
    "distillation_config = TrainingConfig(\n",
    "    num_epochs=20,  # More epochs for distillation\n",
    "    batch_size=128,\n",
    "    learning_rate=0.001,\n",
    "    weight_decay=1e-4,\n",
    "    loss_fn=nn.CrossEntropyLoss(),  # This won't be used, but needed for parent class\n",
    "    device=device,\n",
    "    use_wandb=True,\n",
    "    wandb_project=\"dl-lab1-knowledge-distillation\",\n",
    "    use_early_stopping=True,\n",
    "    early_stopping_patience=7,\n",
    "    early_stopping_metric=\"accuracy\",\n",
    "    maximize_metric=True,\n",
    "    output_dir=\"models/\"\n",
    ")\n",
    "\n",
    "# Train student with knowledge distillation\n",
    "print(\"Training student model with knowledge distillation...\")\n",
    "distillation_trainer = KnowledgeDistillationTrainer(\n",
    "    student_model=student_distilled,\n",
    "    teacher_model=teacher_for_distillation,\n",
    "    config=distillation_config,\n",
    "    train_dataset=cifar10_train,\n",
    "    test_dataset=cifar10_test,\n",
    "    temperature=3.0,  # Temperature for softmax\n",
    "    alpha=0.7  # Weight for distillation loss (0.7 distillation + 0.3 hard labels)\n",
    ")\n",
    "\n",
    "student_distilled_results = distillation_trainer.train()\n",
    "print(f\"\\nStudent (distilled) test accuracy: {student_distilled_results['accuracy']:.4f}\")\n",
    "\n",
    "# Save distilled student model\n",
    "distilled_path = distillation_trainer.save_model(\"student_distilled\")\n",
    "student_distilled_accuracy = student_distilled_results['accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abc2168",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Step 4: Compare Results\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"KNOWLEDGE DISTILLATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Teacher Model (ResidualCNN depth=4):     {teacher_accuracy:.4f}\")\n",
    "print(f\"Student Baseline (ResidualCNN depth=2):  {student_baseline_accuracy:.4f}\")\n",
    "print(f\"Student Distilled (ResidualCNN depth=2): {student_distilled_accuracy:.4f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "improvement = student_distilled_accuracy - student_baseline_accuracy\n",
    "print(f\"Improvement from Knowledge Distillation: {improvement:.4f} ({improvement*100:.2f}%)\")\n",
    "\n",
    "# Calculate parameter reduction\n",
    "teacher_params = sum(p.numel() for p in teacher_model.parameters())\n",
    "student_params = sum(p.numel() for p in student_distilled.parameters())\n",
    "print(f\"Parameter reduction: {student_params/teacher_params:.2%} of teacher size\")\n",
    "\n",
    "# Visualize results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "models = ['Teacher\\n(ResNet-4)', 'Student\\nBaseline', 'Student\\nDistilled']\n",
    "accuracies = [teacher_accuracy, student_baseline_accuracy, student_distilled_accuracy]\n",
    "colors = ['#2E8B57', '#CD5C5C', '#4169E1']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(models, accuracies, color=colors, alpha=0.7, edgecolor='black', linewidth=1)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + 0.002,\n",
    "             f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.ylabel('Test Accuracy', fontsize=12)\n",
    "plt.title('Knowledge Distillation Results on CIFAR-10', fontsize=14, fontweight='bold')\n",
    "plt.ylim(0, max(accuracies) + 0.05)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add parameter count annotations\n",
    "param_counts = [teacher_params, student_params, student_params]\n",
    "for i, (bar, params) in enumerate(zip(bars, param_counts)):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., 0.02,\n",
    "             f'{params:,}\\nparams', ha='center', va='bottom', \n",
    "             fontsize=9, style='italic')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary\n",
    "if improvement > 0:\n",
    "    print(f\"\\nKnowledge distillation improved student performance by {improvement:.4f}!\")\n",
    "    print(f\"   The distilled student achieved {student_distilled_accuracy:.4f} vs {student_baseline_accuracy:.4f} baseline\")\n",
    "else:\n",
    "    print(f\"\\nThe distilled student did not outperform the baseline.\")\n",
    "    print(\"   Try adjusting hyperparameters (temperature, alpha, learning rate, epochs)\")\n",
    "    \n",
    "print(f\"\\n📊 Knowledge transfer efficiency: {(student_distilled_accuracy/teacher_accuracy)*100:.1f}%\")\n",
    "print(f\"   (Student achieved {(student_distilled_accuracy/teacher_accuracy)*100:.1f}% of teacher performance with {student_params/teacher_params*100:.1f}% of parameters)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1e55d7",
   "metadata": {},
   "source": [
    "### Knowledge Distillation Analysis\n",
    "\n",
    "**What we implemented:**\n",
    "\n",
    "1. **Teacher Model**: ResidualCNN with depth=4, width=64 (~larger model)\n",
    "2. **Student Model**: ResidualCNN with depth=2, width=32 (~50% fewer parameters)\n",
    "3. **Knowledge Distillation**: Combined loss using:\n",
    "   - **Hard loss**: Cross-entropy with true labels (weight = 1-α)\n",
    "   - **Soft loss**: KL-divergence with teacher predictions (weight = α)\n",
    "   - **Temperature**: Softens probability distributions for better knowledge transfer\n",
    "\n",
    "**Key Hyperparameters:**\n",
    "- **Temperature (T=3.0)**: Controls softness of probability distributions\n",
    "- **Alpha (α=0.7)**: Balance between distillation loss and hard labels\n",
    "- **Extended training**: More epochs to allow proper knowledge transfer\n",
    "\n",
    "**Expected Outcome:**\n",
    "The distilled student should outperform the baseline student, demonstrating that soft labels from the teacher provide additional information beyond hard labels alone.\n",
    "\n",
    "**Why Knowledge Distillation Works:**\n",
    "- Teacher provides \"dark knowledge\" - information about wrong classes\n",
    "- Soft targets reveal similarities between classes\n",
    "- Student learns from teacher's feature representations\n",
    "- More efficient than training large models from scratch"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "33cc12cc-8422-47bf-8d8e-0950ac05ae96",
    "24e05e96-7707-4490-98b8-50cb5e330af1",
    "875008c3-306c-4e39-a845-d7bda7862621",
    "4ae06e26-8fa3-414e-a502-8d1c18ba9eb7",
    "de2cad13-ee2c-4e43-b5c7-31760da8c2df"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
